{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "Show that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Without loss of generality, assume that the smaller model has had the final feature $\\mathbf{x}_p$ removed. Let $\\hat{\\mathbf{y}}$ denote the least squares approximation for the larger model and $\\hat{\\mathbf{y}}^{\\prime}$ be that for the smaller. We need to show that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\text{RSS}_0 - \\text{RSS}_1}{\\text{RSS}_1 / (N-p-1)} = \\frac{\\hat{\\beta}_p^2}{\\hat{\\sigma}^2 v_p},\n",
    "\\end{equation}\n",
    "\n",
    "where $v_j$ is the $j$th diagonal element of $(\\mathbf{X}^T \\mathbf{X})^{-1}$. \n",
    "\n",
    "First note that by definition $\\hat{\\sigma}^2 = \\text{RSS}_0 / (N-p-1)$. Moreover, since $\\hat{\\mathbf{y}}$ is the projection of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$, their difference is orthogonal to any element of the columns space. In particular, it is orthogonal to $\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime}$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2 \n",
    "        + \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Rightarrow \\text{RSS}_0 - \\text{RSS}_1\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2 \n",
    "            - \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2\n",
    "         = \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "Now let $\\mathbf{z}_0,\\ldots ,\\mathbf{z}_p$ denote the orthogonal basis of the column space of $\\mathbf{X}$, obtained from $\\mathbf{x}_0,\\ldots,\\mathbf{x}_p$ using the Gram-Schmidt process (Algorithm 3.1). The least squares estimates  $\\hat{\\mathbf{y}}$ and $\\hat{\\mathbf{y}}^{\\prime}$ are the projections of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$ and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{span}(\\{\\, \\mathbf{x}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\} = \\text{span}(\\{\\, \\mathbf{z}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\}\n",
    "\\end{equation}\n",
    "\n",
    "respectively. Since the $\\mathbf{z}_j$ are orthogonal, this implies that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} = \\frac{\\langle \\mathbf{z}_p, \\mathbf{y}\\rangle}{\\langle \\mathbf{z}_p, \\mathbf{z}_p\\rangle}\\mathbf{z}_p = \\hat{\\beta}_p \\mathbf{z}_p.\n",
    "\\end{equation}\n",
    "\n",
    "Putting these elements together, it just remains to show that $v_p = \\lVert \\mathbf{z}_p\\rVert ^{-2}$. But, if $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ is the QR-decomposition of $\\mathbf{X}$ then $(\\mathbf{X}^T \\mathbf{X})^{-1} = \\mathbf{R}^{-1}(\\mathbf{R}^{-1})^T$. Since $\\mathbf{R}$ is upper-triangular, the $p$th diagonal element of $\\mathbf{R}^{-1}$ is $R_{pp}^{-1} = \\lVert \\mathbf{z}_p\\rVert ^{-1}$ and the claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "\n",
    "Given data on two variables $X$ and $Y$, consider fitting a cubic polynomial regression model $f(X) = \\sum_{j=0}^3\\beta_jX^j$. In addition to plotting the fitted curve, you would like a 95% confidence band about the curve. Consider the following two approaches:\n",
    "\n",
    "1. At each point $x_0$, form a 95% confidence interval for the linear function $a^T\\beta = \\sum_{j=0}^3 \\beta_jx_0^j$.\n",
    "2. Form a 95% confidence set for $\\beta$ as in (3.15), which in turn generates confidence intervals for $f(x_0)$.\n",
    "\n",
    "How do these approaches differ? Which band is likely to be wider? Conduct a small simulation experiment to compare the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### Construction of Confidence Intervals\n",
    "\n",
    "Let $p=3$ and $\\alpha=0.05$ and write $x = (1, x_0, x_0^2, x_0^3)$.\n",
    "\n",
    "**1.** We have $\\beta\\sim\\mathcal{N}(\\hat{\\beta}, \\sigma^2\\mathbf{I})$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    x^T(\\hat{\\beta}-\\beta)\\sim\\mathcal{N}(0,x^T(\\mathbf{X}^T\\mathbf{X})^{-1}x\\sigma^2).\n",
    "\\end{equation}\n",
    "\n",
    "Let $v = x^T(\\mathbf{X}^T\\mathbf{X})^{-1}x\\in\\mathbf{R}$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{x^T(\\hat{\\beta} - \\beta)}{\\hat{\\sigma}\\sqrt{v}}\\sim t_{N-p-1},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{\\sigma}$ is the unbiased estimate for $\\sigma$ on p.47. Therefore, a $100(1-\\alpha)$% confidence interval for $f(x_0) = x^T\\beta$ has endpoints\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x_0) \\pm \\hat{\\sigma}\\sqrt{x^T(\\mathbf{X}^T\\mathbf{X})^{-1}x}~t_{N-p-1,\\alpha/2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{f}(x_0) = x^T\\hat{\\beta}$ and $t_{N-p-1,\\alpha/2}$ is the $\\frac{1}{2}\\alpha$th percentile of a $T$ distribution with $N-p-1$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** By the argument on p.49, an approximate 95% confidence set for $f(x_0)$ is the set of $x^T\\beta$ such that $\\beta$ lies in \n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\big\\{\\, \\beta \\,\\big| \\, \\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 \\leq \\hat{\\sigma}^2\\chi^2 \\,\\big\\},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\chi^2 = \\chi_{p+1,\\alpha}^2$ is the $\\alpha$th percentile of a chi-squared distribution with $p+1$ degrees of freedom.\n",
    "\n",
    "First note that $C$ is an ellipsoid in $\\mathbb{R}^{p+1}$. This implies that the restriction of the linear function $x^T\\beta$ to $C$ achieves its maximum and minimum on the boundary $\\partial C$ of $C$ and takes every value in between. In particular, $\\{ x^T\\beta\\mid\\beta\\in C\\}$ is an interval and the endpoints of this interval are the maximum and minimum of $x^T\\beta$ subject to the constraint $\\beta\\in\\partial C$, or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 = \\hat{\\sigma}^2\\chi^2.\n",
    "\\end{equation}\n",
    "\n",
    "We solve this problem using Lagrange multipliers. Let\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\beta,\\lambda) = x^T\\beta - \\lambda\\left(\\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 - \\hat{\\sigma}^2\\chi^2\\right).\n",
    "\\end{equation}\n",
    "\n",
    "This has gradient $\\nabla \\mathcal{L} = (\\frac{\\partial \\mathcal{L}}{\\partial \\beta},\\frac{\\partial\\mathcal{L}}{\\partial \\lambda})$ with\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = x - 2\\lambda\\mathbf{X}^T(\\mathbf{X}\\beta - \\mathbf{X}\\hat{\\beta}),\n",
    "        \\qquad \\frac{\\partial\\mathcal{L}}{\\partial \\lambda} = \\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 - \\hat{\\sigma}^2\\chi^2.\n",
    "\\end{equation}\n",
    "\n",
    "Any solution to our optimisation problem will have $\\nabla\\mathcal{L}=0$. Setting the first partial derivative equation to zero gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}^T\\mathbf{X}(\\beta-\\hat{\\beta}) = \\frac{1}{2\\lambda}x \n",
    "        \\quad\\Rightarrow\\quad \\beta = \\hat{\\beta} + \\frac{1}{2\\lambda} \\mathbf{X}^T\\mathbf{X}x.\n",
    "\\end{equation}\n",
    "\n",
    "Setting $\\frac{\\partial\\mathcal{L}}{\\partial \\lambda}=0$ and substituting this in give\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Big\\lVert \\frac{1}{2\\lambda}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x\\Big\\rVert^2 = \\hat{\\sigma}^2\\chi^2\n",
    "        \\quad\\Rightarrow\\quad \\frac{1}{2\\lambda} = \\pm \\frac{\\hat{\\sigma}\\chi}{\\lVert \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x\\rVert}.\n",
    "\\end{equation}\n",
    "\n",
    "So, since we know the optimisation problem has a maximuma and a minimum, they must occur at\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta = \\hat{\\beta} \\pm \\hat{\\sigma}\\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}x}{\\lVert \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x\\rVert}\\chi.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the confidence interval for $f(x_0)$ has endpoints\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x_0) \\pm \\hat{\\sigma}\\frac{x^T(\\mathbf{X}^T\\mathbf{X})^{-1}x}{\\lVert \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x\\rVert}\\chi_{p+1,\\alpha}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "The key difference is that the first method gives a $100(1-\\alpha)$% confidence interval at a *particular* $x_0$.  That is, for a fixed $x_0$ the probability that $f(x_0)$ lies inside the confidence bands is $1-\\alpha$. The second method starts with a $100(1-\\alpha)$% confidence set for $\\beta$ and so is valid for all $x_0$ simultaneously. More precisely, there is a probability of $1-\\alpha$ that $\\beta$ lies in the confidence set and thus $f(x_0)$ lies in the confidence interval *for all $x_0$*.\n",
    "\n",
    "A less important distinction is that first method is exact (given the assumptions), whereas the second uses the approximation of $t_{N-p-1}$ by the standard normal distribution $p+1$ times. The $T$ distribution is less peaked than the standard normal, $t_{k, \\alpha} > z_{\\alpha}$ for $\\alpha<0.5$, but for $N$ sufficiently large the difference will be insignificant.\n",
    "\n",
    "Both of these differences imply that the chi-squared confidence interval should be wider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function from a custom module calculates and prints the confidence bands from the two methods against the regression function $f(x_0)$. We use a simulated data set of size $N$ with $X$ sampled from a normal distribution with mean `xmean` and standard deviation `xstdev`, and show the $100(1-\\alpha)$% confidence bands for $x_0$ in the range `plot_range` (=`xmean`$\\pm 2$`xstdev` by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta: [-1, -3, 1, 1]\n",
      "Betahat: [-1.055 -2.678  1.113  0.949]\n",
      "\n",
      "\n",
      "RSS:  90.9\n",
      "Standard error:  0.973\n",
      "\n",
      "\n",
      "Endpoints of 95.0% confidence interval at xmean:\n",
      "T method:  [-1.29, -0.821]\n",
      "Chi-square method:  [-1.419, -0.692]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from confband import run_sim\n",
    "\n",
    "beta = [-1, -3, 1, 1]\n",
    "N = 100\n",
    "sigma = 1\n",
    "xmean = 0\n",
    "xstdev = 1\n",
    "alpha = 0.05\n",
    "plot_range = None\n",
    "\n",
    "run_sim(beta, alpha, N, sigma, xmean, xstdev, plot_range = plot_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3\n",
    "\n",
    "GaussMarkov theorem:\n",
    "\n",
    "**(a)** Prove the GaussMarkov theorem: the least squares estimate of a parameter $a^T \\beta$ has variance no bigger than that of any other linear unbiased estimate of $a^T \\beta$ (Section 3.2.2).\n",
    "\n",
    "**(b)** The matrix inequality $\\mathbf{B} \\preccurlyeq \\mathbf{A}$ holds if $\\mathbf{A}  \\mathbf{B}$ is positive semidefinite. Show that if $\\hat{\\mathbf{V}}$ is the variance-covariance matrix of the least squares estimate of $\\beta$ and $\\tilde{\\mathbf{V}}$ is the variance-covariance matrix of any other linear unbiased estimate, then $\\hat{\\mathbf{V}} \\preccurlyeq \\tilde{\\mathbf{V}}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** Let $\\theta = a^T\\beta$. The least squares estimator $\\hat{\\beta}$ of $\\beta$ satisfies $\\hat{\\beta}\\sim\\mathcal{N}(\\beta, \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1})$ so the least squares estimate $\\hat{\\theta} = a^T\\hat{\\beta}$ of $\\theta$ has\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Var}(\\hat{\\theta}) = \\sigma^2a^T(\\mathbf{X}^{-1}\\mathbf{X})^{-1}a.\n",
    "\\end{equation}\n",
    "\n",
    "Suppose $\\tilde{\\theta}=c^T\\mathbf{y}$ is a linear unbiased estimator of $\\theta$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{y}\\sim\\mathcal{N}(\\mathbf{X}\\beta,\\sigma^2I_N)\n",
    "        \\quad\\Rightarrow\\quad \\tilde{\\theta}\\sim\\mathcal{N}(c^T\\mathbf{X}\\beta, \\sigma^2c^Tc).\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\tilde{\\theta}$ is unbiased for $\\theta$,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{E}(\\tilde{\\theta})=\\theta\n",
    "        \\quad\\Rightarrow\\quad c^T\\mathbf{X}\\beta = a^T\\beta\n",
    "        \\quad\\Rightarrow\\quad (c^T\\mathbf{X} - a^T)\\beta = 0.\n",
    "\\end{equation}\n",
    "\n",
    "But this must hold for *every* $\\beta\\in\\mathbb{R}^{p+1}$, so $c^T\\mathbf{X} = a^T$. \n",
    "\n",
    "We have reduced the problem to showing that if $c^T\\mathbf{X} = a^T$ then $c^Tc\\geq a^T(\\mathbf{X}^T\\mathbf{X})^{-1}a$. We will establish this by showing that $a^T(\\mathbf{X}^T\\mathbf{X})^{-1}a$ is a global minimum for $\\lVert c\\rVert^2$ subject to the constraint $c^T\\mathbf{X} = a^T$.\n",
    "\n",
    "First, observe that a minimum exists since the solution set is non-empty (consider $c = (\\mathbf{X}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^T$), bounded below by 0, and closed. We will find it using Lagrange multipliers.\n",
    "\n",
    "Let $\\lambda\\in\\mathbb{R}^{p+1}$ be variables and define\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(c,\\lambda) = \\lVert c\\rVert^2 = \\lambda^T(\\mathbf{X}^Tc - a).\n",
    "\\end{equation}\n",
    "\n",
    "This satisfies\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial c} = 2c - \\mathbf{X}\\lambda,\n",
    "        \\qquad \\frac{\\partial\\mathcal{L}}{\\partial c} = \\mathbf{X}^Tc - a.\n",
    "\\end{equation}\n",
    "\n",
    "At an extremum of our optimisation problem both of these are zero, so\n",
    "\n",
    "\\begin{align}\n",
    "    c = \\frac{1}{2}\\mathbf{X}\\lambda\n",
    "        & \\quad\\Rightarrow\\quad \\frac{1}{2}\\mathbf{X}^T\\mathbf{X}\\lambda = a \\\\\n",
    "        & \\quad\\Rightarrow\\quad \\lambda = 2(\\mathbf{X}^T\\mathbf{X})^{-1}a \\\\\n",
    "        & \\quad\\Rightarrow\\quad c = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}a \\\\\n",
    "        & \\quad\\Rightarrow\\quad \\tilde{\\theta} = c^T\\mathbf{y} = \\hat{\\theta}.\n",
    "\\end{align}\n",
    "\n",
    "Since the Lagrange multiplier has a unique solution, this must be the global minimum of the constrained optimisation problem. In fact we have proved something slightly stronger: that $\\hat{\\theta}$ is the unique unbiased estimator with minimal variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Our proof is analogous to that in part (a). The covariance matrix of $\\hat{\\beta}$ is $\\text{Var}(\\hat{\\beta}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$. If $\\tilde{\\beta} = \\mathbf{A}\\mathbf{y}$ is another unbiased estimator for $\\beta$ then $\\tilde{\\beta} \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{X}\\beta, \\sigma^2\\mathbf{A}\\mathbf{A}^T)$. Since $\\tilde{\\beta}$ is unbiased, $\\mathbf{A}\\mathbf{X}\\beta = \\beta$ for all $\\beta\\in\\mathbb{R}^{p+1}$ and so $\\mathbf{A}\\mathbf{X} = \\mathbf{I}_{P+1}$. Thus we have reduced the problem to showing that if $\\mathbf{A}$ is a $(p+1)\\times N$ matrix with $\\mathbf{A}\\mathbf{X} = \\mathbf{I}_{p+1}$ then $\\mathbf{A}\\mathbf{A}^T - (\\mathbf{X}^T\\mathbf{X})^{-1}$ is positive semi-definite, or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "    v^T\\mathbf{A}\\mathbf{A}v \\geq v^T(\\mathbf{X}^T\\mathbf{X})^{-1}v \\quad \\text{for all } v\\in\\mathbb{R}^{p+1}\\setminus \\{0\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Again we treat this as a constrained optimisation problem and employ Lagrange multipliers. Fix $v\\in\\mathbb{R}^{p+1}$. The set $\\{ \\lVert\\mathbf{A}^Tv\\rVert^2 \\mid \\mathbf{A}\\mathbf{X}=\\mathbf{I}_{p+1}\\}$ is non-empty (take $\\mathbf{A} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$), bounded below by zero, and closed so has a minimum.\n",
    "\n",
    "Let $\\Lambda$ be a $(p+1)\\times(p+1)$ matrix of variables and define\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\mathbf{A},\\Lambda) = \\lVert \\mathbf{A}^T v\\rVert^2 - \\Lambda \\cdot(\\mathbf{A}\\mathbf{X} - \\mathbf{I}_{p+1},\n",
    "\\end{equation}\n",
    "\n",
    "where'$\\cdot$' denotes the dot product. This has\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial A} = 2vv^T\\mathbf{A} - \\Lambda\\mathbf{X}^T,\n",
    "        \\qquad \\frac{\\partial\\mathcal{L}}{\\partial \\Lambda} = \\mathbf{A}\\mathbf{X} - \\mathbf{I}_{p+1}.\n",
    "\\end{equation}\n",
    "\n",
    "At an extremum of the constrained optimisation problem both of these are zero, so\n",
    "\n",
    "\\begin{align}\n",
    "    2vv^T\\mathbf{A}\\mathbf{X} = \\Lambda\\mathbf{X}^T\\mathbf{X}\n",
    "        & \\quad\\Rightarrow\\quad 2vv^T  = \\Lambda\\mathbf{X}^T\\mathbf{X} \\\\\n",
    "        & \\quad\\Rightarrow\\quad \\Lambda = vv^T(\\mathbf{X}^T\\mathbf{X}^{-1}) \\\\\n",
    "        & \\quad\\Rightarrow\\quad vv^T\\mathbf{A} = vv^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T.\n",
    "\\end{align}\n",
    "\n",
    "Multiplying each term on the right by its transpose yields\n",
    "\n",
    "\\begin{align}\n",
    "    vv^T\\mathbf{A}\\mathbf{A}^Tvv^T & = vv^T(\\mathbf{X}^T\\mathbf{X})^{-1}vv^T \\\\\n",
    "    \\left[v^T\\mathbf{A}\\mathbf{A}^Tv\\right]vv^T & = \\left[v^T(\\mathbf{X}^T\\mathbf{X})^{-1}v\\right]vv^T \\\\\n",
    "    v^T\\mathbf{A}\\mathbf{A}^Tv & = v^T(\\mathbf{X}^T\\mathbf{X})^{-1}v,\n",
    "\\end{align}\n",
    "\n",
    "where the second line holds as both terms in square brackets are scalars and so commute with everything. Since the constrained optimisation problem has a minimum this must be it. This establishes the claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4\n",
    "\n",
    "Show how the vector of least squares coefficients can be obtained from a single pass of the GramSchmidt procedure (Algorithm 3.1). Represent your solution in terms of the $QR$ decomposition of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "To obtain $\\hat{\\beta}$ from the Gram-Schmidt process add an extra step $2^\\prime$. Take $j\\in\\{1,\\ldots, p\\}$ and suppose that for $k<j$ we have written\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}_k = \\mathbf{x}_k + \\sum_{l=0}^{k-1}\\hat{\\delta}_{lk}\\mathbf{x}_l.\n",
    "\\end{equation}\n",
    "\n",
    "Then we define coefficients $\\hat{\\delta}_{kj}$ for $k=0,\\ldots, j-1$ by\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{z}_j\n",
    "        & = \\mathbf{x}_j - \\sum_{k=0}^{j-1} \\hat{\\gamma}_{kj}\\mathbf{z}_k \\\\\n",
    "        & = \\mathbf{x}_j - \\sum_{k=0}^{j-1} \\hat{\\gamma}_{kj}\\left(\\mathbf{x}_k + \\sum_{l=0}^k\\hat{\\delta}_{lk}\\mathbf{x}_l\\right) \\\\\n",
    "        & = \\mathbf{x}_j + \\sum_{k=0}^{j-1}\\hat{\\delta}_{kj}\\mathbf{x}_k.\n",
    "\\end{align}\n",
    "\n",
    "Let $\\Delta$ be the matrix with $\\Delta_{ij}=\\hat{\\delta}_{ij}$ for $i<j$, ones on the diagonal, and zeros elsewhere. By construction,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{Z} = \\mathbf{X}\\Delta \\quad\\Rightarrow\\quad \\Delta = \\Gamma^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "By (3.32), $\\hat{\\beta} = \\Gamma^{-1}\\mathbf{Z}^T\\mathbf{y} = \\Delta\\mathbf{Z}^T\\mathbf{y}$ so we can calculate $\\hat{\\beta}$ explicitly:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\beta}_j = \\mathbf{z}_j+\\sum_{k=j+1}^p\\hat{\\delta}_{jk}\\mathbf{z}_k.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5\n",
    "\n",
    "Consider the ridge regression problem (3.41). Show that this problem is equivalent to the problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^c = \\underset{\\beta^c}{\\text{argmin}} \\Bigg\\{ \\sum_{i=1}^N \\big[ y_i - \\beta_0^c  - \\sum_{j=1}^p (x_{ij}-\\bar{x}_j)\\beta_j^c\\big]^2 + \\lambda \\sum_{j=1}^p (\\beta_j^c)^2\\Bigg\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Give the correspondence between $\\beta^c$ and the original $\\beta$ in (3.41). Characterize the solution to this modified criterion. Show that a similar result holds for the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Our solution is the same for both ridge regresssion and lasso. Since\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0^c + \\sum_{j=1}^p(x_{ij} - \\bar{x}_j)\\beta_j^c \n",
    "        = \\left( \\beta_0^c - \\sum_{j=1}^p \\bar{x}_j\\beta_j^c\\right) + \\sum_{j=1}^p x_{ij}\\beta_j^c,\n",
    "\\end{equation}\n",
    "\n",
    "the two minimisation problems are equivalent with \n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0^c = \\beta_0 + \\sum_{j=1}^p \\bar{x}_j\\beta_j = \\frac{1}{N}\\sum_{i=1}^N \\hat{y}_i\n",
    "\\end{equation}\n",
    "\n",
    "and $\\beta_j^c = \\beta_j$ for $j\\neq 0$.\n",
    "\n",
    "At the minimum, the derivative with respect to $\\beta_0^c$ of the expression in braces is zero, so\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^N \\left[ y_i - \\beta_0^c  - \\sum_{j=1}^p (x_{ij}-\\bar{x}_j)\\beta_j^c \\right] = 0 \\quad\n",
    "        \\Rightarrow \\quad \\left( \\sum_{i=1}^N y_i \\right) - N\\beta_0^c - 0 = 0\n",
    "\\end{align}\n",
    "\n",
    "and thus the solution to the modified criterion is $\\beta_0^c = \\bar{y}$, $\\beta_j^c = \\beta_j$ for $j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Show that the ridge regression estimate is the mean (and mode) of the posterior distribution, under a Gaussian prior $\\beta\\sim \\mathcal{N}(0, \\tau^2 \\mathbf{I})$, and Gaussian sampling model $y\\sim\\mathcal{N}(\\mathbf{X}\\beta, \\sigma^2\\mathbf{I})$. Find the relationship between the regularization parameter $\\lambda$ in the ridge formula, and the variances $\\tau^2$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The posterior distribution for $\\beta$ has density function\n",
    "\n",
    "\\begin{align}\n",
    "    f_{\\beta\\mid Y_1, \\ldots, Y_N}(\\beta)\n",
    "        & \\propto f_{Y_1, \\ldots, Y_N\\mid\\beta}(y_1,\\ldots, y_N)f_{\\beta}(\\beta) \\\\\n",
    "        & = \\left(\\prod_{i=1}^N f_{Y\\mid\\beta}(y_i)\\right) f_{\\beta}(\\beta) \\\\\n",
    "        & \\propto \\text{exp}\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - x_i^T\\beta)^2 - \\frac{1}{\\tau^2}\\lVert \\beta\\rVert^2\\right)\\\\\n",
    "        & = \\text{exp}\\left( -\\frac{1}{2\\sigma^2}\\left( \\lVert\\mathbf{y} - \\mathbf{X}\\beta\\rVert^2 + \\frac{\\sigma^2}{\\tau^2}\\lVert\\beta\\rVert^2\\right)\\right).\n",
    "\\end{align}\n",
    "\n",
    "So the mode of this distribution is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{\\beta}{\\text{argmax}} \\left(f_{\\beta\\mid Y_1, \\ldots, Y_N}(\\beta)\\right)\n",
    "        = \\underset{\\beta}{\\text{argmin}}\\left(\\lVert\\mathbf{y} - \\mathbf{X}\\beta\\rVert^2 + \\frac{\\sigma^2}{\\tau^2}\\lVert\\beta\\rVert^2\\right).\n",
    "\\end{equation}\n",
    "\n",
    "This is the ridge regression estimate with $\\lambda = \\sigma^2/\\tau^2$. Moreover, since the posterior is Gaussian (p.64) the mean equals the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.7\n",
    "\n",
    "Assume $y_i\\sim\\mathcal{N}(\\beta_0+x_i^T \\beta, \\sigma^2)$, $i=1,2,...,N$, and the parameters $\\beta_j$, $j = 1,...,p$ are each distributed as $\\mathcal{N}(0, \\tau^2)$, independently of one another. Assuming $\\sigma^2$ and $\\tau^2$ are known, and $\\beta_0$ is not governed by a prior (or has a flat improper prior), show that the (minus) log-posterior density of $\\beta$ is proportional to $\\sum_{i=1}^N(y_i  \\beta_0  \\sum_j x_{ij}\\beta_j)^2 + \\lambda\\sum_{j=1}^p \\beta_j^2$ where $\\lambda = \\sigma^2/\\tau^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "This follows from the solution to exercise 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.8\n",
    "\n",
    "Consider the $QR$ decomposition of the uncentered $N \\times (p + 1)$ matrix $\\mathbf{X}$ (whose first column is all ones), and the SVD of the $N \\times p$ centered matrix $\\tilde{\\mathbf{X}}$. Show that $Q_2$ and $U$ span the same subspace, where $Q_2$ is the sub-matrix of $Q$ with the first column removed. Under what circumstances will they be the same, up to sign flips?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "For $j=1,\\ldots,p$, the Gram-Schmidt algorithm implies\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{x}_j \n",
    "        &  = \\mathbf{z}_j + \\sum_{k=0}^{j-1}\\hat{\\gamma}_{kj}\\mathbf{z}_k \\\\\n",
    "    \\Rightarrow\\mathbf{x}_j -\\hat{\\gamma}_{0j}\\mathbf{z}_0 \n",
    "        & = \\mathbf{z}_j + \\sum_{k=1}^{j-1}\\hat{\\gamma}_{kj}\\mathbf{z}_k.\n",
    "\\end{align}\n",
    "\n",
    "By definition,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\gamma}_{0j} \n",
    "        = \\frac{\\langle\\mathbf{z}_0,\\mathbf{x}_j\\rangle}{\\langle\\mathbf{z}_0,\\mathbf{z}_0\\rangle}\n",
    "        = \\frac{1}{N}\\sum_{k=1}^Nx_{jk}\n",
    "\\end{equation}\n",
    "\n",
    "equals the mean of $\\mathbf{x}_j$; that is, the $j$th column $\\tilde{\\mathbf{x}}_j$ of $\\tilde{\\mathbf{X}}$. Since $\\mathbf{z}_1,\\ldots,\\mathbf{z}_p$ are scalar multiples of the columns of $Q_2$ this implies that the column space of $Q_2$ contains the column space of $\\tilde{\\mathbf{X}}$. By dimensions they are equal. The column space of $U$ equals that of $\\tilde{\\mathbf{X}}$ by definition and the first claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the second part, let $R_2$ be the $p\\times p$ submatrix of $R$ obtained by removing its first row and column. Observe that $\\tilde{\\mathbf{X}}=Q_2R_2$ is the $QR$ decomposition of $\\tilde{\\mathbf{X}}$, we can forget $\\mathbf{X}$ and work entirely with the SVD and $QR$ decompositions of $\\tilde{\\mathbf{X}}$.\n",
    "\n",
    "We claim that the columns of $Q_2$ equal those of $U$ up to sign flips if and only if $\\tilde{\\mathbf{X}}$ (equivalently $\\mathbf{X}$) has orthogonal columns.\n",
    "\n",
    "First assume that $\\tilde{\\mathbf{X}}$ has orthogonal columns. Take a permutation $\\sigma\\in S_p$ such that $\\lVert\\tilde{\\mathbf{x}}_{\\sigma(1)}\\rVert\\geq\\cdots\\geq\\lVert\\tilde{\\mathbf{x}}_{\\sigma(p)}\\rVert$. Let $P=P_{\\sigma}$ be the corresponding permutation matrix whose $j$th column is the standard basis vector $e_{\\sigma(j)}$ so that $\\tilde{\\mathbf{X}}P$ has columns $\\tilde{\\mathbf{x}}_{\\sigma(1)},\\ldots ,\\tilde{\\mathbf{x}}_{\\sigma(p)}$. Let $D$ be the diagonal matrix with $j$th diagonal entry $\\lVert \\tilde{\\mathbf{x}}_{\\sigma(j)}\\rVert$, let $U=\\tilde{\\mathbf{X}}PD^{-1}$, and let $V = P$. \n",
    "\n",
    "We claim that $\\tilde{\\mathbf{X}}=UDV^T$ is the SVD of $\\tilde{\\mathbf{X}}$. Indeed, since the columns $\\tilde{\\mathbf{X}}$ are orthogonal, the columns of $U$ are orthonormal and so $U$ and $V$ are orthogonal matrices. Moreover, if $j\\in\\{1,\\ldots,p\\}$ then $\\tilde{\\mathbf{X}}v_j$ is the $j$th column of $\\tilde{\\mathbf{X}}P=(\\tilde{\\mathbf{X}}PD^{-1})D$ so equals $d_ju_j$. Finally, $\\tilde{\\mathbf{X}}^Tu_j$ equals the $j$th column of $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}PD^{-1}$. Since $U$ is orthogonal,\n",
    "\n",
    "\\begin{equation}\n",
    "    U^TU=I\n",
    "        \\quad\\Rightarrow\\quad D^{-1}P^T\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}PD^{-1}=I\n",
    "        \\quad\\Rightarrow\\quad \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}PD^{-1}=PD,\n",
    "\\end{equation}\n",
    "\n",
    "so $\\tilde{\\mathbf{X}}^Tu_j=d_jv_j$. Thus $U$ and $V$ consist of left- and right-singular vectors of $\\tilde{\\mathbf{X}}$ respectively and $\\tilde{\\mathbf{X}}=UDV^T$ is its SVD.\n",
    "\n",
    "It just remains to note that since the columns of $\\tilde{\\mathbf{X}}$ are orthogonal its $QR$-decomposition consists of $Q_2=\\tilde{\\mathbf{X}}D^{-1}$ and $R_2=D$. Observe that $Q_2$ and $U$ have the same columns up to permutation.\n",
    "\n",
    "For the converse, assume that $Q_2$ and $U$ have the same columns up to permutation and sign. Then there exists a permutation matrix $P=P_{\\sigma}$ and a diagonal matrix $S$ with $\\pm1$ on the diagonal such that $U=Q_2PS$. So\n",
    "\n",
    "\\begin{equation}\n",
    "    Q_2R_2=UDV^T\n",
    "        \\qquad\\Rightarrow\\qquad Q_2R_2=Q_2PSDV^T\n",
    "        \\qquad\\Rightarrow\\qquad R_2 = PSDV^T,\n",
    "\\end{equation}\n",
    "\n",
    "since $Q_2$ is orthogonal so has a left-inverse. This implies that\n",
    "\n",
    "\\begin{equation}\n",
    "    R_2^TR_2 = (SPDV^T)(VDP^TS)=SPD^2P^TS=D^2,\n",
    "\\end{equation}\n",
    "\n",
    "so $R_2$ is upper triangular with orthogonal columns and hence diagonal. Therefore $\\tilde{\\mathbf{X}}=Q_2R_2$ has orthogonal columns.\n",
    "\n",
    "A corollary of our proof is the following: the columns of $Q_2$ and $U$ are equal *with the same ordering* if and only if $\\tilde{\\mathbf{X}}$ is orthogonal with $\\lVert\\tilde{\\mathbf{x}}_{1}\\rVert\\geq\\cdots\\geq\\lVert\\tilde{\\mathbf{x}}_{p}\\rVert$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.9\n",
    "\n",
    "*Forward stepwise regression.* Suppose we have the $QR$ decomposition for the $N \\times q$ matrix $\\mathbf{X}_1$ in a multiple regression problem with response $\\mathbf{y}$, and we have an additional $pq$ predictors in the matrix $\\mathbf{X}_2$. Denote the current residual by $\\mathbf{r}$. We wish to establish which one of these additional variables will reduce the residual-sum-of squares the most when included with those in $\\mathbf{X}_1$. Describe an efficient procedure for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let $\\mathbf{X}_1=\\mathbf{Q}_1\\mathbf{R}_1$ be the QR decomposition of $\\mathbf{X}_1$ and let $\\mathbf{q}_0, \\ldots, \\mathbf{q}_q$ denote the columns of $\\mathbf{X}_1$. The current residual is $\\mathbf{r} = \\mathbf{y} - \\mathbf{Q}_1\\mathbf{Q}_1^T\\mathbf{y}$ so the residual sum of squares is\n",
    "\n",
    "\\begin{align}\n",
    "    \\lVert \\mathbf{y} - \\mathbf{Q}_1\\mathbf{Q}_1^T\\mathbf{y} \\rVert^2\n",
    "        & = \\lVert \\mathbf{y} - \\sum_{j=0}^q \\mathbf{q}_j \\mathbf{q}_j^T \\mathbf{y}\\rVert ^2 \\\\\n",
    "        & = \\lVert \\mathbf{y} \\rVert^2 \n",
    "            - \\sum_{j=0}^q 2\\langle \\mathbf{y}, \\mathbf{q}_j\\mathbf{q}_j^T\\mathbf{y}\\rangle\n",
    "            + \\sum_{j=0}^q\\sum_{k=0}^q \\langle \\mathbf{q}_j\\mathbf{q}_j^T\\mathbf{y}, \\mathbf{q}_k\\mathbf{q}_k^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\lVert \\mathbf{y} \\rVert^2 \n",
    "            - \\sum_{j=0}^q 2\\langle \\mathbf{q}_j^T\\mathbf{y}, \\mathbf{q}_j^T\\mathbf{y}\\rangle\n",
    "            + \\sum_{j=0}^q\\sum_{k=0}^q \\langle \\mathbf{q}_j^T\\mathbf{y}, \\mathbf{q}_j^T\\mathbf{q}_k\\mathbf{q}_k^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\lVert \\mathbf{y} \\rVert^2 \n",
    "            - \\sum_{j=0}^q \\langle \\mathbf{q}_j^T\\mathbf{y}, \\mathbf{q}_j^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\lVert \\mathbf{y} \\rVert^2 \n",
    "            - \\sum_{j=0}^q ( \\mathbf{q}_j^T\\mathbf{y})^2.\n",
    "\\end{align}\n",
    "\n",
    "Adding another variable corresponds to adding another column $\\mathbf{q}$ to $\\mathbf{Q}_1$. Since $\\mathbf{q}$ is a normalised version of the Gram-Schmidt vector $\\mathbf{z}$, we can choose which variable to include as follows: for each column $\\mathbf{x}$ of $\\mathbf{X}_2$, calculate\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{z} = \\mathbf{x}-\\sum_{k=0}^q \\langle \\mathbf{q}_k, \\mathbf{x}\\rangle\\mathbf{q}_k\n",
    "\\end{equation}\n",
    "\n",
    "and pick the variable for which $\\lvert\\mathbf{z}^T\\mathbf{y}\\rvert\\big/ \\lVert\\mathbf{z}\\rVert$ is maximal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it follows relatively easy, we deduce a full procedure for forward-stepwise regression. This is implemented in `stepsel.py`.\n",
    "\n",
    "We run a loop for $k=0, \\ldots, p$. At the start of step $k$ assume that we have:\n",
    "- $\\mathcal{A}$: a set of $k$ active variables\n",
    "- $RSS$: the residual sum-of-squares at the end of the previous step\n",
    "- $\\mathbf{B}$: a $(p+1)\\times k$ matrix whose $j$th column gives the approximation to $\\beta$ at the end of the $j$th step\n",
    "- $\\mathbf{Q}$: an $N\\times k$ matrix that gives the $QR$-decomposition for $\\mathbf{X}$ restricted to the active columns\n",
    "- $\\mathbf{R}_{\\text{inv}}$: a $(p+1)\\times k$ matrix whose restriction to its $k$ non-zero rows gives the $\\mathbf{R}^{-1}$ in the QR-decomposition described above.\n",
    "\n",
    "In the $k$th step let $\\mathbf{M}=\\mathbf{I}_{p+1} - \\mathbf{R}_{\\text{inv}}\\mathbf{Q}^T\\mathbf{X}$ and consider\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}\\mathbf{M} \n",
    "        = \\mathbf{X} - \\mathbf{X}\\mathbf{R}_{\\text{inv}}\\mathbf{Q}^T\\mathbf{X}\n",
    "        = \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^T\\mathbf{X}.\n",
    "\\end{equation}\n",
    "\n",
    "The $j$th column of this is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}_j-\\sum_{a\\in\\mathcal{A}} \\langle \\mathbf{q}_a, \\mathbf{x}_j\\rangle\\mathbf{q}_a\n",
    "\\end{equation}\n",
    "\n",
    "and so right multiplication by $\\mathbf{M}$ gives the orthogonal projection of $\\mathbf{X}$ onto the space spanned by its active columns. In particular, the columns of $\\mathbf{X}\\mathbf{M}$ indexed by the active set are zero.\n",
    "\n",
    "Rescale the columns of $\\mathbf{M}$ such that the columns of $\\mathbf{X}\\mathbf{M}$ have unit norm. Let $j$ be the index at which the element-wise square of $(\\mathbf{X}\\mathbf{M})^T\\mathbf{y}$ is greatest and let $m$ denote this maximal value. By the solution to Exercise 3.9 described above, $j$ is the index that will decrease the $RSS$ the most and $m$ is the amount it will decrease by. So, at the end of this step we add $j$ to the active set, subtract $m$ from $RSS$, and append the $j$th columns of $\\mathbf{M}$ and $\\mathbf{X}\\mathbf{M}$ to $\\mathbf{R}_{\\text{inv}}$ and $\\mathbf{Q}$ respectively. The new approximation to $\\beta$ is $\\mathbf{R}_{\\text{inv}}\\mathbf{Q}^T\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.10\n",
    "\n",
    "*Backward stepwise regression.* Suppose we have the multiple regression fit of $\\mathbf{y}$ on $\\mathbf{X}$, along with the standard errors and $Z$-scores as in Table 3.2. We wish to establish which variable, when dropped, will increase the residual sum-of-squares the least. How would you do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "As in the defintion of the $F$ statistic (3.13), let $RSS_1$ denote the residual sum of squares for the existing model and $RSS_0$ the same quantity for the model with one variable removed. We wish to choose the variable which minimises $RSS_0$. This is equivalent to minimising the $F$ statistic\n",
    "\n",
    "\\begin{equation}\n",
    "    F=\\frac{RSS_0-RSS_1}{RSS_1 \\big/ (N-p-1)}\n",
    "\\end{equation}\n",
    "\n",
    "or its square root. But by exercise 3.1 $\\sqrt{F}$ equals the $Z$-score. Thus eliminating the variable with the lowest $Z$-score will increase the residual sum of squares the least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now describe an efficient full procedure for backward-stepwise selection. This is implemented in `stepsel.py`.\n",
    "\n",
    "We run a loop for $k=0,\\ldots,p$. At the start of the $k$th step we have:\n",
    "- $\\mathcal{A}$: a set of $p+1-k$ active variables\n",
    "- $RSS$: the residual sum-of-squares at the end of the previous step\n",
    "- $\\mathbf{B}$: a $(p+1)\\times k$ matrix whose $j$th column gives the approximation to $\\beta$ at the start of the $j$th step\n",
    "- $\\mathbf{Q}$: an $N\\times (p+1-k)$ matrix that gives the $QR$-decomposition for $\\mathbf{X}$ restricted to the active columns\n",
    "- $\\mathbf{R}_{\\text{inv}}$: a $(p+1-k)\\times (p+1-k)$ matrix that gives the $\\mathbf{R}^{-1}$ in the QR-decomposition described above.\n",
    "\n",
    "At the start, $\\mathcal{A}=\\{0, \\ldots, p\\}$, $\\mathbf{B}$ is empty, $\\mathbf{Q}$ and $\\mathbf{R}_{\\text{inv}}$ come from the $QR$ decomposition of $\\mathbf{X}$, and\n",
    "\n",
    "\\begin{equation}\n",
    "    RSS = \\lVert \\mathbf{y}-\\hat{\\mathbf{y}}\\rVert^2\n",
    "        = \\lVert\\mathbf{y}-\\mathbf{Q}\\mathbf{Q}^T\\mathbf{y}\\rVert^2\n",
    "        =\\mathbf{y}^T (\\mathbf{I}_N - \\mathbf{Q}\\mathbf{Q}^T)^2\\mathbf{y}.\n",
    "\\end{equation}\n",
    "\n",
    "In the $k$th step we calculate the current approximation $\\mathbf{R}_{\\text{\\inv}}\\mathbf{Q}^T\\mathbf{y}$ to $\\beta$ and add it as the last column of $\\mathbf{B}$. Then we examine the $Z$-scores to determine which feature to remove. We can ignore the constant $\\hat{\\sigma}$. Moreover, $v_j$ is the $j$th diagonal element of $\\mathbf{R}_{\\text{\\inv}} \\mathbf{R}_{\\text{\\inv}}^T$, which equals the square norm of the $j$th row of $\\mathbf{R}_{\\text{\\inv}}$. Thus it suffices to examine $\\hat{\\beta}$ (the last column of $\\mathbf{B}$) divided element-wise by the row norms of $\\mathbf{R}_{\\text{\\inv}}$ and remove the index $j$ with the smallest value.\n",
    "\n",
    "We then calculate the new residual sum-of-squares $RSS + (\\mathbf{q}_j^T\\mathbf{y})^2$, remove the $j$th column and the $j$th row and column from $\\mathbf{Q}$ and $\\mathbf{R}_{\\text{\\inv}}$ respectively, and remove $j$ from $\\mathcal{A}$.\n",
    "\n",
    "By keeping track of things as we go we end with the list of variables in the order they were removed, the approximation to $\\beta$ at each step, and the $RSS$ at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.11\n",
    "\n",
    "Show that the solution to the multivariate linear regression problem (3.40) is given by (3.39). What happens if the covariance matrices $\\Sigma_i$ are different for each observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We wish the find the minimum of\n",
    "\n",
    "\\begin{equation}\n",
    "    RSS(\\mathbf{B};\\Sigma) = \\sum_{i=1}^N(y_i-f(x_i))^T\\Sigma^{-1}(y_i-f(x_i)).\n",
    "\\end{equation}\n",
    "\n",
    "Let $a_{ij}$ denote the $(i,j)$ entry of $\\Sigma^{-1}$. Note that since $\\Sigma$ is a covariance matrix, both it and its inverse are symmetric. We can expand $RSS$ into\n",
    "\n",
    "\\begin{equation}\n",
    "    RSS=\\sum_{i=1}^N\\sum_{k=1}^K\\sum_{l=1}^K(y_{ik}-f_k(x_i))a_{kl}(y_{il}-f_l(x_i)),\n",
    "\\end{equation}\n",
    "\n",
    "where $f_k(x_i)=\\sum_{j=0}^p x_{ij}\\beta_{jk}$ is the $(i,k)$ entry of $\\mathbf{X}\\mathbf{B}$. This implies that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial RSS}{\\partial \\beta_{mn}}\n",
    "         = -2\\sum_{i=1}^N x_{im}\\left(\\sum_{k=1}^K (y_{ik}-f_k(x_i))a_{kn}\\right) \\\\\n",
    "    \\Rightarrow \\frac{\\partial RSS}{\\partial \\mathbf{B}}\n",
    "        = -\\mathbf{X}^T(\\mathbf{Y}-\\mathbf{X}\\mathbf{B})\\Sigma^{-1}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $RSS$ is bounded below and quadratic in $\\mathbf{B}$ it has a global minimum. Setting $\\frac{\\partial RSS}{\\partial \\mathbf{B}}=0$ gives the unique solution $\\hat{\\mathbf{B}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$ (assuming $\\mathbf{X}$ has full column rank). So the solution is the same as $K$ uncorrelated regressions.\n",
    "\n",
    "If the covariance matrices $\\Sigma_i$ are different for different observations then we need to minimise\n",
    "\n",
    "\\begin{equation}\n",
    "    RSS(\\mathbf{B};\\Sigma) = \\sum_{i=1}^N(y_i-f(x_i))^T\\Sigma_i^{-1}(y_i-f(x_i)).\n",
    "\\end{equation}\n",
    "\n",
    "Let $a^{(i)}_{jk}$ denote the $(j,k)$ entry of $\\Sigma_i^{-1}$. Then \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial RSS}{\\partial \\beta_{mn}}\n",
    "         = -2\\sum_{i=1}^N x_{im}\\left(\\sum_{k=1}^K (y_{ik}-f_k(x_i))a^{(i)}_{kn}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "If $\\mathbf{Z}$ denotes the matrix whose $i$th row is the $i$th row of $(\\mathbf{Y}-\\mathbf{X}\\mathbf{B})\\Sigma^{-1}$ then $\\frac{\\partial RSS}{\\partial \\mathbf{B}}=-\\mathbf{X}^T\\mathbf{Z}$. Again, $RSS$ must have a global minimum at which $\\mathbf{X}^T\\mathbf{Z}=0$. Since this is a system of linear equations in the $\\beta_{ij}$ it's easy to find the solutions in practice but I cannot find a closed-form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.12\n",
    "\n",
    "Show that the ridge regression estimates can be obtained by ordinary least squares regression on an augmented data set. We augment the centered matrix $\\mathbf{X}$ with $p$ additional rows $\\sqrt{\\lambda}\\mathbf{I}$, and augment $\\mathbf{y}$ with $p$ zeros. By introducing artificial data having response value zero, the fitting procedure is forced to shrink the coefficients toward zero. This is related to the idea of hints due to Abu-Mostafa (1995), where model constraints are implemented by adding artificial data examples that satisfy them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let $\\tilde{\\mathbf{X}}$ and $\\tilde{\\mathbf{y}}$ denote the augmented matrix and vector, respectively. Without loss of generality we assume that the new rows have been added at the bottom. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}} = \\mathbf{X}^T\\mathbf{X} + (\\sqrt{\\lambda}\\mathbf{I}_p)^T(\\sqrt{\\lambda}\\mathbf{I}_p) = \\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I}_p\n",
    "\\end{equation}\n",
    "\n",
    "and $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{y}} = \\mathbf{X}^T\\mathbf{y}$, so the least squares estimate using $\\tilde{\\mathbf{X}}$ and $\\tilde{\\mathbf{y}}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "    (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{y}}\n",
    "         = (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}_p)^{-1}\\mathbf{X}^T\\mathbf{y};\n",
    "\\end{equation}\n",
    "\n",
    "the ridge regression estimate for the non-augmented data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.13\n",
    "\n",
    "Derive the expression (3.62), and show that $\\hat{\\beta}^{\\text{pcr}}(p)=\\hat{\\beta}^{\\text{ls}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\n",
    "For the first part observe that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}}^{\\text{pcr}}=\\sum_{m=1}^M \\hat{\\theta}_m\\mathbf{z}_m=\\sum_{m=1}^M \\hat{\\theta}_m\\mathbf{X}v_m=\\mathbf{X}\\left(\\sum_{m=1}^M \\hat{\\theta}_mv_m\\right),\n",
    "\\end{equation}\n",
    "\n",
    "so $\\hat{\\beta}^{\\text{pcr}}(M) = \\sum_{m=1}^M \\hat{\\theta}_mv_m$. The second part follows from the fact that $\\mathbf{X}$ and $\\mathbf{Z}=\\mathbf{U}\\mathbf{D}$ have the same column space, but we can also show it explicitly. Using the SVD decomposition $\\mathbf{X}=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T$,\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\beta}^{\\text{ls}}\n",
    "        & = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "        & = (\\mathbf{V}\\mathbf{D}^2\\mathbf{V}^T)^{-1}\\mathbf{V}\\mathbf{D}\\mathbf{U}^T\\mathbf{y} \\\\\n",
    "        & = \\mathbf{V}\\mathbf{D}^{-1}\\mathbf{U}^T\\mathbf{y} \\\\\n",
    "        & = \\mathbf{V}\\mathbf{D}^{-2}\\mathbf{Z}^T\\mathbf{y} \\\\\n",
    "        & = \\sum_{m=1}^p \\frac{\\langle\\mathbf{z}_m,\\mathbf{y}\\rangle}{d_m^2}v_m.\n",
    "\\end{align}\n",
    "\n",
    "We're done since $\\lVert\\mathbf{z}_m\\rVert = d_m^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.14\n",
    "\n",
    "Show that in the orthogonal case, PLS stops after $m = 1$ steps, because subsequent $\\varphi_{mj}$ in step 2 in Algorithm 3.3 are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "It suffices to show that $\\langle\\mathbf{x}_j^{(1)},\\mathbf{y}\\rangle=0$ for all $j$, or equivalently that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\langle\\mathbf{z}_0,\\mathbf{x}_j\\rangle}{\\langle\\mathbf{z}_0,\\mathbf{z}_0\\rangle}\\langle\\mathbf{z}_0,\\mathbf{y}\\rangle=\\langle \\mathbf{x}_j,\\mathbf{y}\\rangle.\n",
    "\\end{equation}\n",
    "\n",
    "Using the fact that the $\\mathbf{x}_j$ are orthonormal (they have norm one since they are standardised with mean zero and unit variance) we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\langle\\mathbf{z}_0,\\mathbf{x}_j\\rangle\n",
    "         = \\langle \\mathbf{x}_j,\\mathbf{y}\\rangle\\lVert \\mathbf{x}_j\\rVert^2\n",
    "          = \\langle \\mathbf{x}_j,\\mathbf{y}\\rangle, \\\\\n",
    "    \\langle\\mathbf{z}_0,\\mathbf{z}_0\\rangle\n",
    "         = \\sum_{k=1}^p \\langle\\mathbf{x}_k,\\mathbf{y}\\rangle^2\\lVert \\mathbf{x}_k\\rVert^2\n",
    "          = \\sum_{k=1}^p \\langle\\mathbf{x}_k,\\mathbf{y}\\rangle^2, \\\\\n",
    "    \\langle\\mathbf{z}_0,\\mathbf{y}\\rangle\n",
    "         = \\sum_{k=1}^p \\langle\\mathbf{x}_k,\\mathbf{y}\\rangle^2.\n",
    "\\end{equation}\n",
    "\n",
    "The claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.15\n",
    "\n",
    "Verify expression (3.64), and hence show that the partial least squares directions are a compromise between the ordinary regression coefficient and the principal component directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We start with $m=1$. Note that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Corr}^2(\\mathbf{y},\\mathbf{X}\\alpha)\\text{Var}(\\mathbf{X}\\alpha)\n",
    "        = \\frac{\\text{Cov}^2(\\mathbf{y},\\mathbf{X}\\alpha)}{\\text{Var}(\\mathbf{y})\\text{Var}(\\mathbf{X}\\alpha)}\\text{Var}(\\mathbf{X}\\alpha),\n",
    "\\end{equation}\n",
    "\n",
    "so maximising this over $\\lVert\\alpha\\rVert=1$ is equivalent to maximising $\\mathbf{y}^T\\mathbf{X}\\alpha$. We have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{y}^T\\mathbf{X}\\alpha\n",
    "        = \\langle\\mathbf{X}^T\\mathbf{y}, \\alpha\\rangle\n",
    "        = \\lVert \\mathbf{X}^T\\mathbf{y}\\rVert\\lVert\\alpha\\rVert\\cos(\\theta),\n",
    "\\end{equation}\n",
    "\n",
    "so this is maximised at $\\alpha = \\mathbf{X}^T\\mathbf{y}\\big/\\lVert\\mathbf{X}^T\\mathbf{y}\\rVert$, which equals $\\hat{\\varphi}_1$ up to a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take $m>1$. The sample covariance matrix is $\\mathbf{S}=\\frac{1}{N}\\mathbf{X}^T\\mathbf{X}$ so we need to maximise $\\mathbf{y}^T\\mathbf{X}\\alpha$ subject to $\\lVert\\alpha\\rVert=1$ and $\\langle \\mathbf{X}\\alpha,\\mathbf{z}_l\\rangle=0$ for $l<m$. Let $\\pi$ denote projection from $\\mathbb{R}^p$ onto the subspace spanned by $\\{\\mathbf{z}_1,\\ldots,\\mathbf{z}_{m-1}\\}$, where $\\mathbf{z}_l=\\mathbf{X}\\hat{\\varphi}_l$. Note that $(1-\\pi)(\\mathbf{x}_j)=\\mathbf{x}_j^{(m-1)}$ for all $j$. \n",
    "\n",
    "The condition $\\langle \\mathbf{X}\\alpha,\\mathbf{z}_l\\rangle=0$ for $l<m$ is equivalent to $(1-\\pi)(\\mathbf{X}\\alpha)=\\mathbf{X}\\alpha$. However, \n",
    "\n",
    "\\begin{equation}\n",
    "    (1-\\pi)\\left(\\mathbf{X}\\alpha\\right)\n",
    "        = (1-\\pi)\\left(\\sum_{j=1}^p \\alpha_j\\mathbf{x}_j\\right)\n",
    "        = \\sum_{j=1}^p \\alpha_j\\mathbf{x}^{(m-1)}_j\n",
    "        = \\mathbf{X}^{(m-1)}\\alpha,\n",
    "\\end{equation}\n",
    "\n",
    "where $X^{(m-1)}$ is the matrix whose $j$th column is $\\mathbf{x}^{(m-1)}_j$, so the orthogonality condition holds iff $\\mathbf{X}\\alpha = \\mathbf{X}^{(m-1)}\\alpha$. If so then \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{y}^T\\mathbf{X}\\alpha\n",
    "        = \\mathbf{y}^T\\mathbf{X}^{(m-1)}\\alpha\n",
    "        = \\langle(\\mathbf{X}^{(m-1)})^T\\mathbf{y}, \\alpha\\rangle\n",
    "        = \\lVert (\\mathbf{X}^{(m-1)})^T\\mathbf{y}\\rVert\\lVert\\alpha\\rVert\\cos(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Clearly this is maximised at $\\alpha = (\\mathbf{X}^{(m-1)})^T\\mathbf{y} \\big/ \\big\\lVert(\\mathbf{X}^{(m-1)})^T\\mathbf{y}\\big\\rVert$, which equals $\\hat{\\varphi}_m$ up to a scalar.\n",
    "\n",
    "Note that as a corollary we have $\\mathbf{z}_m=\\sum_j \\hat{\\varphi}_{mj}\\mathbf{x}^{(m-1)}_j=\\sum_j \\hat{\\varphi}_{mj}\\mathbf{x}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part, note that the principal component directions maximise $\\text{Var}(\\mathbf{X}\\alpha)$ and the partial least squares directions maximise $\\text{Corr}^2(\\mathbf{y},\\mathbf{X}\\alpha)\\text{Var}(\\mathbf{X}\\alpha)$. We claim that the ordinary regression coefficient $\\hat{\\beta}$ maximises $\\text{Corr}^2(\\mathbf{y},\\mathbf{X}\\beta)$ and hence PLS is a compromise between the other two.\n",
    "\n",
    "It suffices to maximise the log\n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\ln \\left(\\text{Corr}^2(\\mathbf{y},\\mathbf{X}\\beta)\\right)\n",
    "      = 2\\ln\\langle\\mathbf{y},\\mathbf{X}\\beta\\rangle - \\ln\\langle y,y\\rangle - \\ln \\langle\\mathbf{X}\\beta,\\mathbf{X}\\beta\\rangle.\n",
    "\\end{equation}\n",
    "\n",
    "This has partial derivative\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C}{\\partial \\beta} = 2\\left[ \\frac{\\mathbf{X}^T\\mathbf{y}}{\\langle\\mathbf{y},\\mathbf{X}\\beta\\rangle} - \\frac{\\mathbf{X}^T\\mathbf{X}\\beta}{\\langle\\mathbf{X}\\beta,\\mathbf{X}\\beta\\rangle}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Setting this to zero yields\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\langle\\mathbf{y},\\mathbf{X}\\beta\\rangle}{\\langle\\mathbf{X}\\beta,\\mathbf{X}\\beta\\rangle} \\beta = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}=\\hat{\\beta},\n",
    "\\end{equation}\n",
    "\n",
    "so $\\beta$ must be a scalar multiple of $\\hat{\\beta}$. In fact any non-zero multiple satisfies the equation.\n",
    "\n",
    "It remains to show that this is a maximum. However, note that $\\text{Corr}^2(\\mathbf{y},\\mathbf{X}\\beta)$ is invariant under scaling of $\\beta$. On the closed, bounded set $\\{\\beta\\mid\\lVert\\beta\\rVert=1\\}$ the function must have both a maximum and a minimum. Thus the extremum $\\beta=\\hat{\\beta}$ we found must be a global maximum (the minimum - zero - didn't show up in our analysis as $C$ is undefined when the correlation is zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.16\n",
    "\n",
    "Derive the entries in Table 3.4, the explicit forms for estimators in the orthogonal case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Assume that the columns of $\\mathbf{X}$ are orthonormal. We go through the different methods in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Best subset*: First note that if a variable $\\mathbf{x}_j$ is included in the subset then its coefficient $\\hat{\\beta}_j=\\langle\\mathbf{x}_j,\\mathbf{y}\\rangle$ is independent of what other variables are included. Take $S\\subseteq\\{0,\\ldots,p\\}$ and let $\\beta=(\\beta_j)$ where $\\beta_j=\\hat{\\beta}_j$ if $j\\in S$ and zero otherwise. Then\n",
    "\n",
    "\\begin{align}\n",
    "    \\lVert \\mathbf{y}-\\mathbf{X}^T\\beta\\rVert^2\n",
    "        & = \\big\\lVert \\mathbf{y}-\\sum_{j\\in S}\\beta_j\\mathbf{x}_j\\big\\rVert^2 \\\\\n",
    "        & = \\lVert\\mathbf{y}\\rVert^2 \n",
    "            - 2\\sum_{j\\in S}\\langle\\mathbf{x}_j,\\mathbf{y}\\rangle^2 \n",
    "            + \\sum_{j,k\\in S}\\langle\\mathbf{x}_j,\\mathbf{y}\\rangle \\langle\\mathbf{x}_k,\\mathbf{y}\\rangle \\langle\\mathbf{x}_j,\\mathbf{x}_k\\rangle\\\\\n",
    "        & = \\lVert\\mathbf{y}\\rVert^2 - \\sum_{j\\in S}\\langle\\mathbf{x}_j,\\mathbf{y}\\rangle^2.\n",
    "\\end{align}\n",
    "\n",
    "So the best subset of size $M$ consists of the $M$ indices with the greatest $\\lvert \\hat{\\beta}_j\\rvert$. This implies the claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ridge*: If $X$ has orthonormal columns then $\\mathbf{X}^T\\mathbf{X}=\\mathbf{I}$ so the ridge estimate is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta=(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}=\\frac{1}{1+\\lambda}\\hat{\\beta}\n",
    "\\end{equation}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lasso*: Consider the LAR path. Take the variable $\\mathbf{x}_j$ with the greatest absolute correlation with $\\mathbf{y}$. We begin moving along the path $\\hat{\\mathbf{f}}(\\alpha)=\\alpha \\langle\\mathbf{x}_j,\\mathbf{y}\\rangle\\mathbf{x}_j$ for $\\alpha\\in[0,1]$. Along this path the correlations with residuals are given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lvert\\langle\\mathbf{x}_k, \\mathbf{y}-\\hat{\\mathbf{f}}(\\alpha)\\rangle\\rvert = \n",
    "        \\begin{cases}\n",
    "            (1-\\alpha)\\lvert\\langle \\mathbf{x}_j,\\mathbf{y}\\rangle\\rvert & \\text{ if } j=k, \\\\\n",
    "            \\lvert\\langle \\mathbf{x}_k,\\mathbf{y}\\rangle\\rvert & \\text{ otherwise.}\n",
    "        \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Thus we continue along the current path until $(1-\\alpha)\\lvert\\langle \\mathbf{x}_j,\\mathbf{y}\\rangle\\rvert = \\lvert\\langle \\mathbf{x}_k,\\mathbf{y}\\rangle\\rvert$ for some $k\\neq j$ at which point we include $k$ in the active set. As above, the coefficient of $\\mathbf{x}_k$ will only affect the correlation of $\\mathbf{x}_k$ with the residual. Thus\n",
    "\n",
    "Thus the coefficient $\\beta_l$ of $\\mathbf{x}_l$ is zero until the shared absolute covariance $\\lambda$ is greater than $\\lvert\\langle\\mathbf{x}_l,\\mathbf{y}\\rangle\\rvert$, after which it increases at the same rate that $\\lambda$ increases. More precisely, when the shared covariance is $\\lambda$, the coefficient of $\\mathbf{x}_l$ is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_l = \n",
    "        \\begin{cases}\n",
    "            0 & \\text{ if } \\lambda > \\lvert\\langle\\mathbf{x}_l,\\mathbf{y}\\rangle\\rvert, \\\\\n",
    "            \\text{sign}(\\langle\\mathbf{x}_l,\\mathbf{y}\\rangle \\left(\\lvert\\langle \\mathbf{x}_l,\\mathbf{y}\\rangle\\rvert - \\lambda\\right) & \\text{ otherwise.}\n",
    "        \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This agrees with the entry in Table 3.4. Indeed, since no coefficient in the LAR path ever hits zero after being added to the active set, it is identical to the Lasso path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.17\n",
    "\n",
    "Repeat the analysis of Table 3.3 on the spam data discussed in Chapter 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "This analysis can be found in `spam-data/spam-regression-analysis.ipynb`. The test errors were as follows:\n",
    "\n",
    "| Least-Squares | Forward-Stepwise | Ridge | Lasso | PCR | PLS |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 0.1160 | 0.1162 | 0.1141 | 0.1163 | 0.1196 | 0.1163 |\n",
    "\n",
    "Note that best subset couldn't be performed because there were too many featuresm, so forward-stepwise selection was substituted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.18\n",
    "\n",
    "Read about conjugate gradient algorithms (Murray et al., 1981, for example), and establish a connection between these algorithms and partial least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.19\n",
    "\n",
    "Show that $\\lVert \\hat{\\beta}^{\\text{ridge}}\\rVert$ increases as its tuning parameter $\\lambda\\rightarrow 0$. Does the same property hold for the lasso and partial least squares estimates? For the latter, consider the \"tuning parameter\" to be the successive steps in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "*Ridge*: Let\n",
    "\n",
    "\\begin{equation}\n",
    "    R(\\beta; \\lambda) = \\lVert\\mathbf{y}-\\mathbf{X}\\beta\\rVert^2 + \\lambda \\lVert\\beta\\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "Take $\\lambda>\\lambda^{\\prime}>0$ and let $\\hat{\\beta}$ and $\\hat{\\beta}^{\\prime}$ be the ridge approximations with tuning parameters $\\lambda$ and $\\lambda^{\\prime}$, respectively (so they minimise $R(\\beta; \\lambda)$ and $R(\\beta; \\lambda^{\\prime})$). Then by definition\n",
    "\n",
    "\\begin{equation}\n",
    "    R(\\hat{\\beta}; \\lambda)\\leq R(\\hat{\\beta}^{\\prime}; \\lambda)\n",
    "        \\qquad\\text{and}\\qquad R(\\hat{\\beta}^{\\prime}; \\lambda^{\\prime})\\leq R(\\hat{\\beta}; \\lambda^{\\prime}).\n",
    "\\end{equation}\n",
    "\n",
    "Subtract the second inequality form the first yields\n",
    "\n",
    "\\begin{align}\n",
    "    R(\\hat{\\beta}; \\lambda) - R(\\hat{\\beta}; \\lambda^{\\prime}) \n",
    "        & \\leq R(\\hat{\\beta}^{\\prime}; \\lambda)- R(\\hat{\\beta}^{\\prime}; \\lambda^{\\prime}) \\\\\n",
    "    (\\lambda-\\lambda^{\\prime})\\lVert\\hat{\\beta}\\rVert^2\n",
    "        & \\leq (\\lambda-\\lambda^{\\prime})\\lVert\\hat{\\beta}^{\\prime}\\rVert^2 \\\\\n",
    "    \\lVert\\hat{\\beta}\\rVert\n",
    "        & \\leq \\lVert\\hat{\\beta}^{\\prime}\\rVert,\n",
    "\\end{align}\n",
    "\n",
    "so the ridge approximation increases in $L^2$ norm as $\\lambda$ decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lasso*: The same proof as above shows that the lasso approximation to $\\beta$ increases in *$L^1$ norm* as $\\lambda$ decreases. However, this doesn't necessarily mean that it increases in $L^2$ norm. For example suppose that $p=2$ and assume the least squares approximation has $\\hat{\\beta}_1, \\hat{\\beta}_2>0$. If lasso modification of the LAR path went from $(0,0)$ to $(\\hat{\\beta}_1+\\hat{\\beta}_2,0)$ to $(\\hat{\\beta}_1, \\hat{\\beta}_2)$ then the $L^1$ norm of the coefficient vector would always be increasing as $\\lambda$ decreases, but the $L^2$ norm would decrease on the last segment.\n",
    "\n",
    "It remains to give an example to show this behaviour is possible. Take $p=2$ and $N=3$ - the minimal values for an example. Take\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}_1 =\n",
    "        \\begin{pmatrix}\n",
    "            -\\sqrt\\frac{3}{2} \\\\\n",
    "            0 \\\\\n",
    "            \\sqrt\\frac{3}{2} \\\\\n",
    "        \\end{pmatrix}, \\qquad\n",
    "    \\mathbf{x}_2 =\n",
    "        \\begin{pmatrix}\n",
    "            -\\sqrt\\frac{1}{2} \\\\\n",
    "            -\\sqrt\\frac{1}{2} \\\\\n",
    "            \\sqrt\\frac{1}{2} \\\\\n",
    "        \\end{pmatrix}, \\qquad\n",
    "    \\mathbf{y} =\n",
    "        \\begin{pmatrix}\n",
    "            -1 \\\\\n",
    "            0 \\\\\n",
    "            1 \\\\\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Note that all three vectors have mean zero and the first two have unit variance. Since\n",
    "\n",
    "\\begin{equation}\n",
    "    \\langle\\mathbf{x}_1,\\mathbf{y}\\rangle = \\sqrt{6} > \\sqrt{2} = \\langle\\mathbf{x}_2,\\mathbf{y}\\rangle,\n",
    "\\end{equation}\n",
    "\n",
    "the first step is in the direction of $\\mathbf{x}_1$. This ends at $\\frac{\\sqrt{2}}{11}(\\sqrt{3}+5)\\mathbf{x}_1$ since at this point the residual $\\mathbf{r}$ has equal correlation with $\\mathbf{x}_1$ and $\\mathbf{x}_2$ ($\\langle\\mathbf{x}_j,\\mathbf{r}\\rangle = 5\\sqrt{6}$ for $j=1, 2$). We then continue in a straight line in direction $\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$ to the least squares solution\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\beta} = \\frac{\\sqrt{2}}{11}\n",
    "        \\begin{pmatrix}\n",
    "            \\sqrt{3} \\\\\n",
    "            5\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "So although the coefficient path increases in $L^1$ norm it decreases in $L^2$ norm along the final stretch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PLS*: Let $\\hat{\\beta}^{(m)}$ denote the PLS approximation after $m$ steps. We claim that $\\lVert\\hat{\\beta}^{(m)}\\rVert$ increases as $m$ increases. For $m=0,\\ldots, p$ let $\\mathbf{X}^{(m)}$ denote the matrix whose $j$th column is $\\mathbf{x}_j^{(m)}$. The proof relies heavily on the following three facts:\n",
    "- $\\mathbf{z}_m = \\mathbf{X}^{(m-1)}\\hat{\\varphi}_m$;\n",
    "- $\\hat{\\varphi}_m = \\left(\\mathbf{X}^{(m-1)}\\right)^T\\mathbf{y}$;\n",
    "- $\\mathbf{X}\\hat{\\varphi}_m = \\mathbf{X}^{(l-1)}\\hat{\\varphi}_m$ for any $l\\leq m$.\n",
    "\n",
    "The first two of these are by definition. The last was essentially proved in the course of Exercise 3.15, but we quickly recall the proof here. By the equivalent characterisation of $\\hat{\\varphi}_m$ in Exercise 3.15, $\\mathbf{X}\\hat{\\varphi}_m$ is orthogonal to $\\mathbf{z}_1,\\ldots,\\mathbf{z}_{l-1}$. So if $\\pi$ is the projection onto the subspace they span then $(1-\\pi)\\left(\\mathbf{X}\\hat{\\varphi}_m\\right)=\\mathbf{X}\\hat{\\varphi}_m$. But by construction $(1-\\pi)(\\mathbf{x}_j) = \\mathbf{x}^{(l-1)}_j$ so \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}\\hat{\\varphi}_m \n",
    "        = (1-\\pi)\\left(\\mathbf{X}\\hat{\\varphi}_m\\right) \n",
    "        = (1-\\pi)\\left(\\sum_{j=1}^m\\hat{\\varphi}_{mj}\\mathbf{x}_j\\right) \n",
    "        = \\sum_{j=1}^m\\hat{\\varphi}_{mj}\\mathbf{x}^{(l-1)}_j \n",
    "        = \\mathbf{X}^{(l-1)}\\hat{\\varphi}_m.\n",
    "\\end{equation}\n",
    "\n",
    "Now we continue to the proof. First note that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}}^{(m)} \n",
    "        = \\sum_{l=1}^m \\hat{\\theta}_l\\mathbf{z}_l \n",
    "        = \\sum_{l=1}^m \\hat{\\theta}_l\\mathbf{X}^{(l-1)}\\hat{\\varphi}_l \n",
    "        = \\sum_{l=1}^m \\hat{\\theta}_l\\mathbf{X}\\hat{\\varphi}_l \n",
    "        = \\mathbf{X}\\left(\\sum_{l=1}^m \\hat{\\theta}_l\\hat{\\varphi}_l\\right),\n",
    "\\end{equation}\n",
    "\n",
    "so $\\hat{\\beta}^{(m)} = \\sum_{l=1}^m \\hat{\\theta}_l\\hat{\\varphi}_l$. We claim that $\\hat{\\theta}_l\\geq 0$ and $\\langle\\hat{\\varphi}_l,\\hat{\\varphi}_m\\rangle\\geq 0$ for all $l\\leq m$. Since\n",
    "\n",
    "\\begin{equation}\n",
    "    \\big\\lVert \\hat{\\beta}^{(m)}\\big\\rVert^2\n",
    "        = \\big\\lVert \\hat{\\beta}^{(m-1)}\\big\\rVert^2 \n",
    "            + \\sum_{l=1}^{m-1} \\hat{\\theta}_l\\hat{\\theta}_m\\langle\\hat{\\varphi}_l,\\hat{\\varphi}_m\\rangle \n",
    "            + \\hat{\\theta}_m^2\\lVert\\hat{\\varphi}_m\\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "this will establish that $\\big\\lVert \\hat{\\beta}^{(m)}\\big\\rVert$ increases with $m$.\n",
    "\n",
    "The first claim holds because by definition $\\hat{\\theta}_l = \\langle\\mathbf{y},\\mathbf{z}_l\\rangle \\big/ \\lVert\\mathbf{z}_l\\rVert^2$ and\n",
    "\n",
    "\\begin{align}\n",
    "    \\langle\\mathbf{y},\\mathbf{z}_l\\rangle\n",
    "        & = \\langle\\mathbf{y},\\mathbf{X}^{(l-1)}\\hat{\\varphi}_l\\rangle \\\\\n",
    "        & = \\langle\\mathbf{y},\\mathbf{X}^{(l-1)}\\left(\\mathbf{X}^{(l-1)}\\right)^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\langle\\left(\\mathbf{X}^{(l-1)}\\right)^T\\mathbf{y},\\left(\\mathbf{X}^{(l-1)}\\right)^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\big\\lVert \\hat{\\varphi}_l\\big\\rVert^2\\geq 0.\n",
    "\\end{align}\n",
    "\n",
    "For the second claim observe that if $l\\leq m$ then\n",
    "\n",
    "\\begin{align}\n",
    "    \\langle\\hat{\\varphi}_l,\\hat{\\varphi}_m\\rangle\n",
    "        & = \\langle\\left(\\mathbf{X}^{(l-1)}\\right)^T\\mathbf{y},\\hat{\\varphi}_m\\rangle \\\\\n",
    "        & = \\langle\\mathbf{y},\\mathbf{X}^{(l-1)}\\hat{\\varphi}_m\\rangle \\\\\n",
    "        & = \\langle\\mathbf{y},\\mathbf{X}^{(m-1)}\\hat{\\varphi}_m\\rangle \\\\\n",
    "        & = \\langle\\left(\\mathbf{X}^{(m-1)}\\right)^T\\mathbf{y},\\hat{\\varphi}_m\\rangle \\\\\n",
    "        & =  \\big\\lVert \\hat{\\varphi}_m\\big\\rVert^2\\geq 0\n",
    "\\end{align}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the canonical-correlation problem (3.67). Show that the leading pair of canonical variates $u_1$ and $v_1$ solve the problem\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{\\substack{v^T(\\mathbf{X}^T\\mathbf{X})v=1 \\\\ u^T(\\mathbf{Y}^T\\mathbf{Y})u=1}}{\\text{max}} u^T(\\mathbf{Y}^T\\mathbf{X})v,\n",
    "\\end{equation}\n",
    "\n",
    "a generalised SVD problem. Show that the solution is given by $u_1=(\\mathbf{Y}^T\\mathbf{Y})^{-\\frac{1}{2}}u_1^*$, and $v_1=(\\mathbf{X}^T\\mathbf{X})^{-\\frac{1}{2}}v_1^*$, where $u_1^*$ and $v_1^*$ are the leading left and right singular vectors in\n",
    "\n",
    "\\begin{equation}\n",
    "    (\\mathbf{Y}^T\\mathbf{Y})^{-\\frac{1}{2}}(\\mathbf{Y}^T\\mathbf{X})(\\mathbf{X}^T\\mathbf{X})^{-\\frac{1}{2}} = \\mathbf{U}^*\\mathbf{D}^*\\mathbf{V}^{*T}.\n",
    "\\end{equation}\n",
    "\n",
    "Show that the entire sequence $u_m$, $v_m$, $m=1,\\ldots,\\text{min}(K,p)$ is also given by (3.87)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The leading pair of canonical variates $u_1$ and $v_1$ must maximise:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Corr}^2(\\mathbf{Y}u,\\mathbf{X}v) = \\frac{u^T\\mathbf{Y}^T\\mathbf{X}v}{(u^T\\mathbf{Y}^T\\mathbf{Y}u)(v^T\\mathbf{X}^T\\mathbf{X}v)}.\n",
    "\\end{equation}\n",
    "\n",
    "Since any solution vectors $u$ and $v$ can be scaled so that the two quantities in the denominator equal one, the problem is equivalent to the generalised SVD problem given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take Cholesky decompositions $\\mathbf{X}^T\\mathbf{X}=\\mathbf{K}^T\\mathbf{K}$ and $\\mathbf{Y}^T\\mathbf{Y}=\\mathbf{L}^T\\mathbf{L}$ where $\\mathbf{K}$ and $\\mathbf{L}$ are square and invertible. We will prove a slightly modified version of the problem: $u_1=\\mathbf{L}^{-1}u_1^*$ and $v_1=\\mathbf{K}^{-1}v_1^*$, where the starred vectors are left and right singular vectors of $\\mathbf{M} = (\\mathbf{L}^T)^{-1}\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}^{-1}$. One can convince oneself that these two statements are essentially the same.\n",
    "\n",
    "We begin by performing a change of basis by writing $u^*=\\mathbf{L}u$ and $v^*=\\mathbf{K}v$. In this basis our goal is to maximise\n",
    "\n",
    "\\begin{equation}\n",
    "    u^T\\mathbf{Y}^T\\mathbf{X}\\mathbf{v} = (\\mathbf{L}u)^T(\\mathbf{L}^T)^{-1}\\mathbf{Y}\\mathbf{X}\\mathbf{K}^{-1}(\\mathbf{K}v) = u^{*T}\\mathbf{M}v^*\n",
    "\\end{equation}\n",
    "\n",
    "subject to $\\lVert u^*\\rVert=\\lVert v^*\\rVert=1$.\n",
    "\n",
    "First note that this problem has a maximum since $u^{*T}\\mathbf{M}v^*$ is a continuous function on the compact set $\\{u^*, v^* \\,\\big|\\, \\lVert u^*\\rVert=\\lVert v^*\\rVert=1\\}$. We will find it using Lagrange multipliers. Let\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(u^*, v^*, \\lambda, \\mu) = u^{*T}\\mathbf{M}v^T - \\lambda(u^{*T}u - 1) - \\mu(v^{*T}v - 1).\n",
    "\\end{equation}\n",
    "\n",
    "At a maximum of the constrained optimisation problem, \n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial u^*} & = \\mathbf{M}v^* - 2\\lambda u^* = 0 \\\\\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial v^*} & = \\mathbf{M}^Tu^* - 2\\mu v^* = 0 \\\\\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial \\lambda} & = u^{*T}u - 1 = 0 \\\\\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial \\mu} & = v^{*T}v - 1 = 0.\n",
    "\\end{align}\n",
    "\n",
    "Multiplying the first and second equations on the left by $u^{*T}$ and $v^{*T}$ respectively yields\n",
    "\n",
    "\\begin{equation}\n",
    "    u^{*T}\\mathbf{M}v^* = 2\\lambda \\qquad\\text{ and }\\qquad v^{*T}\\mathbf{M}^T u^* = 2\\mu,\n",
    "\\end{equation}\n",
    "\n",
    "which implies that $\\lambda = \\mu$. Moreover, this implies that $u^*$ and $v^*$ are a pair of singular vectors for $\\mathbf{M}$ with singular value $2\\lambda$. To attain the global maximum of $u^{*T}\\mathbf{M}v^* = 2\\lambda$ they are must be the first singular vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take $1<m\\leq \\text{min}(K,p)$ and suppose that we have shown that $u^*_1,\\ldots ,u^*_{m-1}$ and $v^*_1,\\ldots ,v^*_{m-1}$ are the $m-1$ first singular vectors of $\\mathbf{M}$. The pair $(u^*_m, v^*_m)$ are the solution to the following constrained optimisation problem: maximise $u_m^{*T}\\mathbf{M}v_m^*$ subject to $\\lVert u_m^*\\rVert=\\lVert v_m^*\\rVert=1$ and \n",
    "\n",
    "\\begin{equation}\n",
    "    u_m^{*T}u_k^* = u_m^{*T}v_k^* = v_m^{*T}v_k^* = v_m^{*T}u_k^* = 0\n",
    "\\end{equation}\n",
    "\n",
    "for $k<m$.\n",
    "\n",
    "We approach this problem much the same as above. First note that there is a maximum since we have a continuous function on a compact set. Now consider the function\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L} = & \\; u_m^{*T}\\mathbf{M}v_m^T - \\lambda(u_m^{*T}u_m - 1) - \\mu(v_m^{*T}v_m - 1) \\\\\n",
    "                    & + \\sum_{k=1}^{m-1} \\left( \\alpha_k^{YX}u_m^{*T}\\mathbf{M}v_k^* - \\alpha_k^{YY}u_m^{*T}u_k^* + \\alpha_k^{XY}v_m^{*T}\\mathbf{M}u_k^* -  \\alpha_k^{XX}v_m^{*T}v_k^*\\right)\n",
    "\\end{align}\n",
    "\n",
    "Setting $\\frac{\\partial \\mathcal{L}}{\\partial u^*_m} = \\frac{\\partial \\mathcal{L}}{\\partial v^*_m} = 0$ yields\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{k=1}^m \\alpha_k^{YX} \\mathbf{M}v_k^* = \\sum_{k=1}^m \\alpha_k^{YY} u_k^*\n",
    "        \\qquad\\text{and}\\qquad \\sum_{k=1}^m \\alpha_k^{XY} \\mathbf{M}u_k^* = \\sum_{k=1}^m \\alpha_k^{XX} v_k^*,\n",
    "\\end{equation}\n",
    "\n",
    "where we have written $\\alpha_m^{YX} = \\alpha_m^{XY} = 1$, $\\alpha_m^{YY} = 2\\lambda$, and $\\alpha_m^{XX} = 2\\mu$ for notational convenience. Setting the derivatives with respect to the parameters equal to zero yields the original constraints of the problem.\n",
    "\n",
    "Multiplying the two equations on the left by $u_m^{*T}$ and $v_m^{*T}$ respectively yields\n",
    "\n",
    "\\begin{equation}\n",
    "    u_m^{*T}\\mathbf{M}v_m^* = 2\\lambda \\qquad\\text{ and }\\qquad v_m^{*T}\\mathbf{M}^T u_m^* = 2\\mu,\n",
    "\\end{equation}\n",
    "\n",
    "so $\\lambda = \\mu$. On the other hand, multiplying the equations by $u_k^{*T}$ and $v_k^{*T}$ yields\n",
    "\n",
    "\\begin{equation}\n",
    "    \\alpha_k^{YX}u_k^{*T}\\mathbf{M}v_k^* = \\alpha_k^{YY} \\qquad\\text{ and }\\qquad \\alpha_k^{XY}v_k^{*T}\\mathbf{M}^Tu_k^* = \\alpha_k^{XX}\n",
    "\\end{equation}\n",
    "\n",
    "so $\\alpha_k^{YY} = d^*_k \\alpha_k^{YX}$ and $\\alpha_k^{XX} = d^*_k \\alpha_k^{XY}$, where $d^*_k$ is the $k$th singular value of $\\mathbf{M}$.\n",
    "\n",
    "Using these two sets of equations and the fact that $(u^*_1, v^*_1)\\ldots ,(u^*_{m-1}, v^*_{m-1})$ are pairs of singular vectors, the equations from setting the partial derivatives of $\\mathcal{L}$ equal to zero yield\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{M}v_m^* = 2\\lambda u_m^* \\qquad\\text{and}\\qquad \\mathbf{M}^Tu_m^* = 2\\lambda v_m^*.\n",
    "\\end{equation}\n",
    "\n",
    "So $(u_m^*, v_m^*)$ are a pair of singular vectors for $\\mathbf{M}$. To maximise $u_m^{*T}\\mathbf{M}v_m^*=2\\lambda$ subject to the orthogonality conditions, the must by the $m$th pair of singular vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.21\n",
    "\n",
    "Show that the solution to the reduced-rank regression problem (3.68), with $\\Sigma$ estimated by $\\mathbf{Y}^T\\mathbf{Y}/N$, is given by (3.69). *Hint:* Transform $\\mathbf{Y}$ to $\\mathbf{Y}^ = \\mathbf{Y}\\Sigma^{-\\frac{1}{2}}$, and solve in terms of the canonical vectors $u^$. Show that $\\mathbf{U}_m = \\Sigma^{-\\frac{1}{2}}\\mathbf{U}_m^*$, and a generalized inverse is $\\mathbf{U}_m^ =\\mathbf{U}_m^{T}\\Sigma^{\\frac{1}{2}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "As in the previous problem, take the Cholesky decompositions $\\mathbf{X}^T\\mathbf{X}=\\mathbf{K}^T\\mathbf{K}$ and $\\mathbf{Y}^T\\mathbf{Y}=\\mathbf{L}^T\\mathbf{L}$. Define $\\mathbf{X}^*=\\mathbf{X}\\mathbf{K}^{-1}$, $\\mathbf{Y}^*=\\mathbf{Y}\\mathbf{L}^{-1}$, and $\\mathbf{M}=(\\mathbf{Y}^*)^T\\mathbf{X}^*$. We need to minimise\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^N (y_i - \\mathbf{B}^Tx_i)^T\\Sigma^{-1}(y_i - \\mathbf{B}^Tx_i)\n",
    "\\end{equation}\n",
    "\n",
    "over all $\\mathbf{B}$ of rank $m$. Estimating $\\Sigma$ by $\\mathbf{Y}^T\\mathbf{Y} / N$, this simplifies to\n",
    "\n",
    "\\begin{align}\n",
    "    N\\sum_{i=1}^N (y_i - \\mathbf{B}^T x_i)^T (\\mathbf{Y}^T\\mathbf{Y})^{-1} (y_i - \\mathbf{B}^T x_i)\n",
    "        & = N\\sum_{i=1}^N (y_i - \\mathbf{B}^T x_i)^T \\mathbf{L}^{-1}(\\mathbf{L}^{-1})^T (y_i - \\mathbf{B}^T x_i) \\\\\n",
    "        & = N\\sum_{i=1}^N ((\\mathbf{L}^{-1})^T y_i - (\\mathbf{B}\\mathbf{L}^{-1})^T x_i)^T ((\\mathbf{L}^{-1})^T y_i - (\\mathbf{B}\\mathbf{L}^{-1})^T x_i) \\\\\n",
    "        & = N \\lVert \\mathbf{Y}\\mathbf{L}^{-1} - \\mathbf{X}\\mathbf{B}\\mathbf{L}^{-1} \\rVert_2^2 \\\\\n",
    "        & = N \\lVert \\mathbf{Y}^* - \\mathbf{X}^*\\mathbf{B}^* \\rVert_2^2,\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{B}^* = \\mathbf{K}\\mathbf{B}\\mathbf{L}^{-1}$ and $\\lVert\\cdot\\rVert_2$ denotes the $L^2$ matrix norm. Note that since $\\mathbf{K}$ and $\\mathbf{L}$ are invertible, $\\mathbf{B}^*$ has the same rank as $\\mathbf{B}$.\n",
    "\n",
    "First we minimise the RSS expression above over all $\\mathbf{B}^*$ whose columns lie in a fixed $m$-dimensional subspace of $\\mathbb{R}^K$. Let $v_1^*, \\ldots, v_m^*\\in\\mathbb{R}^p$ be an orthonormal set and let $\\mathbf{V}_m^*$ denote the $p\\times p$ matrix with columns $v_1^*, \\ldots, v_m^*, 0, \\ldots, 0$. Minimising over all $\\mathbf{B}^*$ whose columns  lie in $\\text{span}\\{v_1^*,\\ldots, v_m^*\\}$ is equivalent to replacing $\\mathbf{B}^*$ with $\\mathbf{V}_m^*\\mathbf{B}^*$ and minimising over all $\\mathbf{B}^*$ without constraint. \n",
    "\n",
    "Making this substitution means that we need to minimise $\\lVert \\mathbf{Y}^* - \\mathbf{X}^*\\mathbf{V}_m^*\\mathbf{B}^*\\rVert_2^2$ over all $\\mathbf{B}^*$. This is now a standard multiple outcome regression problem; the minimium is achieved by taking the orthogonal projection of $\\mathbf{Y}^*$ onto the column space of $\\mathbf{X}^*\\mathbf{V}^*_m$. The projection matrix is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{H} \n",
    "        = \\mathbf{X}^*\\mathbf{V}_m^*(\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{X}^*\\mathbf{V}_m^*)^{-1}\\mathbf{V}_m^{*T}\\mathbf{X}^{*T} \n",
    "        = \\mathbf{X}^*\\mathbf{V}_m^*\\mathbf{V}_m^{*T}\\mathbf{X}^{*T},\n",
    "\\end{equation}\n",
    "\n",
    "(the change of basis implies that $\\mathbf{X}^{*T}\\mathbf{X}^*=\\mathbf{I}$ and $\\mathbf{V}_m^{*T}\\mathbf{V}_m^*=\\mathbf{I}$ since the columns are orthonormal) and so the RSS equals\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\mathbf{Y}^* - \\mathbf{X}^*\\mathbf{V}_m^*\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{Y}^*\\rVert_2^2\n",
    "        =  \\lVert \\mathbf{Y}^*\\rVert_2^2 - \\lVert\\mathbf{X}^*\\mathbf{V}_m^*\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{Y}^*\\rVert_2^2,\n",
    "\\end{equation}\n",
    "\n",
    "using orthogonality of $\\mathbf{Y}^*$ and its projection.\n",
    "\n",
    "To get the final solution we need to minimise the expression above over all $m$-dimensional spaces $\\text{span}\\{ v_1^*,\\ldots , v_m^*\\}$. Using the fact that $\\lVert \\mathbf{A}\\rVert_2^2 = \\text{Tr}(\\mathbf{A}^T\\mathbf{A})$, the second term in the expression simplifies:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Tr}(\\mathbf{Y}^{*T}\\mathbf{X}^*\\mathbf{V}_m^*\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{X}^*\\mathbf{V}_m^*\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{Y}^*)\n",
    "        & = \\text{Tr}(\\mathbf{Y}^{*T}\\mathbf{X}^*\\mathbf{V}_m^*\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{Y}^*) \\\\\n",
    "        & = \\text{Tr}(\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{Y}^*\\mathbf{Y}^{*T}\\mathbf{X}^*\\mathbf{V}_m^*) \\\\\n",
    "        & = \\text{Tr}(\\mathbf{V}_m^{*T}\\mathbf{M}^{T}\\mathbf{M}\\mathbf{V}_m^*) \\\\\n",
    "        & = \\sum_{k=1}^m v_k^{*T}\\mathbf{M}^{T}\\mathbf{M}v_k^*,\n",
    "\\end{align}\n",
    "\n",
    "where recall $\\mathbf{M} = \\mathbf{Y}^{*T}\\mathbf{X}^*$. This is of course maximised by the first $m$ right singular vectors of $\\mathbf{M}$, which conveniently were labelled $v_1^*,\\ldots, v_m^*$ in the last problem. The corresponding minimum value of the RSS is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Tr}(\\mathbf{Y}^{*T}\\mathbf{Y}^*) - \\sum_{k=1}^m (d_k^*)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $d_1^*,\\ldots, d_m^*$ are the first $m$ singular values of $\\mathbf{M}$.\n",
    "\n",
    "Thus the final solution is \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{B}^* = \\mathbf{V}_m^*\\mathbf{V}_m^{*T}\\mathbf{X}^{*T}\\mathbf{Y}^* = \\mathbf{V}_m^* \\mathbf{V}_m^{*T} \\mathbf{M}^T,\n",
    "\\end{equation}\n",
    "\n",
    "(note that we we have incorporated $\\mathbf{V}_m^*$ back into $\\mathbf{B}^*$). This looks slightly different from the answer in the book so we will now show that they are equivalent by showing that the respective RSS are equal. \n",
    "\n",
    "Let $\\mathbf{U}_m^*$ denote the $K\\times p$ matrix whose first $m$ columns $u_1^*,\\ldots, u_m^*$ are the left singular vectors of $\\mathbf{M}$ and whose other columns are zero. If $\\mathbf{B}^* = \\mathbf{M}^T\\mathbf{U}_m^*\\mathbf{U}_m^{*T}$ then the residual sum-of-squares simplifies as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    \\lVert \\mathbf{Y}^* - \\mathbf{X}^*\\mathbf{M}^T\\mathbf{U}_m^*\\mathbf{U}_m^{*T}\\rVert_2^2\n",
    "        & = \\text{Tr}\\left((\\mathbf{Y}^* - \\mathbf{X}^*\\mathbf{M}^T\\mathbf{U}_m^*\\mathbf{U}_m^{*T})^T (\\mathbf{Y}^* - \\mathbf{X}^*\\mathbf{M}^T\\mathbf{U}_m^*\\mathbf{U}_m^{*T})\\right) \\\\\n",
    "        & = \\text{Tr}(\\mathbf{Y}^{*T}\\mathbf{Y}^*) \n",
    "            - \\text{Tr}(\\mathbf{M}\\mathbf{M}^T\\mathbf{U}_m^*\\mathbf{U}_m^{*T})\n",
    "            - \\text{Tr}(\\mathbf{U}_m^*\\mathbf{U}_m^{*T}\\mathbf{M}\\mathbf{M}^T)\n",
    "            - \\text{Tr}(\\mathbf{U}_m^*\\mathbf{U}_m^{*T}\\mathbf{M}\\mathbf{X}^{*T}\\mathbf{X}^*\\mathbf{M}^T\\mathbf{U}_m^*\\mathbf{U}_m^{*T}) \\\\\n",
    "        & = \\text{Tr}(\\mathbf{Y}^{*T}\\mathbf{Y}^*) - \\text{Tr}(\\mathbf{U}_m^{*T}\\mathbf{M}\\mathbf{M}^T\\mathbf{U}_m^*) \\\\\n",
    "        & = \\text{Tr}(\\mathbf{Y}^{*T}\\mathbf{Y}^*) - \\sum_{k=1}^m (d_k^*)^2.\n",
    "\\end{align}\n",
    "\n",
    "Thus the two answers are equivalent.\n",
    "\n",
    "Finally note that if $\\mathbf{B}^* = \\mathbf{M}^T\\mathbf{U}_m^*\\mathbf{U}_m^{*T}$ then\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{B} \n",
    "        & = \\mathbf{K}^{-1}\\mathbf{B}^*\\mathbf{L} \\\\\n",
    "        & = \\mathbf{K}^{-1}\\mathbf{X}^{*T}\\mathbf{Y}^*\\mathbf{U}_m^*\\mathbf{U}_m^{*T}\\mathbf{L} \\\\\n",
    "        & = \\mathbf{K}^{-1}(\\mathbf{K}^T)^{-1}\\mathbf{X}^{T}\\mathbf{Y}\\mathbf{L}^{-1}\\mathbf{U}_m^*\\mathbf{U}_m^{*T}\\mathbf{L} \\\\\n",
    "        & = \\hat{\\mathbf{B}}\\mathbf{U}_m\\mathbf{U}_m^-.\n",
    "\\end{align}\n",
    "\n",
    "This is the solution given in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.22\n",
    "\n",
    "Show that the solution in Exercise 3.21 does not change if $\\Sigma$ is estimated by the more natural quantity $(\\mathbf{Y}-\\mathbf{X}\\hat{\\mathbf{B}})^T(\\mathbf{Y}-\\mathbf{X}\\hat{\\mathbf{B}}) \\big/ (N-pK)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Take the Cholesky decomposition $(\\mathbf{Y}-\\mathbf{X}\\hat{\\mathbf{B}})^T(\\mathbf{Y}-\\mathbf{X}\\hat{\\mathbf{B}}) = \\tilde{\\mathbf{L}}^T\\tilde{\\mathbf{L}}$. We can run through the exact same solution as in Exercise 3.21 using the change of bases $\\tilde{\\mathbf{Y}}^*=\\mathbf{Y}\\tilde{\\mathbf{L}}^{-1}$, $\\tilde{\\mathbf{B}}^* = \\mathbf{K}\\tilde{\\mathbf{B}}\\tilde{\\mathbf{L}}^{-1}$, and $\\tilde{\\mathbf{M}}=\\tilde{\\mathbf{Y}}^{*T}\\mathbf{X}^*$ until we get to final solution\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{\\mathbf{B}} = \\hat{\\mathbf{B}}\\tilde{\\mathbf{L}}^{-1}\\tilde{\\mathbf{U}}_m^*\\tilde{\\mathbf{U}}_m^{*T}\\tilde{\\mathbf{L}},\n",
    "\\end{equation}\n",
    "\n",
    "where the columns of $\\tilde{\\mathbf{U}}_m^*$ are the first $m$ left singular vectors of $\\tilde{\\mathbf{M}}$. Thus to show that the two solutions are the same, it suffices to show that $\\tilde{\\mathbf{L}}^{-1}\\tilde{\\mathbf{U}}_m^* = \\mathbf{L}^{-1}\\mathbf{U}_m^*$ (the matrix $\\mathbf{U}_m^{*T}\\mathbf{L}$ is uniquely determined by the fact that it is left-inverse to $\\mathbf{L}^{-1}\\mathbf{U}_m^*$ and the same is true of the tilde analogues).\n",
    "\n",
    "To show this we return to the solution of Exercise 3.21 (the penultimate block of equations) and recall that $\\mathbf{U}_m^*$ was determined by the following property: it is the $K\\times m$ matrix with orthonormal columns that minimises $\\text{Tr}(\\mathbf{U}_m^{*T}\\mathbf{M}\\mathbf{M}^T\\mathbf{U}_m^*)$. But note that\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{U}_m^{*T}\\mathbf{M}\\mathbf{M}^T\\mathbf{U}_m^*\n",
    "        & = \\mathbf{U}_m^{*T}(\\mathbf{L}^{-1})^T\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}^{-1}(\\mathbf{K}^{-1})^T\\mathbf{X}^T\\mathbf{Y}\\mathbf{L}^{-1}\\mathbf{U}_m^*) \\\\\n",
    "        & = (\\mathbf{L}^{-1}\\mathbf{U}_m^{*})^T\\mathbf{Y}^T\\mathbf{X}\\hat{\\mathbf{B}}(\\mathbf{L}^{-1}\\mathbf{U}_m^*)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus to show that the two solutions are the same it suffices to show that $N^{-1}$ applied to the $k$th left singular vector of $(\\mathbf{N}^{-1})^T\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}^{-1}$ equals $\\mathbf{L}^{-1}$ applied to the $k$th left singular vector of $(\\mathbf{L}^{-1})^T\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}^{-1}$. But this is clear: the left singular vectors of $(\\mathbf{N}^{-1})^T\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}^{-1}$ are the eigenvectors of\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left((\\mathbf{N}^{-1})^T\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}^{-1}\\right) \\left((\\mathbf{N}^{-1})^T\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}^{-1}\\right)^T \n",
    "        = (\\mathbf{N}^{-1})^T\\left(\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}\\right) \\left(\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}\\right)^T \\mathbf{N}^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "and so applying $\\mathbf{N}^{-1}$ to them yields the eigenvectors of $\\left(\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}\\right) \\left(\\mathbf{Y}^T\\mathbf{X}\\mathbf{K}\\right)^T$. We get the same result if we use $\\mathbf{L}$ in place of $\\mathbf{N}$ and so the solutions are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.23\n",
    "\n",
    "Consider a regression problem with all variables and response having mean zero and standard deviation one. Suppose also that each variable has identical absolute correlation with the response:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N}\\lvert\\langle\\mathbf{x}_j,\\mathbf{y}\\rangle\\rvert = \\lambda,\\quad j=1, \\ldots, p.\n",
    "\\end{equation}\n",
    "\n",
    "Let $\\hat{\\beta}$ be the least-squares coefficient of $\\mathbf{y}$ on $\\mathbf{X}$, and let $\\mathbf{u}(\\alpha) = \\alpha\\mathbf{X}\\hat{\\beta}$ for $\\alpha\\in[0, 1]$ be the vector that moves a fraction $\\alpha$ toward the least squares fit $\\mathbf{u}$. Let $RSS$ be the residual sum-of-squares from the full least squares fit.\n",
    "\n",
    "**(a)** Show that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N}\\lvert\\langle \\mathbf{x}_j, \\mathbf{y}-\\mathbf{u}(\\alpha)\\rangle\\rvert \n",
    "        = (1-\\alpha)\\lambda,\\quad j=1, \\ldots, p\n",
    "\\end{equation}\n",
    "\n",
    "and hence the correlations of each $\\mathbf{x}_j$ with the residuals remain equal in magnitude as we progress toward $\\mathbf{u}$.\n",
    "\n",
    "**(b)** Show that these correlations are all equal to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lambda(\\alpha)\n",
    "        = \\frac{1-\\alpha}{\\sqrt{(1-\\alpha)^2+\\frac{\\alpha(2-\\alpha)}{N}\\cdot RSS}}\\cdot \\lambda,\n",
    "\\end{equation}\n",
    "\n",
    "and hence they decrease monotonically to zero.\n",
    "\n",
    "**(c)** Use these results to show that the LAR algorithm in Section 3.4.4 keeps the correlations tied and monotonically decreasing, as claimed in (3.55)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** Take $j\\in\\{0,\\ldots,p\\}$ and let $\\mathbf{e}_j$ denote the coordinate vector with a 1 in the $j$th position and 0s elsewhere. Then\n",
    "\n",
    "\\begin{align}\n",
    "    \\langle\\mathbf{x}_j, \\mathbf{u}(\\alpha)\\rangle\n",
    "        & = \\langle\\mathbf{x}_j, \\alpha\\mathbf{X}\\hat{\\beta}\\rangle \\\\\n",
    "        & = \\alpha\\langle\\mathbf{X}\\mathbf{e}_j, \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\alpha\\langle\\mathbf{e}_j, \\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\alpha\\langle\\mathbf{e}_j, \\mathbf{X}^T\\mathbf{y}\\rangle \\\\\n",
    "        & = \\alpha\\langle\\mathbf{X}\\mathbf{e}_j, \\mathbf{y}\\rangle \\\\\n",
    "        & = \\alpha\\langle\\mathbf{x}_j, \\mathbf{y}\\rangle\n",
    "\\end{align}\n",
    "\n",
    "so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N}\\lvert\\langle\\mathbf{x}_j, \\mathbf{y}-\\mathbf{u}(\\alpha)\\rangle\\rvert\n",
    "        = \\frac{1}{N}\\lvert (1-\\alpha)\\langle\\mathbf{x}_j, \\mathbf{y}\\rangle\\rvert\n",
    "        = (1-\\alpha)\\lambda\n",
    "\\end{equation}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** It will be useful in part (c) to drop the assumption that $\\mathbf{y}$ has unit standard deviation and show that $\\mathbf{x}_j$ and $\\mathbf{y}$ have absolute correlation\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lambda(\\alpha)\n",
    "        = \\frac{1-\\alpha}{\\sqrt{\\frac{(1-\\alpha)^2}{N}\\lVert\\mathbf{y}\\rVert^2+\\frac{\\alpha(2-\\alpha)}{N}\\cdot RSS}}\\cdot \\lambda.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\bar{\\mathbf{y}}=0$, the variance of $\\mathbf{y}$ is $\\lVert\\mathbf{y}\\rVert^2/N$ and so reduces to the formula in the question if $\\mathbf{y}$ has standard deviation one.\n",
    "\n",
    "Recall that the correlation between two variables is equal to their covariance over the product of their standard deviations. The standard deviation of each $\\mathbf{x}_j$ is 1 so it suffices to show that the sample standard deviation of $\\mathbf{y}-\\mathbf{u}(\\alpha)$ is equal to the denominator of the expression. Since $\\bar{\\mathbf{y}}=0$ by assumption and $\\overline{\\mathbf{u}(\\alpha)}=\\alpha\\sum \\hat{\\beta}_j\\bar{\\mathbf{x}_j}=0$, this is equivalent to showing\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N}\\lVert \\mathbf{y} - \\alpha\\hat{\\mathbf{y}}\\rVert^2 = (1-\\alpha)^2 + \\frac{\\alpha(2-\\alpha)}{N}RSS.\n",
    "\\end{equation}\n",
    "\n",
    "First note that $\\mathbf{y}-\\hat{\\mathbf{y}}$ is orthogonal to the column space of $\\mathbf{X}$ so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert\\mathbf{y}-\\hat{\\mathbf{y}}\\rVert^2\n",
    "        = \\langle \\mathbf{y}-\\hat{\\mathbf{y}}, \\mathbf{y}\\rangle - \\langle \\mathbf{y}-\\hat{\\mathbf{y}}, \\hat{\\mathbf{y}}\\rangle\n",
    "        = \\langle \\mathbf{y}-\\hat{\\mathbf{y}}, \\mathbf{y}\\rangle.\n",
    "\\end{equation}\n",
    "\n",
    "Using this, we have\n",
    "\n",
    "\\begin{align}\n",
    "    \\lVert\\mathbf{y} - \\alpha\\hat{\\mathbf{y}}\\rVert^2\n",
    "        & = \\lVert \\alpha(\\mathbf{y} - \\hat{\\mathbf{y}}) + (1-\\alpha)\\mathbf{y} \\rVert^2 \\\\\n",
    "        & = \\alpha^2\\lVert\\mathbf{y} - \\hat{\\mathbf{y}}\\rVert^2\n",
    "            + 2\\alpha(1-\\alpha)\\langle \\mathbf{y}-\\hat{\\mathbf{y}}, \\mathbf{y}\\rangle\n",
    "            + (1 - \\alpha)^2\\lVert \\mathbf{y}\\rVert^2 \\\\\n",
    "        & = \\alpha^2\\lVert\\mathbf{y} - \\hat{\\mathbf{y}}\\rVert^2\n",
    "            + 2\\alpha(1-\\alpha)\\lVert\\mathbf{y} - \\hat{\\mathbf{y}}\\rVert^2\n",
    "            + (1 - \\alpha)^2\\lVert \\mathbf{y}\\rVert^2,\n",
    "\\end{align}\n",
    "\n",
    "which establishes the formula for $\\lambda(\\alpha)$.\n",
    "\n",
    "Finally, one can show that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lambda^{\\prime}(\\alpha) = \\frac{-RSS\\lambda}{N\\left( \\frac{(1-\\alpha)^2}{N}\\lVert\\mathbf{y}\\rVert^2+\\frac{\\alpha(2-\\alpha)}{N}\\cdot RSS\\right)^{3/2}} < 0\n",
    "\\end{equation}\n",
    "\n",
    "so that $\\lambda(\\alpha)$ decreases monotonically to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Start with standardised predictors with mean zero and unit variance. \n",
    "\n",
    "Take $j\\in\\{1,\\ldots ,p\\}$ and assume that at the start of step $j$ we have a residual vector $\\mathbf{r}_j$ which has mean zero and identical absolute correlation $\\lambda_j$ with $\\mathbf{x}_1, \\ldots, \\mathbf{x}_j$. Let $\\mathbf{u}_j(\\alpha)$ for $\\alpha\\in[0,1]$ be the vector that moves a fraction $\\alpha$ towards the least squares fit of $\\mathbf{x}_1, \\ldots,\\mathbf{x}_j$ to $\\mathbf{r}_j$ (for the first step we set $\\mathbf{r}_1 = \\mathbf{y} - \\bar{\\mathbf{y}}$ and assume wlog that $\\mathbf{x}_1$ has the greatest absolute correlation with $\\mathbf{r}_1$).\n",
    "\n",
    "Take $\\alpha_j\\in[0,1]$ minimal such that a predictor $\\mathbf{x}_k$ with $k>j$ has correlation with $\\mathbf{r}_j - \\mathbf{u}_j(\\alpha_j)$ equal to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lambda_j(\\alpha_j)\n",
    "            = \\frac{(1-\\alpha_j)\\lambda_j}{\\sqrt{\\frac{1}{N}\\left((1-\\alpha_j)^2\\lVert\\mathbf{r}_j\\rVert^2+\\alpha_j(2-\\alpha_j)RSS\\right)}},\n",
    "\\end{equation}\n",
    "\n",
    "where $RSS$ denotes the residual sum of squares between $\\mathbf{r}_j$ and its least squares fit (if there is no such $k$ then we're done). Wlog assume $k=j+1$ and set $\\mathbf{r}_{j+1} = \\mathbf{r}_j-\\mathbf{u}_j(\\alpha_j)$ and $\\lambda_{j+1} = \\lambda_j(\\alpha_j)$. By (b) we are now in the right position to start step $j+1$.\n",
    "\n",
    "Assuming that $\\mathbf{X}$ has full column rank this process will terminate at the $p$th step where it reaches the least squares solution. This gives us a piecewise linear path $\\mathbf{u}(\\alpha)$ for $\\alpha\\in[0,\\sum_0^p\\alpha_j]$ with $\\mathbf{u}(0)=\\mathbf{y}$, $\\mathbf{u}(\\sum_0^p\\alpha_j)=\\hat{\\beta}$ and \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{u}\\left(\\sum_{j=1}^k \\alpha_j + t\\right) = \\mathbf{r}_k+\\mathbf{u}_j(t)\n",
    "\\end{equation}\n",
    "\n",
    "for $t\\in[0,\\alpha_{j+1}]$. By part (a) the correlations stay tied and decreasing along this path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.24\n",
    "\n",
    "*LAR directions.* Using the notation around equation (3.55) on page 74, show that the LAR direction makes an equal angle with each of the predictors in $\\mathcal{A}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The angle between $\\mathbf{u}_k$ and $\\mathbf{x}_j$ is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{arccos}\\left(\\frac{\\lvert\\langle \\mathbf{x}_j,\\mathbf{u}_k\\rangle\\rvert}{\\lVert \\mathbf{x}_j\\rVert\\lVert \\mathbf{u}_k\\rVert}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "so the smallest angle corresponds to the greatest value of $\\frac{\\lvert\\langle \\mathbf{x}_j,\\mathbf{u}_k\\rangle\\rvert}{\\lVert \\mathbf{x}_j\\rVert\\lVert \\mathbf{u}_k\\rVert}$, that is the greatest absolute correlation. But by exercise 3.23 elements of $\\mathcal{A}_k$ have equal absolute correlation with $\\mathbf{u}_k$, which is maximal among $\\mathbf{x}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.25\n",
    "\n",
    "*LAR look-ahead (Efron et al., 2004, Sec. 2).* Starting at the beginning of the $k$th step of the LAR algorithm, derive expressions to identify the next variable to enter the active set at step $k + 1$, and the value of  at which this occurs (using the notation around equation (3.55) on page 74)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Take $\\mathbf{x}_a\\in\\mathcal{A}_k$ and $\\mathbf{x}_b\\notin\\mathcal{A}_k$. We wish to identify the values of $\\alpha$ such that the two vectors have the same absolute correlation with $\\mathbf{r}_k-\\alpha\\mathbf{u}_k$. This is the case if and only if\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\lvert\\langle \\mathbf{x}_a,\\mathbf{r}_k-\\alpha\\mathbf{u}_k\\rangle\\rvert}{\\lVert \\mathbf{r}_k-\\alpha\\mathbf{u}_k\\rVert}\n",
    "        = \\frac{\\lvert\\langle \\mathbf{x}_b,\\mathbf{r}_k-\\alpha\\mathbf{u}_k\\rvert}{\\lVert \\mathbf{r}_k-\\alpha\\mathbf{u}_k\\rangle\\rVert}\n",
    "    \\quad\\Longleftrightarrow\\quad \\lvert\\langle\\mathbf{x}_a,\\mathbf{r}_k-\\alpha\\mathbf{u}_k\\rangle\\rvert\n",
    "        = \\lvert\\langle \\mathbf{x}_b,\\mathbf{r}_k-\\alpha\\mathbf{u}_k\\rangle\\rvert.\n",
    "\\end{align}\n",
    "\n",
    "This has two solutions:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\alpha = \\frac{\\langle\\mathbf{x}_a\\pm\\mathbf{x}_b,\\mathbf{r}_k\\rangle}{\\langle\\mathbf{x}_a\\pm\\mathbf{x}_b,\\mathbf{u}_k\\rangle}.\n",
    "\\end{equation}\n",
    "\n",
    "So we can identify the next variable to enter the active set as follows. Fix $\\mathbf{x}_a\\in\\mathcal{A}_k$ and calculate the two $\\alpha$ values above for each $\\mathbf{x}_b\\notin\\mathcal{A}_k$. The $\\mathbf{x}_b$ with the greatest $\\alpha$ value lying in $[0,1]$ is the next to join the active set and it will occur at that value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.26\n",
    "\n",
    "Forward stepwise regression enters the variable at each step that most reduces the residual sum-of-squares. LAR adjusts variables that have the most (absolute) correlation with the current residuals. Show that these two entry criteria are not necessarily the same. [Hint: let $\\mathbf{x}_{j.\\mathcal{A}}$ be the $j$th variable, linearly adjusted for all the variables currently in the model. Show that the first criterion amounts to identifying the $j$ for which $\\text{Cor}(\\mathbf{x}_{j.\\mathcal{A}},\\mathbf{r})$ is largest in magnitude.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "I believe that the statement in the hint was more-or-less shown in Exercise 3.9. Regress $\\mathbf{x}_j$ on the variables currently in the model and let $\\mathbf{x}_{j.\\mathcal{A}}$ denote the residual. In Exercise 3.9 we showed that the $j$ with the greatest $\\lvert\\mathbf{x}_{j.\\mathcal{A}}^T\\mathbf{y}\\rvert/\\lVert \\mathbf{x}_{j.\\mathcal{A}}\\rVert$ will reduce the residual sum-of-squares the most. Since by construction $\\mathbf{x}_{j.\\mathcal{A}}$ is orthogonal the variables in the active set, $\\mathbf{x}_{j.\\mathcal{A}}^T\\mathbf{y} = \\mathbf{x}_{j.\\mathcal{A}}^T\\mathbf{r}$ and so this is the same as identifying the $j$ with the greatest value of $\\lvert\\text{Cor}(\\mathbf{x}_{j.\\mathcal{A}},\\mathbf{r})\\rvert$.\n",
    "\n",
    "\n",
    "To illustrate how these criteria could lead to different results consider the situation where the current active set consists of a single variable $\\mathbf{x}_k$. Then \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{r} = \\mathbf{y}-\\frac{\\langle\\mathbf{x}_k,\\mathbf{y}\\rangle}{\\langle\\mathbf{x}_k,\\mathbf{x}_k\\rangle}\\mathbf{x}_k,\n",
    "    \\qquad\\text{and}\\qquad \n",
    "    \\mathbf{x}_{j.\\mathcal{A}} = \\mathbf{y}-\\frac{\\langle\\mathbf{x}_k,\\mathbf{x}_j\\rangle}{\\langle\\mathbf{x}_k,\\mathbf{x}_k\\rangle}\\mathbf{x}_k \\qquad \\text{for all }j\\neq k.\n",
    "\\end{equation}\n",
    "\n",
    "A little calculation shows that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Corr}(\\mathbf{x}_{j.\\mathcal{A}}, \\mathbf{r}) = \\left(1 - \\text{Corr}^2(\\mathbf{x}_j,\\mathbf{x}_k)\\right)\\text{Corr}(\\mathbf{x}_j, \\mathbf{r}).\n",
    "\\end{equation}\n",
    "\n",
    "So if $\\mathbf{x}_j$ is highly correlated with $\\mathbf{x}_k$ then it may be included in the active set under the LAR criterion but not the forward stepwise criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.27\n",
    "\n",
    "*Lasso and LAR*: Consider the lasso problem in Lagrange multiplier form: with $L(\\beta)=\\frac{1}{2}\\sum_i(y_i-\\sum_jx_{ij}\\beta_j)^2$, we minimise \n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\beta) + \\lambda\\sum_j\\lvert\\beta_j\\rvert\n",
    "\\end{equation}\n",
    "\n",
    "for fixed $\\lambda>0$.\n",
    "\n",
    "**(a)** Setting $\\beta_j=\\beta_j^+-\\beta_j^-$ with $\\beta_j^+,\\beta_j^-\\geq 0$, expression (3.88) [above] becomes $L(\\beta) + \\lambda\\sum_j(\\beta_j^+ + \\beta_j^-)$. Show that the Lagrange dual function is\n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\beta) + \\lambda\\sum_j(\\beta_j^+ + \\beta_j^-)-\\sum_j\\lambda_j^+\\beta_j^+ - -\\sum_j\\lambda_j^-\\beta_j^-\n",
    "\\end{equation}\n",
    "\n",
    "and the Karush-Kuhn-Tucker optimality conditions are\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla L(\\beta)_j + \\lambda - \\lambda_j^+ & = 0 \\\\\n",
    "    -\\nabla L(\\beta)_j + \\lambda - \\lambda_j^- & = 0 \\\\\n",
    "    \\lambda_j^+\\beta_j^+ & = 0 \\\\\n",
    "    \\lambda_j^-\\beta_j^- & = 0,\n",
    "\\end{align}\n",
    "\n",
    "along with the non-negativity constraints on the parameters and all the Lagrange multipliers.\n",
    "\n",
    "**(b)** Show that $\\lvert\\nabla L(\\beta)_j\\rvert\\leq \\lambda$ $\\forall j$, and that the KKT conditions imply one of the following three scenarios:\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda = 0\n",
    "        & \\qquad\\Rightarrow\\qquad \\nabla L(\\beta)_j = 0 \\, \\forall j \\\\\n",
    "    \\beta_j^+>0,\\, \\lambda>0\n",
    "        & \\qquad\\Rightarrow\\qquad \\lambda_j^+=0,\\, \\nabla L(\\beta)_j=-\\lambda<0,\\, \\beta_j^-=0 \\\\\n",
    "    \\beta_j^->0,\\, \\lambda>0\n",
    "        & \\qquad\\Rightarrow\\qquad \\lambda_j^-=0,\\, \\nabla L(\\beta)_j=\\lambda>0,\\, \\beta_j^+=0.\n",
    "\\end{align}\n",
    "\n",
    "Hence show that for any active predictor having $\\beta_k\\neq 0$, we must have $\\nabla L(\\beta)_j = -\\lambda$ if $\\beta_j>0$, and $\\nabla L(\\beta)_j = \\lambda$ if $\\beta_j<0$. Assuming the predictors are standardized, relate $\\lambda$ to the correlation between the $j$th predictor and the current residuals.\n",
    "\n",
    "**(c)** Suppose that the set of active predictors is unchanged for $\\lambda_0\\geq\\lambda\\geq\\lambda_1$. Show that there is a vector $\\gamma_0$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\beta}(\\lambda) = \\hat{\\beta}(\\lambda_0) - (\\lambda-\\lambda_0)\\gamma_0.\n",
    "\\end{equation}\n",
    "\n",
    "Thus the lasso solution path is linear as $\\lambda$ ranges from $\\lambda_0$ to $\\lambda_1$ (Efron et al., 2004; Rosset and Zhu, 2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
