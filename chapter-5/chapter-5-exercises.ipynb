{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1\n",
    "\n",
    "Show that the truncated power basis functions in (5.3) represent a basis for a cubic spline with the two knots as indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Recall that\n",
    "\n",
    "\\begin{align}\n",
    "    h_1(X) = 1,\\qquad h_3(X) = X^2, \\qquad h_5(X) = (X-\\xi_1)^3 \\\\\n",
    "    h_2(X) = X,\\qquad h_4(X) = X^3, \\qquad h_6(X) = (X-\\xi_2)^3.\n",
    "\\end{align}\n",
    "\n",
    "First note that each of these has continuity up to the second derivative so any linear combination of them does also. Moreover they are linearly independent: if $\\sum_{m=1}^6 \\beta_m h_m(X)\\equiv 0$ then $\\sum_{m=1}^4 \\beta_m h_m(X)\\equiv 0$ for $X<\\xi_1$ so $\\beta_1=\\beta_2=\\beta_3=\\beta_4=0$, so $\\beta_5 h_5(X)\\equiv 0$ for $X < \\xi_2$ implying $\\beta_5=0$, so $\\beta_6 h_6(X)\\equiv 0$ and thus all $\\beta_m=0$.\n",
    "\n",
    "It remains to show that the $h_m(X)$ span the space of cubic splines with knots $\\xi_1$ and $\\xi_2$. We establish this via the following claim. \n",
    "\n",
    "**Claim:** Suppose that $f^{(1)}(X)$ and $f^{(2)}(X)$ are cubics and\n",
    "\n",
    "\\begin{equation}\n",
    "    f(X) =\n",
    "        \\begin{cases}\n",
    "            f^{(1)}(X) & \\text{if } X < \\xi_1 \\\\\n",
    "            f^{(2)}(X) & \\text{if } X \\geq \\xi_1\n",
    "        \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "is continuous up to its second derivative. Then there exists $\\beta\\in\\mathbb{R}$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "    f^{(2)}(X) \\equiv f^{(1)}(X) + \\beta(X - \\xi)^3.\n",
    "\\end{equation}\n",
    "\n",
    "for $X \\geq \\xi$.\n",
    "\n",
    "Spanning follows from this: if $f(X)$ is a cubic spline with $f(X)=f^{(1)}(X)$ for $X<\\xi_1$, $f(X)=f^{(2)}(X)$ for $\\xi_1\\leq X<\\xi_2$, and $f(X)=f^{(3)}(X)$ for $\\xi_2\\leq X<$ then $f^{(1)}$ can be written as a linear combination of $h_1,\\ldots,h_4$ and then claim shows that adding in suitable multiples of $h_5$ and $h_6$ yields a function that equals $f^{(2)}$ and $f^{(3)}$ on their domains.\n",
    "\n",
    "**Proof of Claim:** To establish this, write\n",
    "\n",
    "\\begin{equation}\n",
    "    f^{(i)}(X) = \\sum_{m=0}^3 \\alpha_m^{(i)} (X - \\xi)^m\n",
    "\\end{equation}\n",
    "\n",
    "for $i=1, 2$. Then\n",
    "\n",
    "\\begin{align}\n",
    "    f^{(1)}(\\xi) = f^{(2)}(\\xi)\n",
    "        \\qquad & \\Rightarrow \\qquad  \\alpha_0^{(1)}=\\alpha_0^{(2)} \\\\\n",
    "    (f^{(1)})^{\\prime}(\\xi) = (f^{(2)})^{\\prime}(\\xi)\n",
    "        \\qquad & \\Rightarrow \\qquad \\alpha_1^{(1)}=\\alpha_1^{(2)} \\\\\n",
    "    (f^{(1)})^{\\prime\\prime}(\\xi) = (f^{(2)})^{\\prime\\prime}(\\xi)\n",
    "        \\qquad & \\Rightarrow \\qquad \\alpha_2^{(1)}=\\alpha_2^{(2)}\n",
    "\\end{align}\n",
    "\n",
    "so $\\beta = \\alpha_3^{(2)} - \\alpha_3^{(1)}$ suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2\n",
    "\n",
    "Suppose that $B_{i, M}(x)$ is an order-$M$ $B$-spline defined in the Apppendix on page 186 through the sequence (5.77)–(5.78).\n",
    "\n",
    "**(a)** Show by induction that $B_{i,M}(x)=0$ for $x\\notin [\\tau_i,\\tau_{i+M}]$. This shows, for example, that the support of cubic $B$-splines is at most 5 knots.\n",
    "\n",
    "**(b)** Show by induction that $B_{i,M}(x)>0$ for $x\\in (\\tau_i,\\tau_{i+M})$. The $B$-splines are positive in the interior of their support.\n",
    "\n",
    "**(c)** Show by induction that $\\sum_{i=1}^{K+M} B_{i,M}(x)=1 \\, \\forall x\\in[\\xi_0, \\xi_{K+1}]$.\n",
    "\n",
    "**(d)** Show that $B_{i,M}$ is a piecewise polynomial of order $M$ (degree $M − 1$) on $[\\xi_0, \\xi_{K+1}]$, with breaks only at the knots $\\xi_1,\\ldots,\\xi_K$.\n",
    "\n",
    "**(e)** Show that an order-$M$ $B$-spline basis function is the density function of a convolution of $M$ uniform random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** For $M=1$ this is by definition. For the inductive step, suppose that $B_{i,M-1}$ and $B_{i+1, M-1}$ are zero outside $[\\tau_i, \\tau_{i+M-1}]$ and $[\\tau_{i+1}, \\tau_{i+M}]$, respectively. Since $B_{i,M}$ is a linear combination of these it is zero outside $[\\tau_i, \\tau_{i+M}]$ as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** It makes the induction slightly easier to prove that $B_{i,M}>0$ for $x\\in[\\tau_i,\\tau_{i+M})$. Again, for $M=1$ this is by definition. For $M>1$, $B_{i,M}$ is a linear combination of $B_{i,M-1}$ and $B_{i+1,M-1}$, their coefficients $(x-\\tau_i) / (\\tau_{i+M-1}-\\tau_i)$ and $(\\tau_{i+M}-x) / (\\tau_{i+M}-\\tau_{i+1})$ are non-zero on the supports $[\\tau_{i}, \\tau_{i+M-1})$ and $[\\tau_{i+1}, \\tau_{i+M-1})$, and finally $[\\tau_{i}, \\tau_{i+M-1}) \\cup [\\tau_{i+1}, \\tau_{i+M-1}) = [\\tau_i, \\tau_{i+M})$. The claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** For $M=1$ this is clear. If we assume that it's true for $M-1$ then\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{K+M} B_{i,M}(x)\n",
    "        & = \\sum_{i=1}^{K+M} \\left( \\frac{x-\\tau_i}{\\tau_{i+M-1}-\\tau_i} B_{i,M-1}(x) + \\frac{\\tau_{i+M}-x}{\\tau_{i+M}-\\tau_{i+1}} B_{i+1, M-1}(x)\\right) \\\\\n",
    "        & = \\frac{x-\\tau_1}{\\tau_{M}-\\tau_1} B_{1,M-1}(x) \n",
    "            + \\sum_{i=2}^{K+M} \\left( \\frac{x-\\tau_i}{\\tau_{i+M-1}-\\tau_i} + \\frac{\\tau_{i+M-1}-x}{\\tau_{i+M-1}-\\tau_{i}}\\right) B_{i+1, M-1}(x)\n",
    "            + \\frac{\\tau_{K+2M}-x}{\\tau_{K+2M}-\\tau_{K+M+1}} B_{K+M+1, M-1}(x) \\\\\n",
    "        & = \\frac{x-\\tau_1}{\\tau_{M}-\\tau_1} B_{1,M-1}(x) \n",
    "            + \\sum_{i=2}^{K+M} B_{i+1, M-1}(x)\n",
    "            + \\frac{\\tau_{K+2M}-x}{\\tau_{K+2M}-\\tau_{K+M+1}} B_{K+M+1, M-1}(x).\n",
    "\\end{align}\n",
    "\n",
    "By part (a), $B_{1, M-1}(x)$ is zero for $x<\\tau_M\\leq \\xi_0$ and $B_{K+M+1,M-1}(x)$ is zero for $x>\\tau_{K+M+1}\\geq \\xi_{K+1}$ so on $[\\xi_0, \\xi_{K+1}]$, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^{K+M} B_{i,M}(x) = \\sum_{i=2}^{K+M-1} B_{i,M-1}(x) = 1.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** We prove by induction that $B_{i,M}$ is a piecewise polynomial of order $M$ with breaks only at $\\tau_i,\\ldots, \\tau_{i+M}$. This claim clearly follows from this. For $M=1$ this is clear. The inductive step is clear since $B_{i,M}(x)$ is of the form $(a_1x+b_1)B_{i,M-1}(x) + (a_2x+b_2)B_{i+1,M-1}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** For the statement to be true, the differences $\\delta = \\tau_{i+1}-\\tau_i$ must be independent of $i$ (this is necessary for the simplest non-trivial case $M=2$), so suppose this is the case.  Then\n",
    "\n",
    "\\begin{equation}\n",
    "    B_{i,M}(x)=\\frac{1}{(M-1)\\delta}\\left( (x-\\tau_i)B_{i,M-1}(x) + (\\tau_i+M\\delta-x)B_{i+1,M-1}(x)\\right).\n",
    "\\end{equation}\n",
    "\n",
    "Morevoer, a simple induction shows that $B_{i+1,M}(x)=B_{i,M}(x-\\delta)$. \n",
    "\n",
    "Assume for the moment that $\\tau_i=0$ (we will remove this assumption at the end). Let $X$ be a be a random variable with a uniform distribution on $[0,\\delta)$. For $M\\in\\mathbb{N}$, let $f_M$ denote the probability density function of $MX=\\sum_{m=1}^M X$. We claim that $B_{i,M}(x)=\\delta f_M(x)$ for all $x$. By the previous paragraph, it suffices to show that $B_{i,1}(x)=\\delta f_1(x)$ and\n",
    "\n",
    "\\begin{equation}\n",
    "    f_M(x)=\\frac{1}{(M-1)\\delta}\\left( x f_{M-1}(x) + (M\\delta-x)f_{M-1}(x-\\delta)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "for $M>1$. The $M=1$ claim is immediate since $f_1(x)=\\frac{1}{\\delta}I_{[0,\\delta)}(x)$. We will prove the recursive formula using induction.\n",
    "\n",
    "We apply strong induction to the following statement. For $M\\in\\mathbb{N}$, let $f_M$ and $F_M$ denote the pdf and cumulative density function (cdf) of $MX$. Then for $M>1$,\n",
    "\n",
    "\\begin{align}\n",
    "    f_M(x) & = \\frac{1}{(M-1)\\delta}\\left( x f_{M-1}(x) + (M\\delta-x)f_{M-1}(x-\\delta)\\right) \\\\\n",
    "    F_M(x) & = \\frac{1}{M\\delta}\\left( x F_{M-1}(x) + (M\\delta-x)F_{M-1}(x-\\delta)\\right)\n",
    "\\end{align}\n",
    "\n",
    "and for $M\\geq 1$,\n",
    "\n",
    "\\begin{equation}\n",
    "    F_M(x) = \\sum_{k=1}^M \\left(\\frac{1}{k}(x-(M-k)\\delta)f_k(x-(M-k)\\delta)\\right) + I_{[M\\delta, \\infty)}(x).\n",
    "\\end{equation}\n",
    "\n",
    "For the base case, it suffices to show the summation formula for $F_1(x)$. We calculate directly:\n",
    "\n",
    "\\begin{align}\n",
    "    F_1(x) \n",
    "        & = \\int f_1(x) \\,\\mathrm{d}x \\\\\n",
    "        & = \\frac{1}{\\delta} \\int I_{[0,\\delta)}(x) \\,\\mathrm{d}x \\\\\n",
    "        & = \\frac{1}{\\delta} x I_{[0,\\delta)}(x) + I_{[\\delta,\\infty)}(x)\n",
    "\\end{align}\n",
    "\n",
    "which agrees with the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inductive step, take $M > 1$ and assume that all applicable formulas hold for $1\\leq k \\leq M$. We will prove them for $M+1$.\n",
    "\n",
    "First we claim that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{k=1}^{M-1} \\frac{1}{k\\delta}\\left((x-(M-k)\\delta)f_k(x-(M-k)\\delta) + ((M-k+1)\\delta-x)f_k(x-(M-k+1)\\delta)\\right) = f_M(x-\\delta) - f_1(x-M\\delta).\n",
    "\\end{equation}\n",
    "\n",
    "Indeed, we can rewrite the summand as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{k\\delta}\\left((x-(M-k)\\delta)f_k(x-(M-k)\\delta) + ((k+1)\\delta - (x-(M-k)\\delta)f_k(x-(M-k+1)\\delta)\\right) - f_k(x-(M-k+1)\\delta).\n",
    "\\end{equation}\n",
    "\n",
    "Using the recursive formula for $f_k$, this equals\n",
    "\n",
    "\\begin{equation}\n",
    "    f_{k+1}(x-(M-k)\\delta) - f_k(x-(M-k+1)\\delta)\n",
    "\\end{equation}\n",
    "\n",
    "so the sum telescopes to leave $f_M(x-\\delta) - f_1(x-M\\delta)$.\n",
    "\n",
    "Now we show the recursive formula for $f_{M+1}$. The pdf of a sum of random variables is equal to the convolution of the constituent pdfs so\n",
    "\n",
    "\\begin{align}\n",
    "    f_{M+1}(x)\n",
    "        & = (f_1 * f_M)(x) \\\\\n",
    "        & = \\int_{-\\infty}^{\\infty} f_1(t)f_M(x-t) \\,\\mathrm{d}t \\\\\n",
    "        & = \\frac{1}{\\delta}\\int_{-\\infty}^{\\infty} I_{[0, \\delta)}(t)f_M(x-t) \\,\\mathrm{d}t \\\\\n",
    "        & = \\frac{1}{\\delta}\\int_{0}^{\\delta} f_M(x-t) \\,\\mathrm{d}t \\\\\n",
    "        & = \\frac{1}{\\delta}\\int_{x}^{x-\\delta} f_M(s) \\,\\mathrm{d}t \\\\\n",
    "        & = \\frac{1}{\\delta}\\left(F_M(x) - F_M(x-\\delta)\\right)\n",
    "\\end{align}\n",
    "\n",
    "Using the summation formula for $F_M$, this gives\n",
    "\n",
    "\\begin{align}\n",
    "    f_{M+1}(x) = \\frac{1}{\\delta}\\sum_{k=1}^M \\left[\\frac{1}{k}\\left((x-(M-k)\\delta)f_k(x-(M-k)\\delta) + ((M-k+1)\\delta-x)f_k(x-(M-k+1)\\delta)\\right)\\right] + \\frac{1}{\\delta}(I_{[M\\delta, \\infty)}(x) - I_{[M\\delta, \\infty)}(x-\\delta)).\n",
    "\\end{align}\n",
    "\n",
    "The formula we proved at the start of the inductive step shows that this simplifies to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{M\\delta}\\left(x f_M(x) + (\\delta - x)f_M(x-\\delta)\\right) + (f_M(x-\\delta) - f_1(x-M\\delta)) + \\frac{1}{\\delta}(I_{[M\\delta, \\infty)}(x) - I_{[M\\delta, \\infty)}(x-\\delta)).\n",
    "\\end{equation}\n",
    "\n",
    "But $I_{[a, b)}(x) = I_{[a-\\delta, b-\\delta)}(x-\\delta)$, so\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{1}{\\delta}(I_{[M\\delta, \\infty)}(x) - I_{[M\\delta, \\infty)}(x-\\delta))\n",
    "        & = \\frac{1}{\\delta}(I_{[0, \\infty)}(x-M\\delta) - I_{[\\delta, \\infty)}(x-M\\delta)) \\\\\n",
    "        & = \\frac{1}{\\delta}I_{[0, \\delta)}(x-M\\delta) \\\\\n",
    "        & = f_1(x-M\\delta)\n",
    "\\end{align}\n",
    "\n",
    "and the expression above simplifies to give\n",
    "\n",
    "\\begin{equation}\n",
    "    f_{M+1}(x) = \\frac{1}{M\\delta}\\left( x f_{M}(x) + ((M+1)\\delta-x)f_{M}(x-\\delta)\\right).\n",
    "\\end{equation}\n",
    "\n",
    "Now we move onto the recursive formula for $F_{M+1}(x)$. It suffices to differentiate the expression given and show that this equals $f_{M+1}(x)$. Indeed,\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left(x F_M(x) + ((M+1)\\delta - x)F_M(x-\\delta)\\right) \n",
    "        & = F_M(x) + x f_M(x) - F_{M}(x-\\delta) + ((M+1)\\delta - x)f_M(x-\\delta) \\\\\n",
    "        & = F_M(x) - F_M(x-\\delta) + M\\delta f_{M+1}(x)\n",
    "\\end{align}\n",
    "\n",
    "using the formula we just proved. But in the course of the proof of the formula for $f_{M+1}$ we also showed that $f_{M+1}(x) = \\frac{1}{\\delta}(F_M(x) - F_M(x-\\delta)$. Thus the derivative above equals $(M+1)\\delta f_{M+1}(x)$ and\n",
    "\n",
    "\\begin{equation}\n",
    "    F_{M+1}(x) = \\frac{1}{(M+1)\\delta}\\left(xF_M(x)+((M+1)\\delta-x)F_M(x-\\delta)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "as required.\n",
    "\n",
    "We will now use this formula to prove the summation formula for $F_{M+1}(x)$. Indeed, using the summation formulas for $F_M(x)$ and $F_M(x-\\delta)$, we can expand it to\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{1}{(M+1)\\delta}\\sum_{k=1}^M \\left[\\frac{1}{k}\\Big(x(x-(M-k)\\delta)f_k(x-(M-k)\\delta) + ((M+1)\\delta -x)(x-(M-k+1)\\delta)f_k(x-(M-k+1)\\delta)\\Big)\\right] \\\\\n",
    "            + \\frac{1}{(M+1)\\delta} \\left(x I_{[M\\delta, \\infty)}(x) + ((M+1)\\delta-x)I_{[M\\delta, \\infty)}(x-\\delta)\\right).\n",
    "\\end{align}\n",
    "\n",
    "The second expression can be simplified:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{1}{(M+1)\\delta} \\left(x I_{[M\\delta, \\infty)}(x) + ((M+1)\\delta-x)I_{[M\\delta, \\infty)}(x-\\delta)\\right)\n",
    "        & = \\frac{1}{(M+1)\\delta} \\left(x I_{[M\\delta, \\infty)}(x) + ((M+1)\\delta-x)I_{[(M+1)\\delta, \\infty)}(x)\\right) \\\\\n",
    "        & = \\frac{1}{(M+1)\\delta} x I_{[M\\delta, (M+1)\\delta)}(x) + I_{[(M+1)\\delta, \\infty)}(x) \\\\\n",
    "        & = \\frac{1}{M+1} x f_1(x-M\\delta) + I_{[(M+1)\\delta, \\infty)}(x).\n",
    "\\end{align}\n",
    "\n",
    "Moreover, we can rearrange the summation to get\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{1}{M+1}\\left[\\sum_{k=1}^M \\frac{x}{k\\delta}\\Big((x-(M-k)\\delta)f_k(x-(M-k)\\delta) + ((k+1)\\delta -(x-(M-k)\\delta)f_k(x-(M-k+1)\\delta)\\Big) + \\frac{M-k+1}{k}(x - (M+1)\\delta)f_k(x-(M-k+1)\\delta)\\right]\n",
    "\\end{align}\n",
    "\n",
    "and using the recursive formula for $f_k$, this simplifies to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{M+1} \\sum_{k=1}^M \\left[x f_{k+1}(x-(M-k)\\delta) + \\frac{M-k+1}{k}(x-(M+1)\\delta)f_k(x-(M-k+1)\\delta).\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Thus\n",
    "\n",
    "\\begin{align}\n",
    "    F_{M+1}(x) \n",
    "        & = \\frac{x}{M+1}f_{M+1}(x) + \\sum_{k=2}^M \\frac{1}{M+1}\\left( x + \\frac{M-k+1}{k}(x-(M+1)\\delta)\\right)f_k(x-(M-k+1)\\delta) + I_{[(M+1)\\delta, \\infty)} \\\\\n",
    "        & = \\sum_{k=1}^{M+1}\\frac{1}{k}(x-(M-k+1)\\delta)f_k(x-(M-k+1)\\delta) + I_{[(M+1)\\delta, \\infty)}\n",
    "\\end{align}\n",
    "\n",
    "as required. This concludes the induction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have shown that $B_{i,M}(x)=\\delta f_M(x)$. It just remains the remove the assumption $\\tau_i=0$. For this it suffices to note that if $X^{\\prime}$ is uniformly distributed on $[a, a+\\delta)$ then $f_{X^{\\prime}}(x) = f_1(x-a)$. So if $X_k$ is distributed uniformly on $[a_k, a_k+\\delta)$ and $Y=\\sum_{k=1}^M X_k$ then $f_Y(x)=f_M(x-\\sum a_k)$. Thus if we choose the $a_k$ such that $\\sum a_k=\\tau_i$, then $B_{i, M}(x) = f_Y(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
