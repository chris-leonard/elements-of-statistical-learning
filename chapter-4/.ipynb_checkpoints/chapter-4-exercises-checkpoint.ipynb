{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2\n",
    "\n",
    "Suppose we have features $x \\in \\mathbb{R}^p$, a two-class response, with class sizes $N_1$, $N_2$, and the targets coded as $âˆ’N/N_1$, $N/N_2$.\n",
    "\n",
    "**(a)** Show that the LDA rule classifies to class 2 if\n",
    "\n",
    "\\begin{equation}\n",
    "    x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        > \\frac{1}{2}(\\hat{\\mu}_2+\\hat{\\mu}_1)^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)-\\log(N_2/N_1),\n",
    "\\end{equation}\n",
    "\n",
    "and class 1 otherwise.\n",
    "\n",
    "**(b)** Consider minimization of the least squares criterion\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^N (y_i-\\beta_0-x_i^T\\beta)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Show that the solution $\\hat{\\beta}$ satisfies\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}_B\\right]\\beta = N(\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "(after simplification), where $\\hat{\\Sigma}_B=\\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)(\\hat{\\mu}_2-\\hat{\\mu}_1)^T$.\n",
    "\n",
    "**(c)** Hence show that $\\hat{\\Sigma}_B\\beta$ is in the direction $(\\hat{\\mu}_2-\\hat{\\mu}_1)$ and thus\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.\n",
    "\n",
    "**(d)** Show that this result holds for any (distinct) coding of the two classes.\n",
    "\n",
    "**(e)** Find the solution $\\hat{\\beta}_0$ (up to the same scalar multiple as in (c)), and hence the predicted value $\\hat{f}(x) = \\hat{\\beta}_0 + x^T\\hat{\\beta}$. Consider the following rule: classify to class 2 if $\\hat{f}(x) > 0$ and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** The discriminant functions for LDA are\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_k(x) = x\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\ln(\\pi_k).\n",
    "\\end{equation}\n",
    "\n",
    "Estimating $\\mu_k$ by $\\hat{\\mu}_k$, $\\Sigma$ by $\\hat{\\Sigma}$, and $\\pi_k$ by $N_k/N$, we classify to class 2 if and only if\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\delta}_2(x) \n",
    "        & > \\hat{\\delta}_1(x) \\\\\n",
    "    \\Longleftrightarrow x\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 - \\frac{1}{2}\\hat{\\mu}_2^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 + \\ln(N_2/N) \n",
    "        & > x\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 - \\frac{1}{2}\\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + \\ln(N_1/N) \\\\\n",
    "    \\Longleftrightarrow x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        & > \\frac{1}{2}\\left(\\hat{\\mu}_2\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 - \\hat{\\mu}_1\\hat{\\Sigma}^{-1}\\hat{\\mu}_1\\right) + \\ln(N_1/N_2) \\\\\n",
    "    \\Longleftrightarrow x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        & > \\frac{1}{2}(\\hat{\\mu}_2+\\hat{\\mu}_1)^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)-\\log(N_2/N_1)    \n",
    "\\end{align}\n",
    "\n",
    "and to class 1 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Let $\\tilde{\\mathbf{X}}$ denote the input data matrix augmented with the intercept column. We know that $\\hat{\\beta}$ satisfies $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta} = \\tilde{\\mathbf{X}}^T\\mathbf{y}$. \n",
    "\n",
    "We wish to relate $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta}$ and $\\mathbf{X}^T\\mathbf{X}\\beta$. The $(j, k)$ entry of $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$ is $\\sum_i x_{ij}x_{ik}^T$. Taking $k=0$, the $j$th entry of the 0th column is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_i x_{ij}x_{i0}^T = \\sum_{g_i=1}x_{ij} + \\sum_{g_i=2}x_{ij} = N_1\\hat{\\mu}_{1j} + N_2\\hat{\\mu}_{2j}.\n",
    "\\end{equation}\n",
    "\n",
    "So $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$ is a $(p+1)\\times(p+1)$-matrix with $(0, 0)$ entry $N$, $(1,0),\\ldots (p, 0)$ entries taken up by $N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2$, $(0,1),\\ldots (0, p)$ entries taken up by $N_1\\hat{\\mu}^T_1 + N_2\\hat{\\mu}^T_2$, and bottom right $p\\times p$ submatrix equal to $\\mathbf{X}^T\\mathbf{X}$. This implies that $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta}$ is equal to the block matrix\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        N\\beta_0 + (N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta \\\\\n",
    "        \\beta_0(N_1\\hat{\\mu}_2 + N_2\\hat{\\mu}_2) + \\mathbf{X}^T\\mathbf{X}\\beta\n",
    "    \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where the top expression is a scaler and the bottom is a $p\\times 1$ vector.\n",
    "\n",
    "We have already seen that this is equal to $\\tilde{\\mathbf{X}}^T\\mathbf{y}$. The $j$th entry of $\\tilde{\\mathbf{X}}^T\\mathbf{y}$ is\n",
    "\n",
    "\\begin{equation}    \n",
    "    \\mathbf{x}_j^T\\mathbf{y} = \\frac{N}{N_2}\\left(\\sum_{g_i=2}x_{ij}\\right) - \\frac{N}{N_1}\\left(\\sum_{g_i=1}x_{ij}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "so $\\tilde{\\mathbf{X}}^T\\mathbf{y}$ is $N(\\hat{\\mu}_2 - \\hat{\\mu}_2)$ augmented with a zero in the 0th place. Equating the expressions in the 0th place gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta.\n",
    "\\end{equation}\n",
    "\n",
    "Equating the remaining terms and substituting this in gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[ \\mathbf{X}\\mathbf{X}^T - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)^T(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\right]\\beta = N(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "It remains to show that the expression in square brackets equals $(N-2)\\hat{\\Sigma} + N\\hat{\\Sigma}_B$.\n",
    "\n",
    "Indeed,\n",
    "\n",
    "\\begin{align}\n",
    "    (N-2)\\hat{\\Sigma}\n",
    "        & = \\sum_{k=1}^2\\sum_{g_i=k} (x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T \\\\\n",
    "        & = \\sum_{k=1}^2\\sum_{g_i=k} (x_ix_i^T-\\hat{\\mu}_kx_i^T -x_i\\hat{\\mu}_k^T + \\hat{\\mu}_k\\hat{\\mu}_k^T \\\\\n",
    "        & = \\sum_{k=1}^2 \\left[\\left(\\sum_{g_i=k} x_i x_i^T\\right)-N_k\\hat{\\mu}_k\\hat{\\mu}^T_k - N_k\\hat{\\mu}_k\\hat{\\mu}^T_k + N_k\\hat{\\mu}_k\\hat{\\mu}^T_k\\right]  \\\\\n",
    "        & = \\mathbf{X}\\mathbf{X}^T - \\sum_{k=1}^2 N_k\\hat{\\mu}_k\\hat{\\mu}^T_k.\n",
    "\\end{align}\n",
    "\n",
    "So,\n",
    "\n",
    "\\begin{align}\n",
    "    (N-2)\\hat{\\Sigma} + N\\hat{\\Sigma}_B\n",
    "    & = \\mathbf{X}\\mathbf{X}^T \n",
    "        - N_1\\hat{\\mu}_1\\hat{\\mu}_1^T \n",
    "        - N_2\\hat{\\mu}_2\\hat{\\mu}_2^T\n",
    "        +\\frac{N_1N_2}{N}(\\hat{\\mu}_2 - \\hat{\\mu}_1)(\\hat{\\mu}_2 - \\hat{\\mu}_1)^T \\\\\n",
    "    & = \\mathbf{X}\\mathbf{X}^T \n",
    "        + \\left(\\frac{N_1N_2}{N} - N_1\\right) \\hat{\\mu}_1\\hat{\\mu}_1^T \n",
    "        + \\left(\\frac{N_1N_2}{N} - N_2\\right) \\hat{\\mu}_2\\hat{\\mu}_2^T \n",
    "        - \\frac{N_1N_2}{N} (\\hat{\\mu}_1\\hat{\\mu}_2^T + \\hat{\\mu}_2\\hat{\\mu}_1^T) \\\\\n",
    "    & = \\mathbf{X}\\mathbf{X}^T \n",
    "        - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)^T(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2),\n",
    "\\end{align}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** By definition, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}_B\\beta \n",
    "        = \\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta \n",
    "        = \\left[\\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta\\right] (\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}\\hat{\\beta} = \\frac{N}{N-2}\\left[ 1 - \\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta\\right] (\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "and thus $\\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Changing the coding makes little difference to our solution to part (b). Suppose that the targets are coded as $c_1$ and $c_2$. Then \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}^T\\mathbf{y} = \n",
    "        \\begin{pmatrix}\n",
    "            c_1N_1 + c_2N_2 \\\\\n",
    "            c_1N_1\\hat{\\mu}_1 + c_2N_2\\hat{\\mu}_2\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Taking this through our solution we end up with\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma} + N \\hat{\\Sigma}_B\\right]\\beta + \\frac{c_1N_1+c_2N_2}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\n",
    "        = c_1N_1\\hat{\\mu}_1 + c_2N_2\\hat{\\mu}_2.\n",
    "\\end{equation}\n",
    "\n",
    "Rearranging this gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma} + N \\hat{\\Sigma}_B\\right]\\beta = (c_2-c_1)\\frac{N_1N_2}{N}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "This implies that $\\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1)$ as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** By part (b), $\\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0 \\propto -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "with the same constant of proportionality as $\\beta\\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)$. Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x) \\propto \\left(x - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\right)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "So we classify to class 2 iff\n",
    "\n",
    "\\begin{equation}\n",
    "    x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1) > \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "Since $N=N_1 + N_2$, this is the same as the LDA rule if $N_1=N_2$. If they are not equal, the rules will be different in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
