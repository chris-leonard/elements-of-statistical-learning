{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "\n",
    "Suppose each of $K$-classes has an associated target $t_k$, which is a vector or all zeros, except a one in the $k$th position. Show that classifying to the largest element of $\\hat{y}$ amount to choosing the closest target, $\\text{min}_k \\lVert t_k - \\hat{y} \\rVert$, if the elements of $\\hat{y}$ sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Write $\\hat{y} = \\sum_{k=1}^K a_k t_k$. Then\n",
    "\n",
    "\\begin{align}\n",
    "    \\lVert t_{k_1} - \\hat{y} \\rVert \\geq \\lVert t_{k_2} - \\hat{y} \\rVert\n",
    "        & \\iff (1 - a_{k_1})^2 + a_{k_2}^2 \\geq a_{k_1}^2 + (1 - a_{k_2})^2 \\\\\n",
    "        & \\iff 2 a_{k_1} \\leq 2 a_{k_2} \\\\\n",
    "        & \\iff a_{k_1} \\leq a_{k_2}.\n",
    "\\end{align}\n",
    "\n",
    "The claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2\n",
    "\n",
    "Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The Bayes decision boundary for binary classification is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lbrace x \\,\\mid\\, P\\left(\\text{Orange} \\,\\mid\\, X = x \\right) = \\tfrac{1}{2} \\rbrace = \\lbrace x \\,\\mid\\, P\\left(\\text{Blue} \\,\\mid\\, X = x \\right) = \\tfrac{1}{2} \\rbrace.\n",
    "\\end{equation}\n",
    "\n",
    "By Bayes' Theorem,\n",
    "\n",
    "\\begin{equation}\n",
    "    P\\left(\\text{Blue} \\,\\mid\\, X = x\\right)\n",
    "        = \\frac{ P\\left(X = x \\,\\mid\\, \\text{Blue}\\right) P\\left(\\text{Blue}\\right) }\n",
    "               { P\\left(X = x \\,\\mid\\, \\text{Blue}\\right) P\\left(\\text{Blue}\\right) + P\\left(X = x \\,\\mid\\, \\text{Orange}\\right) P\\left(\\text{Orange}\\right) }.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, if $f_O$ and $f_B$ are the probability density functions for the two classes then the Bayes' decision boundary is the line\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lbrace x \\,\\mid\\, \\frac{ f_B(x) }{ f_B(x) + f_O(x) } = \\tfrac{1}{2} \\rbrace.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3\n",
    "\n",
    "Derive equation (2.24)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let $X_1, \\ldots, X_N$ be random variables representing the $N$ data points and let $D$ denote the distance from the origin to the closest data point. Since the $X_i$ are i.i.d,\n",
    "\n",
    "\\begin{align}\n",
    "    P(D \\geq d)\n",
    "        & = P \\left( \\bigcap_{n=1}^N ( \\lVert X_n \\rVert \\geq d )\\right) \\\\\n",
    "        & = \\prod_{n=1}^N P \\left( \\lVert X_n \\rVert \\geq d \\right) \\\\\n",
    "        & = P \\left( \\lVert X \\rVert \\geq d \\right)^N \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $X=X_1$. So\n",
    "\n",
    "\\begin{equation}\n",
    "    P(D \\geq d) = \\frac{1}{2} \\quad \\iff \\quad P \\left( \\lVert X \\rVert \\geq d \\right) = \\left( \\frac{1}{2}\\right)^{1/N}. \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Let $V(d, p)$ denote the volume of a $p$-dimensional sphere of radius $d$. Since the data points are distributed uniformly in the ball,\n",
    "\n",
    "\\begin{align}\n",
    "    P \\left( \\lVert X \\rVert \\leq d \\right)\n",
    "        & = \\frac{ V(d, p) }{ V(1, p) } \\\\\n",
    "    P \\left( \\lVert X \\rVert \\geq d \\right)\n",
    "        & = 1 - \\frac{ V(d, p) }{ V(1, p) }.\n",
    "\\end{align}\n",
    "\n",
    "So, the median $d$ of $D$ satisfies\n",
    "\n",
    "\\begin{equation}\n",
    "    1 - \\frac{ V(d, p) }{ V(1, p) } = \\left(\\frac{1}{2}\\right)^{1/N} \\quad \\Rightarrow \\quad \\frac{ V(d, p) }{ V(1, p) } = 1 - \\left(\\frac{1}{2}\\right)^{1/N}.\n",
    "\\end{equation}\n",
    "\n",
    "But $V(d, p)$ is proportional to $d^p$ and therefore\n",
    "\n",
    "\\begin{equation}\n",
    "    d = \\left( 1 - \\left(\\frac{1}{2}\\right)^{1/N} \\right) ^ {1/p}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.4\n",
    "\n",
    "The edge effect problem discussed on page 23 is not peculiar to uniform sampling from bounded domains. Consider inputs drawn from a spherical multinormal distribution $X \\sim N(0,~\\mathbf{I}_p)$. The squared distance from any sample point to the origin has a $\\chi^2_p$ distribution with mean $p$. Consider a prediction point $x_0$ drawn from this distribution, and let $a = x_0/\\lVert x_0 \\rVert$ be an associated unit vector. Let $z_i = a^T x_i$ be the projection of each of the training points on this direction.\n",
    "    \n",
    "Show that the $z_i$ are distributed $N(0,~1)$ with expected squared distance from the origin 1, while the target point has expected squared distance $p$ from the origin.\n",
    "\n",
    "Hence for $p = 10$, a randomly drawn test point is about 3.1 standard deviations from the origin, while all the training points are on average one standard deviation along direction $a$. So most prediction points see themselves as lying on the edge of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution $N(0,~\\mathbf{I}_p)$ is spherically symmetric. Since $a$ is normalised, $z_i$ is independent of $x_0$. Hence is suffices to determine the distribution of one of the components of $x_i$.\n",
    "\n",
    "Let $\\pi: \\mathbb{R}^p \\to \\mathbb{R}$ denote the projection onto its first component and let $f_X$ denote the probability density function of $X$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\pi(X)\\leq a) = \\int f_X(x) \\,\\mathrm{d}x,\n",
    "\\end{equation}\n",
    "\n",
    "where the integral is taken over all $x\\in \\mathbb{R}^p$ whose first component is less than or equal to $a$. Since $f_X$ is a product of standard normal pdfs $f_N$ for the different components, the integral above simplifies to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left( \\int_{x \\leq a} f_N(x) \\,\\mathrm{d}x \\right) \\cdot \\left( \\int_{\\mathbb{R}} f_N(x) \\,\\mathrm{d}x \\right)^{N-1} = \\int_{x \\leq a} f_N(x) \\,\\mathrm{d}x.\n",
    "\\end{equation}\n",
    "\n",
    "Thus $z_i$ has a standard normal distribution and $E(Z^2) = \\text{Var}(Z) + E(Z)^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.5\n",
    "\n",
    "**(a)** Derive equation (2.27). The last line makes use of (3.8) through a conditioning argument.\n",
    "\n",
    "**(b)** Derive equation (2.28), making use of the cyclic property of the trace operator (trace(AB) = trace(BA)), and its linearity (which allows us to interchange the order of trace and expectation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)**  We have\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{EPE}(x_0)\n",
    "        & = \\text{E}_{y_0, \\mathcal{T}} \\left( (y_0 - \\hat{y}_0)^2\\right) \\\\\n",
    "        & = \\text{E}_{y_0} \\left( \\text{E}_{\\mathcal{T}} \\left( (y_0 - \\hat{y}_0)^2 \\mid y_0 \\right)\\right) \\\\\n",
    "        & = \\text{E}_{y_0} \\left( \\text{E}_{\\mathcal{T}} (\\hat{y}_0^2) \n",
    "            - 2y_0 \\text{E}_{\\mathcal{T}} (\\hat{y}_0) \n",
    "            + y_0^2\\right) \\\\\n",
    "        & = \\text{E}_{\\mathcal{T}} (\\hat{y}_0^2) \n",
    "            - 2 \\text{E}_{y_0}(y_0) \\text{E}_{\\mathcal{T}}(\\hat{y}_0) \n",
    "            + \\text{E}_{y_0}(y_0^2) \\\\\n",
    "        & = \\text{Var}(\\hat{y}_0) \n",
    "            + \\left( \\text{E} (\\hat{y}_0) \n",
    "            - \\text{E} (y_0) \\right)^2 \n",
    "            + \\text{Var}(y_0) \\\\\n",
    "        & = \\text{Var}(x_0^{\\text{T}} \\hat{\\beta}) \n",
    "            + \\left( x_0^{\\text{T}} \\beta \n",
    "            - x_0^{\\text{T}} \\beta\\right)^2 \\sigma^2 \\\\\n",
    "        & = x_0^{\\text{T}} \\text{Var}(\\hat{\\beta}) x_0 + \\sigma^2.\n",
    "\\end{align}\n",
    "\n",
    "We can split $\\mathcal{T} = (\\mathbf{X}, \\mathbf{y})$. So using the conditional variance identity:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Var}_{\\mathcal{T}}(\\hat{\\beta})\n",
    "        & = \\text{E}_{\\mathbf{X}}\\left( \\text{Var}_{\\mathbf{y}}(\\hat{\\beta} \\mid \\mathbf{X} )\\right)\n",
    "            +  \\text{Var}_{\\mathbf{X}}\\left( \\text{E}_{\\mathbf{y}} (\\hat{\\beta} \\mid \\mathbf{X} )\\right) \\\\\n",
    "        & = \\text{E}_{\\mathbf{X}}\\left( (\\mathbf{X}^{\\text{T}} \\mathbf{X})^{-1}\\sigma^2 \\right)\n",
    "            + \\text{Var}_{\\mathbf{X}}(\\beta) \\\\\n",
    "        & = \\text{E}\\left( (\\mathbf{X}^{\\text{T}} \\mathbf{X})^{-1}\\sigma^2 \\right),\n",
    "\\end{align}\n",
    "\n",
    "where the second line used (3.8). Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{EPE}(x_0) = \\sigma^2 \\text{E}\\left( x_0^{\\text{T}} (\\mathbf{X}^{\\text{T}} \\mathbf{X})^{-1} x_0\\right) + \\sigma^2\n",
    "\\end{equation}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  First observe that the $(i, j)$ entry of $\\mathbf{X}^{\\text{T}}\\mathbf{X}$ is $\\sum_k x_{ki}x_{kj}$, that is, $N$ times the sample mean of $X_iX_j$. Since $\\text{E}(X) = 0$ by assumption, the covariance matrix $\\text{Var}(X)$ has $(i, j)$ entry $\\text{E}(X_iX_j)$ and so by the Law of Large Numbers $\\mathbf{X}^{\\text{T}}\\mathbf{X}\\approx N\\text{Var}(X)$ for $N$ large. \n",
    "\n",
    "Using the previous result and the fact that $\\text{Var}(X)$ is independent of the sample $\\mathcal{T}$,\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{E}_{x_0} \\text{EPE}(x_0)\n",
    "        & = \\text{E}_{x_0}\\left( \\text{E}_{\\mathcal{T}} \\left( \\sigma^2 x_0^{\\text{T}} (N \\text{Var}(X))^{-1} x_0)\\right)\\right) + \\sigma^2 \\\\\n",
    "        & = \\frac{\\sigma^2}{N} \\text{E}_{x_0}\\left( x_0^{\\text{T}} \\text{Var}(X)^{-1} x_0\\right) + \\sigma^2.\n",
    "\\end{align}\n",
    "\n",
    "Using the cyclic property of the trace and the fact that expectation is linear (and hence commutes with trace),\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{E}_{x_0}\\left( x_0^{\\text{T}} \\text{Var}(X)^{-1} x_0\\right)\n",
    "        & = \\text{E}_{x_0}\\left( \\text{Tr}\\left( x_0^{\\text{T}} \\text{Var}(X)^{-1} x_0\\right)\\right) \\\\\n",
    "        & = \\text{E}_{x_0}\\left( \\text{Tr}\\left( \\text{Var}(X)^{-1} x_0x_0^{\\text{T}}\\right)\\right) \\\\\n",
    "        & = \\text{Tr}\\left( \\text{E}_{x_0}\\left( \\text{Var}(X)^{-1} x_0x_0^{\\text{T}}\\right)\\right) \\\\\n",
    "        & = \\text{Tr}\\left( \\text{Var}(X)^{-1} \\text{E}_{x_0}\\left( x_0x_0^{\\text{T}}\\right)\\right) \\\\\n",
    "        & = \\text{Tr}\\left( \\text{Var}(X)^{-1} \\text{Var}(x_0)\\right),\n",
    "\\end{align}\n",
    "\n",
    "as $\\text{E}(x_0) = 0$. But $x_0$ is just an observation of $X$, so $\\text{Var}(X)^{-1} \\text{Var}(x_0) = I_p$ and therefore\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{E}_{x_0} \\text{EPE}(x_0) = \\left( \\frac{p}{N}\\right) \\sigma^2 + \\sigma^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.6\n",
    "\n",
    "Consider a regression problem with inputs $x_i$ and outputs $y_i$, and a parameterized model $f_{\\theta}(x)$ to be fit by least squares. Show that if there are observations with tied or identical values of $x$, then the fit can be obtained from a reduced weighted least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Assume without loss of generality that $x_1 = x_2$. Then\n",
    "\n",
    "\\begin{align}\n",
    "    (y_1 - f_{\\theta}(x_1))^2 + (y_2 - f_{\\theta}(x_2))^2\n",
    "        & = y_1^2 + y_2^2 - 2(y_1 + y_2) f_{\\theta}(x) + 2f_{\\theta}(x)^2 \\\\\n",
    "        & = 2\\left( \\frac{y_1 + y_2}{2} - f_{\\theta}(x)\\right)^2 - y_1y_2 + \\frac{y_1^2 + y_2^2}{2},\n",
    "\\end{align}\n",
    "\n",
    "where we write $x = x_1 = x_2$. In choosing $\\theta$ to minimise this we can ignore the last two terms and so the original least squares problem is equivalent to minimising\n",
    "\n",
    "\\begin{equation}\n",
    "    2\\left( \\frac{y_1 + y_2}{2} - f_{\\theta}(x)\\right)^2 + \\sum_{i=3}^{N} \\left( y_i - f_{\\theta}(x_i)\\right)^2 \n",
    "\\end{equation}\n",
    "\n",
    "over $\\theta$; a reduced weighted least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.7\n",
    "\n",
    "Suppose we have a sample of $N$ pairs $x_i$, $y_i$ drawn i.i.d. from the distribution characterized as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    & x_i \\sim h(x), \\text{ the design density} \\\\\n",
    "    & y_i = f(x_i) + \\epsilon_i, \\text{ is the regression function} \\\\\n",
    "    & \\epsilon_i \\sim (0, \\sigma^2) \\text{ (mean zero, variance $\\sigma^2$)}\n",
    "\\end{align}\n",
    "\n",
    "We construct an estimator for $f$ linear in the $y_i$,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x_0) = \\sum_{i=1}^N l_i(x_i;\\mathcal{X})y_i,\n",
    "\\end{equation}\n",
    "\n",
    "where the weights $l_i(x_i;\\mathcal{X})$ do not depend on the $y_i$, but do depend on the entire training sequence of $x_i$, denoted here by $\\mathcal{X}$.\n",
    "\n",
    "**(a)**  Show that linear regression and $k$-nearest-neighbor regression are members of this class of estimators. Describe explicitly the weights $l_i(x_i;\\mathcal{X})$ in each of these cases.\n",
    "\n",
    "**(b)**  Decompose the conditional mean-squared error\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{E}_{\\mathcal{Y}|\\mathcal{X}}\\left( (f(x_0) - \\hat{f}(x_0))^2\\right)\n",
    "\\end{equation}\n",
    "\n",
    "into a conditional squared bias and a conditional variance component. Like $\\mathcal{X}$, $\\mathcal{Y}$ represents the entire training sequence of $y_i$. \n",
    "\n",
    "**(c)** Decompose the (unconditional) mean-squared error\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{E}_{\\mathcal{Y}, \\mathcal{X}}\\left( (f(x_0) - \\hat{f}(x_0))^2\\right)\n",
    "\\end{equation}\n",
    "\n",
    "into a squared bias and a variance component.\n",
    "\n",
    "**(d)**  Establish a relationship between the squared biases and variances in the above two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** In linear regression we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x_0) = x_0^{\\text{T}} \\hat{\\beta} =  x_0^{\\text{T}} (\\mathbf{X}^{\\text{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\text{T}} \\mathbf{y} = \\sum_{i=1}^N l_i(x_i;\\mathcal{X})y_i,\n",
    "\\end{equation}\n",
    "\n",
    "where $l_i(x_i;\\mathcal{X})$ is the $i$th element of $x_0^{\\text{T}} (\\mathbf{X}^{\\text{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\text{T}}$.\n",
    "\n",
    "In $k$-nearest neighbour,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x_0) = \\frac{1}{k} \\sum_{x_i\\in N_k(x_0)} y_i = \\sum_{i=1}^N l_i(x_i;\\mathcal{X})y_i,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "    l_i(x_i;\\mathcal{X})\n",
    "        = \\begin{cases}\n",
    "            \\frac{1}{k} & \\text{if } x_i\\in N_k(x_0) \\\\\n",
    "            0 & \\text{otherwise}.\n",
    "          \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**  We have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{E}_{\\mathcal{Y} | \\mathcal{X}}\\left( \\hat{f}(x_0)\\right)\n",
    "        = \\sum_{i=1}^N l_i(x_i;\\mathcal{X}) \\text{E}_{\\mathcal{Y} | \\mathcal{X}}(y_i)\n",
    "        = \\sum_{i=1}^N l_i(x_i;\\mathcal{X}) f(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Var}_{\\mathcal{Y} | \\mathcal{X}}\\left( \\hat{f}(x_0)\\right)\n",
    "        = \\sum_{i=1}^N l_i(x_i;\\mathcal{X}) \\text{Var}_{\\mathcal{Y} | \\mathcal{X}}(y_i)\n",
    "        = \\left(\\sum_{i=1}^N l_i(x_i;\\mathcal{X})^2\\right)\\sigma^2.\n",
    "\\end{equation}\n",
    "\n",
    "So the bias-variance decomposition is\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{E}_{\\mathcal{Y}|\\mathcal{X}}\\left( (f(x_0) - \\hat{f}(x_0))^2\\right)\n",
    "        & = \\text{Var}_{\\mathcal{Y}|\\mathcal{X}}\\left(\\hat{f}(x_0)\\right) \n",
    "            + \\text{Var}_{\\mathcal{Y}|\\mathcal{X}}\\left(\\hat{f}(x_0)\\right)^2 \\\\\n",
    "        & = \\left(\\sum_{i=1}^N l_i(x_i;\\mathcal{X})^2\\right)\\sigma^2 \n",
    "            + \\left(\\sum_{i=1}^N l_i(x_i;\\mathcal{X}) f(x_i) - f(x_0)\\right)^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c), (d)**  I really don't know what they're looking for here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.8\n",
    "\n",
    "Compare the classification performance of linear regression and k– nearest neighbor classification on the zipcode data. In particular, consider only the $2$’s and $3$’s, and $k = 1, 3, 5, 7$ and $15$. Show both the training and test error for each choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing packages. The models.py module was written for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models\n",
    "#from models import train_linear_model, apply_linear_model, apply_k_nearest_neighbour, classification_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use Pandas to import and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train = pd.read_csv('zipcode_train.csv', sep = ' ', header = None)\n",
    "test = pd.read_csv('zipcode_test.csv', sep = ' ', header = None)\n",
    "\n",
    "# Rename first columns to y for clarity\n",
    "train = train.rename(columns = {0 : 'y'})\n",
    "test = test.rename(columns = {0 : 'y'})\n",
    "\n",
    "# Restrict to 2s and 3s\n",
    "train = train[train.y.isin([2, 3])]\n",
    "test = test[test.y.isin([2, 3])]\n",
    "\n",
    "# train has an extra NaN column for some reason - drop it\n",
    "train = train.drop(columns = 257)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now switch to Numpy array for linear algebra operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up numpy arrays\n",
    "Xtrain = train.iloc[:, 1:].to_numpy()\n",
    "ytrain = train.loc[:, 'y'].to_numpy()\n",
    "\n",
    "Xtest = test.iloc[:, 1:].to_numpy()\n",
    "ytest = test.loc[:, 'y'].to_numpy()\n",
    "\n",
    "# Store dimensions\n",
    "ptrain, Ntrain = Xtrain.shape\n",
    "ptest, Ntest = Xtest.shape\n",
    "\n",
    "# Replace 2, 3 in y with 0, 1 resp.\n",
    "ytrain = np.where(ytrain == 2, 0, 1)\n",
    "ytest = np.where(ytest == 2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First train and test a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error for Linear Model\n",
      "\n",
      "Training error: 0.005759539236861051\n",
      "Test error: 0.04120879120879121\n"
     ]
    }
   ],
   "source": [
    "# Train linear model\n",
    "beta = models.train_linear_model(Xtrain, ytrain)\n",
    "\n",
    "# Calculate classification errors\n",
    "model = models.apply_linear_model\n",
    "model_args = [beta]\n",
    "\n",
    "print('Classification Error for Linear Model\\n')\n",
    "\n",
    "error_train = models.classification_error(Xtrain, ytrain, model, *model_args)\n",
    "error_test = models.classification_error(Xtest, ytest, model, *model_args)\n",
    "\n",
    "print('Training error: {}'.format(error_train))\n",
    "print('Test error: {}'.format(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $k$-nearest neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error for k-Nearest Neighbours\n",
      "\n",
      "Training error (k = 1): 0.0\n",
      "Test error (k = 1): 0.024725274725274724\n",
      "\n",
      "\n",
      "Training error (k = 3): 0.005039596832253419\n",
      "Test error (k = 3): 0.03021978021978022\n",
      "\n",
      "\n",
      "Training error (k = 5): 0.005759539236861051\n",
      "Test error (k = 5): 0.03021978021978022\n",
      "\n",
      "\n",
      "Training error (k = 7): 0.0064794816414686825\n",
      "Test error (k = 7): 0.03296703296703297\n",
      "\n",
      "\n",
      "Training error (k = 15): 0.009359251259899209\n",
      "Test error (k = 15): 0.038461538461538464\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification errors\n",
    "model = models.apply_k_nearest_neighbour\n",
    "\n",
    "print('Classification Error for k-Nearest Neighbours')\n",
    "\n",
    "for k in [1, 3, 5, 7, 15]:\n",
    "    print('\\n')\n",
    "    model_args = [Xtrain, ytrain, k]\n",
    "    \n",
    "    error_train = models.classification_error(Xtrain, ytrain, model, *model_args)\n",
    "    print('Training error (k = {}): {}'.format(k, error_train))\n",
    "    \n",
    "    error_test = models.classification_error(Xtest, ytest, model, *model_args)\n",
    "    print('Test error (k = {}): {}'.format(k, error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are surprising. We would expect the test error for $k$-nearest neighbours to be concave, but in fact it is most accurate on the test set when $k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.9\n",
    "\n",
    "Consider a linear regression model with $p$ parameters, fit by least squares to a set of training data $(x_1, y_1),\\ldots, (x_N, y_N)$ drawn at random from a population. Let $\\hat{β}$ be the least squares estimate. Suppose we have some test data $(\\tilde{x}_1,\\tilde{y}_1), \\ldots, (\\tilde{x}_N, \\tilde{y}_N)$ drawn at random from the same populations as the training data. If $R_{\\text{tr}}(\\beta) = \\frac{1}{N}\\sum_1^N(y_i-\\beta^{\\text{T}}x_i)^2$ and $R_{\\text{te}}(\\beta) = \\frac{1}{M}\\sum_1^M(\\tilde{y}_i - \\beta^{\\text{T}}\\tilde{x}_i)^2$, prove that\n",
    "\n",
    "\\begin{equation}\n",
    "        \\text{E}\\left[ R_{\\text{tr}}(\\hat{\\beta})\\right] \\leq \\text{E}\\left[ R_{\\text{te}}(\\hat{\\beta})\\right],\n",
    "\\end{equation}\n",
    " \n",
    "where the expectations are over all that is random in each expression. [This exercise was brought to our attention by Ryan Tibshirani, from a homework assignment given by Andrew Ng.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "First assume that $N = M$. Note that $\\text{E}(R_{\\text{tr}}(\\hat{\\beta}))$ is the expected minimum value of $\\frac{1}{N} (\\mathbf{y}-\\mathbf{X}\\beta)^{\\text{T}}(\\mathbf{y} - \\mathbf{X}\\beta)$ over all $\\beta$. Conversely, $\\text{E}(R_{\\text{te}}(\\hat{\\beta}))$ is the expected value of the same expression for a value of $\\beta$ trained on a different data set and so must be larger. But, the test data is i.i.d, so $\\text{E}(R_{\\text{tr}}(\\hat{\\beta})) = \\text{E}\\left((y-\\hat{\\beta}x)^2\\right)$ is independent of $M$. The conclusion follows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
