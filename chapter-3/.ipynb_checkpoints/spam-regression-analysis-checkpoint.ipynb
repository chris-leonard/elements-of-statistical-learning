{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Analysis of Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will conduct linear regression analysis on a set of spam data. The purpose of this is first to complete Exercise 3.17 from Hastie, Tibshirani, and Friedman, but also to get practical experience in applying the principles and methods in Chapter 3 of the text. Exercise 3.17 requires us to compare the test error and standard error of the following linear regression techniques on the spam data:\n",
    "- Least Squares\n",
    "- Best Subset Selection\n",
    "- Ridge Regression\n",
    "- Lasso\n",
    "- Principal Components Regression\n",
    "- Partial Least Squares\n",
    "\n",
    "Because I want to dive a little deeper, I will spend some time analysing the performance of each of these methods rather than just blindly implementing them. Here are some other things I hope to accomplish:\n",
    "- Analyse the covariance between features and conduct principal component analysis to reduce the number of features while losing very little relevant information\n",
    "- Use hypothesis testing to quantify the significance of different variables and create confidence bounds for the least squares fit\n",
    "- Compare best subset selection with forward- and backward-stepwise selection and forward-stagewise regression\n",
    "- Analyse how training error and cross-validation error change as complexity parameters vary\n",
    "- Plot the full coefficient paths for ridge regression, lasso, and LAR\n",
    "\n",
    "For some of these, in particular plotting coefficient paths, it may be necessary to restrict to a subset of features for readability.\n",
    "\n",
    "Also note that if our goal was simply to obtain the best linear regression model possible, we would spend some time expanding our set of features by taking polynomials, logs, etc. Since this is the focus of Chapter 5, I will delay this work until then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was obtained from the website for the textbook which also contains a detailed description of it: \n",
    "\n",
    "https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "It consists of 4601 data points, each representing an e-mail. There are 57 inputs and one binary output: 1 if that e-mail was spam, 0 if it wasn't. The set of features comprises (in order):\n",
    "- 48 continuous real variables with range [0, 100] giving the percentage of words in the e-mail that match WORD;\n",
    "- 6 continuous real variables with range [0, 100] that gives the percentage of characters matching CHAR;\n",
    "- 3 integer variables taking any value $\\geq 1$ that describe the distribution of capital letters in the e-mail.\n",
    "\n",
    "There are no missing values for the features. For more details see the description of the dataset at the URL above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data, extract some basic information, and partition it into training, cross-validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "spam = pd.read_csv('spam-data', delim_whitespace = True, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.333</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.277</td>\n",
       "      <td>12</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>1.890</td>\n",
       "      <td>18</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.611</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.656</td>\n",
       "      <td>54</td>\n",
       "      <td>479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1     2    3     4    5     6    7     8     9   ...   48     49  \\\n",
       "1996  0.00  0.0  0.00  0.0  0.00  0.0  0.00  0.0  0.00  0.00  ...  0.0  0.000   \n",
       "2660  0.00  0.0  0.00  0.0  0.68  0.0  0.00  0.0  0.00  0.68  ...  0.0  0.828   \n",
       "1403  0.25  0.0  0.25  0.0  0.50  0.0  0.25  0.0  0.00  0.00  ...  0.0  0.041   \n",
       "1923  0.00  0.0  2.00  0.0  0.00  0.0  0.00  0.0  0.00  0.00  ...  0.0  0.180   \n",
       "1391  0.60  0.0  0.36  0.0  1.44  0.0  0.00  0.0  0.24  1.32  ...  0.0  0.040   \n",
       "\n",
       "         50     51     52     53     54  55   56  57  \n",
       "1996  0.000  0.000  0.000  0.000  1.333   3   12   0  \n",
       "2660  0.621  0.000  0.000  0.000  2.277  12  123   0  \n",
       "1403  0.000  0.082  0.041  0.041  1.890  18  225   1  \n",
       "1923  0.000  0.000  0.000  0.000  1.611  10   29   0  \n",
       "1391  0.000  0.102  0.224  0.000  3.656  54  479   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "spam.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.305358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>5.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.380</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.160</td>\n",
       "      <td>18.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.059824</td>\n",
       "      <td>0.201545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.541702</td>\n",
       "      <td>0.861698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.800</td>\n",
       "      <td>9.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.093930</td>\n",
       "      <td>0.301036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.058626</td>\n",
       "      <td>0.335184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>0.258843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.248848</td>\n",
       "      <td>0.825792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.142586</td>\n",
       "      <td>0.444055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.184745</td>\n",
       "      <td>0.531122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>1.662100</td>\n",
       "      <td>1.775481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.310</td>\n",
       "      <td>2.640</td>\n",
       "      <td>18.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.085577</td>\n",
       "      <td>0.509767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.809761</td>\n",
       "      <td>1.200810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.220</td>\n",
       "      <td>1.270</td>\n",
       "      <td>11.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.121202</td>\n",
       "      <td>1.025756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.101645</td>\n",
       "      <td>0.350286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.094269</td>\n",
       "      <td>0.442636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.549504</td>\n",
       "      <td>1.671349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.265384</td>\n",
       "      <td>0.886955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.767305</td>\n",
       "      <td>3.367292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>33.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.124845</td>\n",
       "      <td>0.538576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.098915</td>\n",
       "      <td>0.593327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.102852</td>\n",
       "      <td>0.456682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.064753</td>\n",
       "      <td>0.403393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>0.328559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.097229</td>\n",
       "      <td>0.555907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.047835</td>\n",
       "      <td>0.329445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.105412</td>\n",
       "      <td>0.532260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.402623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.136953</td>\n",
       "      <td>0.423451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.220651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.078629</td>\n",
       "      <td>0.434672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.064834</td>\n",
       "      <td>0.349916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.043667</td>\n",
       "      <td>0.361205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.132339</td>\n",
       "      <td>0.766819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>0.223812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.079196</td>\n",
       "      <td>0.621976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.301224</td>\n",
       "      <td>1.011687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.110</td>\n",
       "      <td>21.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.179824</td>\n",
       "      <td>0.911119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.076274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>0.285735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.188</td>\n",
       "      <td>9.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.315</td>\n",
       "      <td>32.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>6.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.588</td>\n",
       "      <td>2.276</td>\n",
       "      <td>3.706</td>\n",
       "      <td>1102.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>9989.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>266.000</td>\n",
       "      <td>15841.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4601.0</td>\n",
       "      <td>0.394045</td>\n",
       "      <td>0.488698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count        mean         std  min     25%     50%      75%        max\n",
       "0   4601.0    0.104553    0.305358  0.0   0.000   0.000    0.000      4.540\n",
       "1   4601.0    0.213015    1.290575  0.0   0.000   0.000    0.000     14.280\n",
       "2   4601.0    0.280656    0.504143  0.0   0.000   0.000    0.420      5.100\n",
       "3   4601.0    0.065425    1.395151  0.0   0.000   0.000    0.000     42.810\n",
       "4   4601.0    0.312223    0.672513  0.0   0.000   0.000    0.380     10.000\n",
       "5   4601.0    0.095901    0.273824  0.0   0.000   0.000    0.000      5.880\n",
       "6   4601.0    0.114208    0.391441  0.0   0.000   0.000    0.000      7.270\n",
       "7   4601.0    0.105295    0.401071  0.0   0.000   0.000    0.000     11.110\n",
       "8   4601.0    0.090067    0.278616  0.0   0.000   0.000    0.000      5.260\n",
       "9   4601.0    0.239413    0.644755  0.0   0.000   0.000    0.160     18.180\n",
       "10  4601.0    0.059824    0.201545  0.0   0.000   0.000    0.000      2.610\n",
       "11  4601.0    0.541702    0.861698  0.0   0.000   0.100    0.800      9.670\n",
       "12  4601.0    0.093930    0.301036  0.0   0.000   0.000    0.000      5.550\n",
       "13  4601.0    0.058626    0.335184  0.0   0.000   0.000    0.000     10.000\n",
       "14  4601.0    0.049205    0.258843  0.0   0.000   0.000    0.000      4.410\n",
       "15  4601.0    0.248848    0.825792  0.0   0.000   0.000    0.100     20.000\n",
       "16  4601.0    0.142586    0.444055  0.0   0.000   0.000    0.000      7.140\n",
       "17  4601.0    0.184745    0.531122  0.0   0.000   0.000    0.000      9.090\n",
       "18  4601.0    1.662100    1.775481  0.0   0.000   1.310    2.640     18.750\n",
       "19  4601.0    0.085577    0.509767  0.0   0.000   0.000    0.000     18.180\n",
       "20  4601.0    0.809761    1.200810  0.0   0.000   0.220    1.270     11.110\n",
       "21  4601.0    0.121202    1.025756  0.0   0.000   0.000    0.000     17.100\n",
       "22  4601.0    0.101645    0.350286  0.0   0.000   0.000    0.000      5.450\n",
       "23  4601.0    0.094269    0.442636  0.0   0.000   0.000    0.000     12.500\n",
       "24  4601.0    0.549504    1.671349  0.0   0.000   0.000    0.000     20.830\n",
       "25  4601.0    0.265384    0.886955  0.0   0.000   0.000    0.000     16.660\n",
       "26  4601.0    0.767305    3.367292  0.0   0.000   0.000    0.000     33.330\n",
       "27  4601.0    0.124845    0.538576  0.0   0.000   0.000    0.000      9.090\n",
       "28  4601.0    0.098915    0.593327  0.0   0.000   0.000    0.000     14.280\n",
       "29  4601.0    0.102852    0.456682  0.0   0.000   0.000    0.000      5.880\n",
       "30  4601.0    0.064753    0.403393  0.0   0.000   0.000    0.000     12.500\n",
       "31  4601.0    0.047048    0.328559  0.0   0.000   0.000    0.000      4.760\n",
       "32  4601.0    0.097229    0.555907  0.0   0.000   0.000    0.000     18.180\n",
       "33  4601.0    0.047835    0.329445  0.0   0.000   0.000    0.000      4.760\n",
       "34  4601.0    0.105412    0.532260  0.0   0.000   0.000    0.000     20.000\n",
       "35  4601.0    0.097477    0.402623  0.0   0.000   0.000    0.000      7.690\n",
       "36  4601.0    0.136953    0.423451  0.0   0.000   0.000    0.000      6.890\n",
       "37  4601.0    0.013201    0.220651  0.0   0.000   0.000    0.000      8.330\n",
       "38  4601.0    0.078629    0.434672  0.0   0.000   0.000    0.000     11.110\n",
       "39  4601.0    0.064834    0.349916  0.0   0.000   0.000    0.000      4.760\n",
       "40  4601.0    0.043667    0.361205  0.0   0.000   0.000    0.000      7.140\n",
       "41  4601.0    0.132339    0.766819  0.0   0.000   0.000    0.000     14.280\n",
       "42  4601.0    0.046099    0.223812  0.0   0.000   0.000    0.000      3.570\n",
       "43  4601.0    0.079196    0.621976  0.0   0.000   0.000    0.000     20.000\n",
       "44  4601.0    0.301224    1.011687  0.0   0.000   0.000    0.110     21.420\n",
       "45  4601.0    0.179824    0.911119  0.0   0.000   0.000    0.000     22.050\n",
       "46  4601.0    0.005444    0.076274  0.0   0.000   0.000    0.000      2.170\n",
       "47  4601.0    0.031869    0.285735  0.0   0.000   0.000    0.000     10.000\n",
       "48  4601.0    0.038575    0.243471  0.0   0.000   0.000    0.000      4.385\n",
       "49  4601.0    0.139030    0.270355  0.0   0.000   0.065    0.188      9.752\n",
       "50  4601.0    0.016976    0.109394  0.0   0.000   0.000    0.000      4.081\n",
       "51  4601.0    0.269071    0.815672  0.0   0.000   0.000    0.315     32.478\n",
       "52  4601.0    0.075811    0.245882  0.0   0.000   0.000    0.052      6.003\n",
       "53  4601.0    0.044238    0.429342  0.0   0.000   0.000    0.000     19.829\n",
       "54  4601.0    5.191515   31.729449  1.0   1.588   2.276    3.706   1102.500\n",
       "55  4601.0   52.172789  194.891310  1.0   6.000  15.000   43.000   9989.000\n",
       "56  4601.0  283.289285  606.347851  1.0  35.000  95.000  266.000  15841.000\n",
       "57  4601.0    0.394045    0.488698  0.0   0.000   0.000    1.000      1.000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display descriptive statistics\n",
    "spam.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that most of the features are skewed extremely far right in their distributions. For example, although the feature labelled as '4' has max 10, ~60% of the time it is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data points where feature 4 has value zero: 0.62\n"
     ]
    }
   ],
   "source": [
    "# Proportion of \n",
    "prop_zero = (spam[4] == 0).value_counts()[True] / len(spam)\n",
    "print('Proportion of data points where feature 4 has value zero: {:.2f}'.format(prop_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Value counts of feature 4')"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram giving value counts of feature 4\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "\n",
    "ax.hist(spam[4], bins  = 100, range = (0, 2))\n",
    "\n",
    "ax.set_title('Value counts of feature 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at correlation between features to understand redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with the greatest correlations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>33</th>\n",
       "      <th>35</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607214</td>\n",
       "      <td>0.660284</td>\n",
       "      <td>0.657941</td>\n",
       "      <td>0.626124</td>\n",
       "      <td>0.602230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.607214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737555</td>\n",
       "      <td>0.735187</td>\n",
       "      <td>0.677790</td>\n",
       "      <td>0.699918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.660284</td>\n",
       "      <td>0.737555</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996066</td>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.848021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.657941</td>\n",
       "      <td>0.735187</td>\n",
       "      <td>0.996066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727119</td>\n",
       "      <td>0.845359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.626124</td>\n",
       "      <td>0.677790</td>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.727119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.674249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.602230</td>\n",
       "      <td>0.699918</td>\n",
       "      <td>0.848021</td>\n",
       "      <td>0.845359</td>\n",
       "      <td>0.674249</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          29        30        31        33        35        39\n",
       "29  1.000000  0.607214  0.660284  0.657941  0.626124  0.602230\n",
       "30  0.607214  1.000000  0.737555  0.735187  0.677790  0.699918\n",
       "31  0.660284  0.737555  1.000000  0.996066  0.729750  0.848021\n",
       "33  0.657941  0.735187  0.996066  1.000000  0.727119  0.845359\n",
       "35  0.626124  0.677790  0.729750  0.727119  1.000000  0.674249\n",
       "39  0.602230  0.699918  0.848021  0.845359  0.674249  1.000000"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation matrix\n",
    "corr = spam.corr(method = 'pearson')\n",
    "abs_corr = np.abs(corr)\n",
    "\n",
    "# Examine the highest correlations between features\n",
    "max_corr = abs_corr[abs_corr != 1].stack().nlargest(30)\n",
    "max_corr_feat = sorted(list({pair[0] for pair in max_corr.index}))\n",
    "print('Features with the greatest correlations:')\n",
    "corr.loc[max_corr_feat, max_corr_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collection of features is highly correlated. Later we will see how dropping some of these improves the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What proportion are spam vs non-spam\n",
    "data_split = (spam[57].value_counts() / len(spam)).round(3)\n",
    "print('Non-spam: {}, Spam: {}'.format(data_split[0], data_split[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a roughly equal split between spam and non-spam data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomise the rows of the data and then split it into:\n",
    "- Training set (60%)\n",
    "- Cross-validation set (20%)\n",
    "- Test set (20%)\n",
    "\n",
    "We would like to roughly preserve the 60/40 split between non-spam and spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomise rows\n",
    "spam = spam.sample(frac = 1)\n",
    "\n",
    "# Split into 3 sets\n",
    "N = spam.shape[0]\n",
    "Ntrain = int(N*0.6)\n",
    "Ncv = int(N*0.2)\n",
    "Ntest = N - Ntrain - Ncv\n",
    "\n",
    "train, cv, test = np.split(spam, [Ntrain, Ntrain + Ncv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-spam: 0.604, Spam: 0.396\n",
      "Non-spam: 0.612, Spam: 0.388\n",
      "Non-spam: 0.606, Spam: 0.394\n"
     ]
    }
   ],
   "source": [
    "# Check spam/non-spam ratio\n",
    "for df in [train, cv, test]:\n",
    "    data_split = (df[57].value_counts() / len(df)).round(3)\n",
    "    print('Non-spam: {}, Spam: {}'.format(data_split[0], data_split[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframes to preserve split\n",
    "#train.to_csv('spam-train.csv', index = False, header = False)\n",
    "#cv.to_csv('spam-cv.csv', index = False, header = False)\n",
    "#test.to_csv('spam-test.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reimport the data in this split, cast it to numpy arrays, and split it into inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport data\n",
    "train = pd.read_csv('spam-train.csv', header = None)\n",
    "cv = pd.read_csv('spam-cv.csv', header = None)\n",
    "test = pd.read_csv('spam-test.csv', header = None)\n",
    "\n",
    "# Split into inputs and outputs\n",
    "X_train = train.loc[:, :56].to_numpy()\n",
    "y_train = train.loc[:, 57].to_numpy()\n",
    "N_train = len(y_train)\n",
    "\n",
    "X_cv = cv.loc[:, :56].to_numpy()\n",
    "y_cv = cv.loc[:, 57].to_numpy()\n",
    "N_cv = len(y_cv)\n",
    "\n",
    "X_test = test.loc[:, :56].to_numpy()\n",
    "y_test = test.loc[:, 57].to_numpy()\n",
    "N_test = len(y_test)\n",
    "\n",
    "# Number of features\n",
    "num_feat = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a standard least squares fit. We wish to\n",
    "- find the parameter vector $\\hat{\\beta}$ of the least squares fit\n",
    "- estimate the standard error of $\\hat{\\beta}$ as a predictor of $\\beta$\n",
    "- calculate the $Z$-scores under the null hypothesis that each $\\beta_j = 0$\n",
    "- conduct a hypothesis test under the null hypothesis that a batch of coefficients are simultaneously zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import linalg\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual sum-of-squares: 278.946\n",
      "Unbiased estimate for variance: 0.103\n"
     ]
    }
   ],
   "source": [
    "# Create linear regresssion object\n",
    "least_squares = linear_model.LinearRegression()\n",
    "\n",
    "# Train using the data\n",
    "least_squares.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the training/cv data\n",
    "y_train_pred_ls = least_squares.predict(X_train)\n",
    "\n",
    "# Calculate an unbiased estimate of the variance in the data assuming a linear relationship\n",
    "RSS_train_ls = (y_train - y_train_pred_ls).T @ (y_train - y_train_pred_ls)\n",
    "var_est = RSS_train_ls / (N_train - num_feat - 1)\n",
    "print('Residual sum-of-squares: {:.3f}'.format(RSS_train_ls))\n",
    "print('Unbiased estimate for variance: {:.3f}'.format(var_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we estimate the standard error of $\\hat{\\beta}$ and the $Z$-scores under the hypothesis that different coefficients are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std_err</th>\n",
       "      <th>z_score</th>\n",
       "      <th>abs_z_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.206</td>\n",
       "      <td>0.015</td>\n",
       "      <td>13.663</td>\n",
       "      <td>13.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.193</td>\n",
       "      <td>0.015</td>\n",
       "      <td>13.013</td>\n",
       "      <td>13.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.008</td>\n",
       "      <td>10.957</td>\n",
       "      <td>10.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.092</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.609</td>\n",
       "      <td>9.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.219</td>\n",
       "      <td>9.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.054</td>\n",
       "      <td>0.006</td>\n",
       "      <td>9.154</td>\n",
       "      <td>9.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.276</td>\n",
       "      <td>0.030</td>\n",
       "      <td>9.103</td>\n",
       "      <td>9.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.174</td>\n",
       "      <td>0.022</td>\n",
       "      <td>8.083</td>\n",
       "      <td>8.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.978</td>\n",
       "      <td>6.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.049</td>\n",
       "      <td>0.007</td>\n",
       "      <td>6.554</td>\n",
       "      <td>6.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-6.338</td>\n",
       "      <td>6.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.089</td>\n",
       "      <td>0.015</td>\n",
       "      <td>6.039</td>\n",
       "      <td>6.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-5.379</td>\n",
       "      <td>5.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.017</td>\n",
       "      <td>5.237</td>\n",
       "      <td>5.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.015</td>\n",
       "      <td>5.114</td>\n",
       "      <td>5.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-5.085</td>\n",
       "      <td>5.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-5.077</td>\n",
       "      <td>5.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-4.860</td>\n",
       "      <td>4.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.112</td>\n",
       "      <td>0.024</td>\n",
       "      <td>4.690</td>\n",
       "      <td>4.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-4.294</td>\n",
       "      <td>4.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.046</td>\n",
       "      <td>0.013</td>\n",
       "      <td>3.662</td>\n",
       "      <td>3.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.015</td>\n",
       "      <td>3.662</td>\n",
       "      <td>3.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-3.581</td>\n",
       "      <td>3.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024</td>\n",
       "      <td>0.007</td>\n",
       "      <td>3.418</td>\n",
       "      <td>3.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.355</td>\n",
       "      <td>3.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-3.299</td>\n",
       "      <td>3.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-3.184</td>\n",
       "      <td>3.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-2.775</td>\n",
       "      <td>2.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.067</td>\n",
       "      <td>0.025</td>\n",
       "      <td>2.696</td>\n",
       "      <td>2.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-2.525</td>\n",
       "      <td>2.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>2.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-2.342</td>\n",
       "      <td>2.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-2.293</td>\n",
       "      <td>2.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-2.254</td>\n",
       "      <td>2.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029</td>\n",
       "      <td>0.013</td>\n",
       "      <td>2.195</td>\n",
       "      <td>2.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-2.182</td>\n",
       "      <td>2.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-1.942</td>\n",
       "      <td>1.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-1.929</td>\n",
       "      <td>1.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.656</td>\n",
       "      <td>1.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.647</td>\n",
       "      <td>1.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.053</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.557</td>\n",
       "      <td>1.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.491</td>\n",
       "      <td>1.491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-1.391</td>\n",
       "      <td>1.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.297</td>\n",
       "      <td>1.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-1.189</td>\n",
       "      <td>1.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.130</td>\n",
       "      <td>1.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-1.051</td>\n",
       "      <td>1.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>1.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.078</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            coef  std_err  z_score  abs_z_score\n",
       "feature                                        \n",
       "6          0.206    0.015   13.663       13.663\n",
       "intercept  0.193    0.015   13.013       13.013\n",
       "15         0.093    0.008   10.957       10.957\n",
       "4          0.092    0.010    9.609        9.609\n",
       "51         0.093    0.010    9.219        9.219\n",
       "20         0.054    0.006    9.154        9.154\n",
       "52         0.276    0.030    9.103        9.103\n",
       "22         0.174    0.022    8.083        8.083\n",
       "56         0.000    0.000    6.978        6.978\n",
       "21         0.049    0.007    6.554        6.554\n",
       "44        -0.044    0.007   -6.338        6.338\n",
       "7          0.089    0.015    6.039        6.039\n",
       "26        -0.010    0.002   -5.379        5.379\n",
       "23         0.091    0.017    5.237        5.237\n",
       "19         0.074    0.015    5.114        5.114\n",
       "45        -0.035    0.007   -5.085        5.085\n",
       "48        -0.146    0.029   -5.077        5.077\n",
       "24        -0.023    0.005   -4.860        4.860\n",
       "5          0.112    0.024    4.690        4.690\n",
       "11        -0.032    0.007   -4.294        4.294\n",
       "17         0.046    0.013    3.662        3.662\n",
       "16         0.053    0.015    3.662        3.662\n",
       "41        -0.034    0.010   -3.581        3.581\n",
       "3          0.024    0.007    3.418        3.418\n",
       "18         0.013    0.004    3.355        3.355\n",
       "32        -0.035    0.011   -3.299        3.299\n",
       "0         -0.069    0.022   -3.184        3.184\n",
       "43        -0.027    0.010   -2.775        2.775\n",
       "8          0.067    0.025    2.696        2.696\n",
       "25        -0.026    0.010   -2.525        2.525\n",
       "1         -0.012    0.005   -2.476        2.476\n",
       "42        -0.069    0.030   -2.342        2.342\n",
       "34        -0.054    0.024   -2.293        2.293\n",
       "47        -0.044    0.019   -2.254        2.254\n",
       "2          0.029    0.013    2.195        2.195\n",
       "49        -0.059    0.027   -2.182        2.182\n",
       "37        -0.071    0.037   -1.942        1.942\n",
       "29        -0.039    0.020   -1.929        1.929\n",
       "9          0.018    0.011    1.656        1.656\n",
       "53         0.019    0.012    1.647        1.647\n",
       "10         0.053    0.034    1.557        1.557\n",
       "35         0.035    0.023    1.491        1.491\n",
       "38        -0.021    0.015   -1.391        1.391\n",
       "54         0.000    0.000    1.297        1.297\n",
       "46        -0.115    0.097   -1.189        1.189\n",
       "39         0.042    0.038    1.130        1.130\n",
       "36        -0.017    0.016   -1.051        1.051\n",
       "55         0.000    0.000    1.022        1.022\n",
       "30        -0.021    0.021   -1.002        1.002\n",
       "14         0.026    0.028    0.918        0.918\n",
       "27         0.016    0.018    0.890        0.890\n",
       "28        -0.012    0.014   -0.803        0.803\n",
       "33         0.078    0.176    0.442        0.442\n",
       "13         0.006    0.019    0.321        0.321\n",
       "50         0.014    0.076    0.185        0.185\n",
       "40         0.003    0.017    0.147        0.147\n",
       "31        -0.013    0.181   -0.073        0.073\n",
       "12        -0.001    0.021   -0.029        0.029"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate the variance of the least squares estimate\n",
    "bias = np.ones(shape=(N_train, 1))\n",
    "X_train_bias = np.concatenate((bias, X_train), axis=1)\n",
    "coef_var_est = var_est * linalg.inv(X_train_bias.T @ X_train_bias)\n",
    "\n",
    "# Display data on Z-scores of coefs\n",
    "feat = list(range(57))\n",
    "ls_coefs = pd.DataFrame(index = ['intercept'] + feat,\n",
    "                        columns = ['coef', 'std_err', 'z_score', 'abs_z_score'],\n",
    "                        dtype = 'float')\n",
    "ls_coefs.index.name = 'feature'\n",
    "\n",
    "ls_coefs.loc[features, 'coef'] = least_squares.coef_\n",
    "ls_coefs.loc['intercept', 'coef'] = least_squares.intercept_\n",
    "\n",
    "ls_coefs.loc[:, 'std_err'] = np.sqrt(np.diagonal(coef_var_est))\n",
    "\n",
    "# Z-score is the coefficient over the estimated standard error\n",
    "ls_coefs.loc[:, 'z_score'] = ls_coefs.coef / ls_coefs.std_err\n",
    "ls_coefs.loc[:, 'abs_z_score'] = np.abs(ls_coefs.z_score)\n",
    "\n",
    "# Print a summary of coefficient data ordered by absolute Z-score\n",
    "ls_coefs.sort_values('abs_z_score', ascending = False).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly a $Z$-score above 2 in absolute value is significantly non-zero at the 95% level. A number of the features have a strong effects and around 36 out of 58 terms are significantly non-zero at that level. Around 40 are significantly non-zero at the 90% level.\n",
    "\n",
    "We perform an $F$-test to test the null hypothesis that the 16 coefficients with the lowest $Z$-scores are all zero. This involves doing a least squares fit without these coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual sum of squares for reduced model: 281.2\n",
      "F-statistic: 1.363\n",
      "95.0th percentile of F-distribution with (16, 2702) degrees of freedom: 1.647\n"
     ]
    }
   ],
   "source": [
    "# Take reduced set of features\n",
    "num_feat_red = 42\n",
    "feat_red = ls_coefs.nlargest(num_feat_red, columns = 'abs_z_score').index\n",
    "#feat_red = ls_coefs[ls_coefs.abs_z_score >= 1.4].index[1:]\n",
    "feat_red = [n for n in feat_red if n != 'intercept']\n",
    "\n",
    "# Slice training set to get just features in feat_red\n",
    "ixgrid = np.ix_(list(range(N_train)), feat_red)\n",
    "X_train_red = X_train[ixgrid]\n",
    "\n",
    "# Fit least squares to reduced data\n",
    "least_squares_red = linear_model.LinearRegression()\n",
    "least_squares_red.fit(X_train_red, y_train)\n",
    "\n",
    "# Calculate the residual sum of squares for the smaller model\n",
    "y_train_pred_ls_red = least_squares_red.predict(X_train_red)\n",
    "RSS_train_ls_red = (y_train - y_train_pred_ls_red).T @ (y_train - y_train_pred_ls_red)\n",
    "print('Residual sum of squares for reduced model: {:.1f}'.format(RSS_train_ls_red))\n",
    "\n",
    "# Calculate F-statistic\n",
    "F = (RSS_train_ls_red - RSS_train_ls) / (num_feat - len(feat_red))\n",
    "F /= var_est\n",
    "print('F-statistic: {:.3f}'.format(F))\n",
    "\n",
    "# Percentiles of F-distribution\n",
    "alpha = 0.05\n",
    "df1 = num_feat - len(feat_red)\n",
    "df2 = Ntrain - num_feat - 1\n",
    "F_perc = stats.f(dfn=df1, dfd=df2).ppf(1-alpha)\n",
    "print('{perc:.1f}th percentile of F-distribution with ({df1}, {df2}) degrees of freedom: {F_perc:.3f}'.format(perc=100*(1-alpha), df1=df1, df2=df2, F_perc=F_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we do not have sufficient evidence so reject the hypothesis that all of these 16 coefficients are zero at the 95% confidence level. In the interest of a parsimonious model we might reasonably decide to use the reduced least-squares model. We can choose between these by comparing the mean squared error on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square error for full least-squares model: 0.1170\n",
      "Mean square error for reduced least-squares model: 0.1185\n"
     ]
    }
   ],
   "source": [
    "# MSE for the full least-squares model\n",
    "y_cv_pred_ls = least_squares.predict(X_cv)\n",
    "mse_cv_ls = mean_squared_error(y_cv, y_cv_pred_ls)\n",
    "print('Mean square error for full least-squares model: {:.4f}'.format(mse_cv_ls))\n",
    "\n",
    "# MSE for reduced model\n",
    "ixgrid = np.ix_(list(range(N_cv)), feat_red)\n",
    "X_cv_red = X_cv[ixgrid]\n",
    "y_cv_pred_ls_red = least_squares_red.predict(X_cv_red)\n",
    "mse_cv_ls_red = mean_squared_error(y_cv, y_cv_pred_ls_red)\n",
    "print('Mean square error for reduced least-squares model: {:.4f}'.format(mse_cv_ls_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in fact cross-validation suggests that the full least-squares model generalises better.\n",
    "\n",
    "Note that our method for choosing which coefficients to drop was flawed because the $Z$-score describes the effect of dropping that feature while keeping the others in the model. The $k$ features which have the least effect on the RSS when dropped *individually* are not necessarily the $k$ features that have the least effect when dropped *together*. For example, features 31 and 33 are near the bottom of our list of $Z$-scores. However, we know that these two are very highly correlated, so one would expect that dropping just one of them would have little effect on the model. Dropping them both however may have a large effect.\n",
    "\n",
    "Below we investigate subset selection in more detail with more reliable methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four methods for subset selection discussed in the text:\n",
    "- best-subset selection\n",
    "- forward-stepwise selection\n",
    "- backward-stepwise selection\n",
    "- forward-stagewise selection\n",
    "\n",
    "Unfortunately none of these appear to be implemented in Scikit-learn. \n",
    "\n",
    "The closest to best-subset selection in scikit-learn is `feature_selection.SelectKBest`. For each index $j$, this conducts an $F$-test for the null hypothesis that $\\beta_j=0$ under the assumption of a simple univariate model $Y=\\beta_0 + \\beta_j X_j$, and then chooses the $k$ indices with the highest $F$-scores. This is equivalent to choosing the indices with the highest coefficient of determination $R^2$. The issue with this is the same as described at the end of the previous section; it only considers the features individually. This can be problematic if some features are highly correlated as in our case.\n",
    "\n",
    "There is also a method `feature_selection.RFE` (RFE stands for Recursive Feature Elimination) that sounds similar to backward-stepwise selection, but it requires the estimator to contain information about the feature importance which `LinearRegression` doesn't.\n",
    "\n",
    "So we have to implement these methods ourselves. We will neglect best-subset and forward-stagewise because the first is only practical for up to 30-40 features and the second is more complex than we would like. Instead we will implement forward- and backward-stepwise selection using the theory in the textbook and compare their performance. In particular, for each method we want to:\n",
    "- compute the full list of indices as they are added/removed\n",
    "- compute the coefficients of the least squares fit at each step\n",
    "- estimate the prediction error and standard error at each step using cross-validation\n",
    "\n",
    "From this we will compare the two methods and use cross-validation to choose the complexity parameter - the number of features to include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let $k$ denote the subset size.\n",
    "\n",
    "Method outline: For k=0,..,57\n",
    "- For k=0,..,57:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of method: \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
