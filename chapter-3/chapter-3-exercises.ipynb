{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "Show that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Without loss of generality, assume that the smaller model has had the final feature $\\mathbf{x}_p$ removed. Let $\\hat{\\mathbf{y}}$ denote the least squares approximation for the larger model and $\\hat{\\mathbf{y}}^{\\prime}$ be that for the smaller. We need to show that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\text{RSS}_0 - \\text{RSS}_1}{\\text{RSS}_1 / (N-p-1)} = \\frac{\\hat{\\beta}_p^2}{\\hat{\\sigma}^2 v_p},\n",
    "\\end{equation}\n",
    "\n",
    "where $v_j$ is the $j$th diagonal element of $(\\mathbf{X}^T \\mathbf{X})^{-1}$. \n",
    "\n",
    "First note that by definition $\\hat{\\sigma}^2 = \\text{RSS}_0 / (N-p-1)$. Moreover, since $\\hat{\\mathbf{y}}$ is the projection of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$, their difference is orthogonal to any element of the columns space. In particular, it is orthogonal to $\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime}$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2 \n",
    "        + \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Rightarrow \\text{RSS}_0 - \\text{RSS}_1\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2 \n",
    "            - \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2\n",
    "         = \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "Now let $\\mathbf{z}_0,\\ldots ,\\mathbf{z}_p$ denote the orthogonal basis of the column space of $\\mathbf{X}$, obtained from $\\mathbf{x}_0,\\ldots,\\mathbf{x}_p$ using the Gram-Schmidt process (Algorithm 3.1). The least squares estimates  $\\hat{\\mathbf{y}}$ and $\\hat{\\mathbf{y}}^{\\prime}$ are the projections of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$ and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{span}(\\{\\, \\mathbf{x}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\} = \\text{span}(\\{\\, \\mathbf{z}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\}\n",
    "\\end{equation}\n",
    "\n",
    "respectively. Since the $\\mathbf{z}_j$ are orthogonal, this implies that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} = \\frac{\\langle \\mathbf{z}_p, \\mathbf{y}\\rangle}{\\langle \\mathbf{z}_p, \\mathbf{z}_p\\rangle}\\mathbf{z}_p = \\hat{\\beta}_p \\mathbf{z}_p.\n",
    "\\end{equation}\n",
    "\n",
    "Putting these elements together, it just remains to show that $v_p = \\lVert \\mathbf{z}_p\\rVert ^{-2}$. But, if $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ is the QR-decomposition of $\\mathbf{X}$ then $(\\mathbf{X}^T \\mathbf{X})^{-1} = \\mathbf{R}^{-1}(\\mathbf{R}^{-1})^T$. Since $\\mathbf{R}$ is upper-triangular, the $p$th diagonal element of $\\mathbf{R}^{-1}$ is $R_{pp}^{-1} = \\lVert \\mathbf{z}_p\\rVert ^{-1}$ and the claim follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
