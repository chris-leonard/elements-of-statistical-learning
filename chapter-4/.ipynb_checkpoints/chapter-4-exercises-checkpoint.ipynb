{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1\n",
    "\n",
    "Show how to solve the generalized eigenvalue problem $\\text{max}\\, a^T \\mathbf{B}a$ subject to $a^T \\mathbf{W}a = 1$ by transforming to a standard eigenvalue problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Take the eigendecomposition $\\mathbf{W}=\\mathbf{U}\\mathbf{D}\\mathbf{U}^T$ where $\\mathbf{U}$ is a $p\\times p$ orthogonal matrix. Let $\\mathbf{M}^* = \\mathbf{M}\\mathbf{U}\\mathbf{D}^{-\\frac{1}{2}}$ and $a^*=\\mathbf{D}^{\\frac{1}{2}}\\mathbf{U}^Ta$. The covariance matrix of $\\mathbf{M}^*$ can be obtained from that of $\\mathbf{M}$ by conjugating:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{B}^* = \\mathbf{U}^T\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{B}\\mathbf{D}^{\\frac{1}{2}}\\mathbf{U},\n",
    "\\end{equation}\n",
    "\n",
    "so the constrained optimisation problem is equivalent to $\\text{max}\\, a^{*T}\\mathbf{B}^*a^*$ subject to $a^{*T}a=1$.\n",
    "\n",
    "Consider the eigendecomposition $\\mathbf{B}^*=\\mathbf{V}^*\\mathbf{D}_B\\mathbf{V}^{*T}$ there the diagonal entries of $\\mathbf{D}_B$ are in decreasing order (they are all positive). Then $a^{*T}\\mathbf{B}^*a^* = \\lVert\\mathbf{D}_B^{\\frac{1}{2}}\\mathbf{V}^{*T}a^*\\rVert^2$. Write $a^*$ as a linear combination of the eigenvectors: $a^* = \\sum_{i=1}^p \\lambda_iv_i^*$. This transforms the optimisation problem into the following: $\\text{max}\\,\\sum d_i\\lambda_i^2$ subejct to $\\sum \\lambda_i^2=1$. This is a standard Lagrange multiplier problem with solution $a^* = v_1^*$.\n",
    "\n",
    "Transforming back to the original coordinates gives\n",
    "\n",
    "\\begin{equation}\n",
    "    a = \\mathbf{U}\\mathbf{D}^{-\\frac{1}{2}}a^* = \\mathbf{U}\\mathbf{D}^{-\\frac{1}{2}} v_1^* = v_1\n",
    "\\end{equation}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2\n",
    "\n",
    "Suppose we have features $x \\in \\mathbb{R}^p$, a two-class response, with class sizes $N_1$, $N_2$, and the targets coded as $−N/N_1$, $N/N_2$.\n",
    "\n",
    "**(a)** Show that the LDA rule classifies to class 2 if\n",
    "\n",
    "\\begin{equation}\n",
    "    x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        > \\frac{1}{2}(\\hat{\\mu}_2+\\hat{\\mu}_1)^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)-\\log(N_2/N_1),\n",
    "\\end{equation}\n",
    "\n",
    "and class 1 otherwise.\n",
    "\n",
    "**(b)** Consider minimization of the least squares criterion\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^N (y_i-\\beta_0-x_i^T\\beta)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Show that the solution $\\hat{\\beta}$ satisfies\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}_B\\right]\\beta = N(\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "(after simplification), where $\\hat{\\Sigma}_B=\\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)(\\hat{\\mu}_2-\\hat{\\mu}_1)^T$.\n",
    "\n",
    "**(c)** Hence show that $\\hat{\\Sigma}_B\\beta$ is in the direction $(\\hat{\\mu}_2-\\hat{\\mu}_1)$ and thus\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.\n",
    "\n",
    "**(d)** Show that this result holds for any (distinct) coding of the two classes.\n",
    "\n",
    "**(e)** Find the solution $\\hat{\\beta}_0$ (up to the same scalar multiple as in (c)), and hence the predicted value $\\hat{f}(x) = \\hat{\\beta}_0 + x^T\\hat{\\beta}$. Consider the following rule: classify to class 2 if $\\hat{f}(x) > 0$ and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** The discriminant functions for LDA are\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_k(x) = x\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\ln(\\pi_k).\n",
    "\\end{equation}\n",
    "\n",
    "Estimating $\\mu_k$ by $\\hat{\\mu}_k$, $\\Sigma$ by $\\hat{\\Sigma}$, and $\\pi_k$ by $N_k/N$, we classify to class 2 if and only if\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\delta}_2(x) \n",
    "        & > \\hat{\\delta}_1(x) \\\\\n",
    "    \\Longleftrightarrow x\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 - \\frac{1}{2}\\hat{\\mu}_2^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 + \\ln(N_2/N) \n",
    "        & > x\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 - \\frac{1}{2}\\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + \\ln(N_1/N) \\\\\n",
    "    \\Longleftrightarrow x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        & > \\frac{1}{2}\\left(\\hat{\\mu}_2\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 - \\hat{\\mu}_1\\hat{\\Sigma}^{-1}\\hat{\\mu}_1\\right) + \\ln(N_1/N_2) \\\\\n",
    "    \\Longleftrightarrow x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        & > \\frac{1}{2}(\\hat{\\mu}_2+\\hat{\\mu}_1)^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)-\\log(N_2/N_1)    \n",
    "\\end{align}\n",
    "\n",
    "and to class 1 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Let $\\tilde{\\mathbf{X}}$ denote the input data matrix augmented with the intercept column. We know that $\\hat{\\beta}$ satisfies $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta} = \\tilde{\\mathbf{X}}^T\\mathbf{y}$. \n",
    "\n",
    "We wish to relate $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta}$ and $\\mathbf{X}^T\\mathbf{X}\\beta$. The $(j, k)$ entry of $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$ is $\\sum_i x_{ij}x_{ik}^T$. Taking $k=0$, the $j$th entry of the 0th column is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_i x_{ij}x_{i0}^T = \\sum_{g_i=1}x_{ij} + \\sum_{g_i=2}x_{ij} = N_1\\hat{\\mu}_{1j} + N_2\\hat{\\mu}_{2j}.\n",
    "\\end{equation}\n",
    "\n",
    "So $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$ is a $(p+1)\\times(p+1)$-matrix with $(0, 0)$ entry $N$, $(1,0),\\ldots (p, 0)$ entries taken up by $N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2$, $(0,1),\\ldots (0, p)$ entries taken up by $N_1\\hat{\\mu}^T_1 + N_2\\hat{\\mu}^T_2$, and bottom right $p\\times p$ submatrix equal to $\\mathbf{X}^T\\mathbf{X}$. This implies that $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta}$ is equal to the block matrix\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        N\\beta_0 + (N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta \\\\\n",
    "        \\beta_0(N_1\\hat{\\mu}_2 + N_2\\hat{\\mu}_2) + \\mathbf{X}^T\\mathbf{X}\\beta\n",
    "    \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where the top expression is a scaler and the bottom is a $p\\times 1$ vector.\n",
    "\n",
    "We have already seen that this is equal to $\\tilde{\\mathbf{X}}^T\\mathbf{y}$. The $j$th entry of $\\tilde{\\mathbf{X}}^T\\mathbf{y}$ is\n",
    "\n",
    "\\begin{equation}    \n",
    "    \\mathbf{x}_j^T\\mathbf{y} = \\frac{N}{N_2}\\left(\\sum_{g_i=2}x_{ij}\\right) - \\frac{N}{N_1}\\left(\\sum_{g_i=1}x_{ij}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "so $\\tilde{\\mathbf{X}}^T\\mathbf{y}$ is $N(\\hat{\\mu}_2 - \\hat{\\mu}_2)$ augmented with a zero in the 0th place. Equating the expressions in the 0th place gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta.\n",
    "\\end{equation}\n",
    "\n",
    "Equating the remaining terms and substituting this in gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[ \\mathbf{X}^T\\mathbf{X} - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)^T(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\right]\\beta = N(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "It remains to show that the expression in square brackets equals $(N-2)\\hat{\\Sigma} + N\\hat{\\Sigma}_B$.\n",
    "\n",
    "Indeed,\n",
    "\n",
    "\\begin{align}\n",
    "    (N-2)\\hat{\\Sigma}\n",
    "        & = \\sum_{k=1}^2\\sum_{g_i=k} (x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T \\\\\n",
    "        & = \\sum_{k=1}^2\\sum_{g_i=k} (x_ix_i^T-\\hat{\\mu}_kx_i^T -x_i\\hat{\\mu}_k^T + \\hat{\\mu}_k\\hat{\\mu}_k^T \\\\\n",
    "        & = \\sum_{k=1}^2 \\left[\\left(\\sum_{g_i=k} x_i x_i^T\\right)-N_k\\hat{\\mu}_k\\hat{\\mu}^T_k - N_k\\hat{\\mu}_k\\hat{\\mu}^T_k + N_k\\hat{\\mu}_k\\hat{\\mu}^T_k\\right]  \\\\\n",
    "        & = \\mathbf{X}^T\\mathbf{X} - \\sum_{k=1}^2 N_k\\hat{\\mu}_k\\hat{\\mu}^T_k.\n",
    "\\end{align}\n",
    "\n",
    "So,\n",
    "\n",
    "\\begin{align}\n",
    "    (N-2)\\hat{\\Sigma} + N\\hat{\\Sigma}_B\n",
    "    & = \\mathbf{X}^T\\mathbf{X}\n",
    "        - N_1\\hat{\\mu}_1\\hat{\\mu}_1^T \n",
    "        - N_2\\hat{\\mu}_2\\hat{\\mu}_2^T\n",
    "        +\\frac{N_1N_2}{N}(\\hat{\\mu}_2 - \\hat{\\mu}_1)(\\hat{\\mu}_2 - \\hat{\\mu}_1)^T \\\\\n",
    "    & = \\mathbf{X}^T\\mathbf{X}\n",
    "        + \\left(\\frac{N_1N_2}{N} - N_1\\right) \\hat{\\mu}_1\\hat{\\mu}_1^T \n",
    "        + \\left(\\frac{N_1N_2}{N} - N_2\\right) \\hat{\\mu}_2\\hat{\\mu}_2^T \n",
    "        - \\frac{N_1N_2}{N} (\\hat{\\mu}_1\\hat{\\mu}_2^T + \\hat{\\mu}_2\\hat{\\mu}_1^T) \\\\\n",
    "    & = \\mathbf{X}^T\\mathbf{X}\n",
    "        - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)^T,\n",
    "\\end{align}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** By definition, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}_B\\beta \n",
    "        = \\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta \n",
    "        = \\left[\\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta\\right] (\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}\\hat{\\beta} = \\frac{N}{N-2}\\left[ 1 - \\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta\\right] (\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "and thus $\\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Changing the coding makes little difference to our solution to part (b). Suppose that the targets are coded as $c_1$ and $c_2$. Then \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}^T\\mathbf{y} = \n",
    "        \\begin{pmatrix}\n",
    "            c_1N_1 + c_2N_2 \\\\\n",
    "            c_1N_1\\hat{\\mu}_1 + c_2N_2\\hat{\\mu}_2\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Taking this through our solution we end up with\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma} + N \\hat{\\Sigma}_B\\right]\\beta + \\frac{c_1N_1+c_2N_2}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\n",
    "        = c_1N_1\\hat{\\mu}_1 + c_2N_2\\hat{\\mu}_2.\n",
    "\\end{equation}\n",
    "\n",
    "Rearranging this gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma} + N \\hat{\\Sigma}_B\\right]\\beta = (c_2-c_1)\\frac{N_1N_2}{N}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "This implies that $\\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1)$ as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** By part (b), $\\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0 \\propto -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "with the same constant of proportionality as $\\beta\\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)$. Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x) \\propto \\left(x - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\right)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "So we classify to class 2 iff\n",
    "\n",
    "\\begin{equation}\n",
    "    x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1) > \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "Since $N=N_1 + N_2$, this is the same as the LDA rule if $N_1=N_2$. If they are not equal, the rules will be different in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3\n",
    "\n",
    "Suppose we transform the original predictors $\\mathbf{X}$ to $\\hat{\\mathbf{Y}}$ via linear regression. In detail, let $\\hat{\\mathbf{Y}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}\\hat{\\mathbf{B}}$, where $\\mathbf{Y}$ is the indicator response matrix. Similarly for any input $x\\in\\mathbb{R}^p$, we get a transformed vector $\\hat{y} = \\hat{\\mathbf{B}}^T x\\in\\mathbb{R}^K$. Show that LDA using $\\hat{\\mathbf{Y}}$ is identical to LDA in the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let $\\hat{\\pi}_k$, $\\hat{\\mu}_k$, and $\\hat{\\Sigma}$ denote the parameter estimates for the original predictors. Clearly $\\hat{\\pi}$ is unchanged under the transformation. The new estimates of class means are\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N_k}\\sum_{g_i = k}\\hat{y}_i = \\frac{1}{N_k}\\sum_{g_i = k}\\hat{\\mathbf{B}}^T x_i = \\hat{\\mathbf{B}}^T\\hat{\\mu}_k.\n",
    "\\end{equation}\n",
    "\n",
    "Since\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}=\\frac{1}{N-K}\\left(\\mathbf{X}^T\\mathbf{X} - \\sum_{k=1}^K N_k\\hat{\\mu}_k\\hat{\\mu}_k^T\\right),\n",
    "\\end{equation}\n",
    "\n",
    "the transformed covariance estimate is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N-K}\\left(\\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} - \\sum_{k=1}^K N_k \\hat{\\mathbf{B}}^T\\hat{\\mu}_k\\hat{\\mu}_k^T\\hat{\\mathbf{B}}\\right) = \\hat{\\mathbf{B}}^T\\hat{\\Sigma}\\hat{\\mathbf{B}}.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore the new linear discriminant functions are\n",
    "\n",
    "\\begin{align}\n",
    "    \\delta_k(\\hat{y}) \n",
    "        & = (x^T\\hat{\\mathbf{B}}) \\left(\\hat{\\mathbf{B}}^{-1} \\hat{\\Sigma}^{-1} (\\hat{\\mathbf{B}}^T)^{-1}\\right) (\\hat{\\mathbf{B}}^T\\hat{\\mu}_k) - \\frac{1}{2} (\\hat{\\mu}_k^T\\hat{\\mathbf{B}}) \\left(\\hat{\\mathbf{B}}^{-1} \\hat{\\Sigma}^{-1} (\\hat{\\mathbf{B}}^T)^{-1}\\right) (\\hat{\\mathbf{B}}^T\\hat{\\mu}_k) + \\ln(\\hat{\\pi}_k) \\\\\n",
    "        & = x^T\\hat{\\Sigma}\\hat{\\mu}_k - \\frac{1}{2} \\hat{\\mu}_k^T\\hat{\\Sigma}\\hat{\\mu}_k + \\ln(\\hat{\\pi}_k) \\\\\n",
    "        & = \\delta_k(x),\n",
    "\\end{align}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4\n",
    "\n",
    "Consider the multilogit model with $K$ classes (4.17). Let $\\beta$ be the $(p + 1)(K − 1)$-vector consisting of all the coefficients. Define a suitably enlarged version of the input vector $x$ to accommodate this vectorized coefficient matrix. Derive the Newton-Raphson algorithm for maximizing the multinomial log-likelihood, and describe how you would implement this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Given a length $p$ input vector $x$ (we assume the intercept 1 is already included), let $\\tilde{x}$ be the $(p+1)(K-1) \\times (K-1)$ block matrix with $K-1$ $x$'s along the diagonal and 0s elsewhere. Then $\\tilde{x}^T\\beta$ is the length $K-1$ vector with $k$th entry $\\beta_k^T x$ (again we include the intercept term in $\\beta_k$). From this the vector of probabilities $p_k(x; \\theta)$ can easily be calculated.\n",
    "\n",
    "We now derive the score equations and Newton-Raphson algorithm. Let $y_i = (y_{i0}, \\ldots, y_{iK})^T$ denote the $i$th row of the indicator response matrix $\\mathbf{Y}$. The multinomial log-likelihood is\n",
    "\n",
    "\\begin{align}\n",
    "    l(\\beta)\n",
    "        & = \\sum_{i=1}^N \\sum_{k=1}^{K} y_{ik} \\ln (p_k(x_i; \\theta)) \\\\\n",
    "        & = \\sum_{i=1}^N \\left[\\sum_{k=1}^{K-1}\\left[ y_{ik} \\left( \\beta_k^T x_i - \\ln(1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i))\\right)\\right] - (1-\\sum_{l=1}^{K-1} y_{il})\\ln(1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i))\\right] \\\\\n",
    "        & = \\sum_{i=1}^N \\left[\\sum_{k=1}^{K-1} \\left( y_{ik} \\beta_k^T x_i \\right) - \\ln(1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i))\\right].\n",
    "\\end{align}\n",
    "\n",
    "Differentiating this with respect to one of the $\\beta_k$ gives\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial l(\\beta)}{\\partial \\beta_k} \n",
    "        & = \\sum_{i=1}^N \\left[ y_{ik}x_i - \\frac{\\exp(\\beta_k^Tx)}{1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i)} x_i \\right] \\\\\n",
    "        & = \\sum_{i=1}^N (y_{ik} - p_k(x_i;\\theta))x_i\n",
    "\\end{align}\n",
    "\n",
    "and differentiating again yields\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k \\partial \\beta_l^T}\n",
    "        = \\begin{cases}\n",
    "                -\\sum_{i=1}^N p_k(x_i; \\theta) p_l(x_i; \\theta) x_i x_i^T & \\text{if } k\\neq l \\\\\n",
    "                -\\sum_{i=1}^N p_k(x_i; \\theta) (1 - p_k(x_i; \\theta)) x_i x_i^T & \\text{if } k=l\n",
    "          \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This gives us enough to implement Newton-Raphson: concatenate the $\\frac{\\partial l(\\beta)}{\\partial \\beta_k}$ and $\\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k \\partial \\beta_l^T}$ into a single vector and block matrix $\\frac{\\partial l(\\beta)}{\\partial \\beta}$ and $\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}$, respectively. Then set\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta^{\\text{new}} = \\beta^{\\text{old}} - \\left(\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\\right)^{-1} \\frac{\\partial l(\\beta)}{\\partial \\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "We will now describe how to use the enlarged vector $\\tilde{x}$ to put everything in terms of matrix products. The idea is to vectorise natural matrices like $\\mathbf{Y}$ in a similar fashion to $\\beta$.\n",
    "\n",
    "Let $\\tilde{\\mathbf{X}}$ denote the $(k-1)N\\times (p+1)(K-1)$-matrix obtained by stacking $\\tilde{x}_1^T, \\ldots, \\tilde{x}_N^T$ on top of each other, or equivalently obtained from $\\mathbf{X}$ by replacing each $x_i^T$ with its tilde counterpart. Let $\\tilde{\\mathbf{y}}$ be the length $(k-1)N$ vector obtained by stacking the observations $y_i$ on top of each other in order. Similarly, let $\\mathbf{p}_i$ denote the length $K-1$ vector with entries $p_1(x_i; \\theta), \\ldots p_{K-1}(x_i; \\theta)$ and let $\\tilde{\\mathbf{p}}$ be the length $(k-1)N$ vector obtained by stacking the $\\mathbf{p}_i$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^N \\tilde{x}_i(y_i - \\mathbf{p}_i) = \\tilde{\\mathbf{X}}^T(\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{p}})\n",
    "\\end{equation}\n",
    "\n",
    "For $i\\in\\{1, \\ldots, N\\}$, let $\\mathbf{W}_i$ denote the $(K-1)\\times(K-1)$ matrix with $(k, l)$ entry $p_k(x_i;\\theta)p_l(x_i;\\theta)$ if $k\\neq l$ and $p_k(x_i;\\theta)(1-p_k(x_i;\\theta)$ if $k=l$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\n",
    "         = \\sum_{i=1}^N \\tilde{x}_i \\mathbf{W}_i \\tilde{x}_i^T = \\tilde{\\mathbf{X}}^T\\mathbf{W}\\tilde{\\mathbf{X}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{W}$ is the $(K-1)N\\times (K-1)N$ block matrix with $\\mathbf{W}_1,\\ldots, \\mathbf{W}_N$ along the diagonal.\n",
    "\n",
    "With this in hand, the Newton-Raphson algorithm can be expressed as an iterated reweighted least squares procedure as in the $K=2$ case:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta^{\\text{new}} = (\\tilde{\\mathbf{X}}^T\\mathbf{W}\\tilde{\\mathbf{X}})^{-1}\\tilde{\\mathbf{X}}^T\\mathbf{W}\\tilde{\\mathbf{z}}, \\quad\\text{ where }\\quad \\tilde{\\mathbf{z}} = \\tilde{\\mathbf{X}}\\beta^{\\text{old}} + \\mathbf{W}^{-1}(\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{p}})\n",
    "\\end{equation}\n",
    "\n",
    "This can be implemented directly, updating $\\mathbf{W}$ and $\\tilde{\\mathbf{p}}$ at each step, or as the solution to the weighted least squares problem\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{\\beta}{\\text{min}} (\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{X}}\\beta)^T \\mathbf{W} (\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{X}}\\beta).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5\n",
    "\n",
    "Consider a two-class logistic regression problem with $x\\in\\mathbb{R}$. Characterize the maximum-likelihood estimates of the slope and intercept parameter if the sample $x_i$ for the two classes are separated by a point $x_0\\in\\mathbb{R}$. Generalize this result to **(a)** $x\\in\\mathbb{R}^p$ (see Figure 4.16), and **(b)** more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "In all these situations the MLEs are undefined. In the first case the log-likelihood is\n",
    "\n",
    "\\begin{equation}\n",
    "    l(\\beta) = \\sum_{i=1}^N \\left[ y_i\\ln(p(x_i;\\theta)) + (1-y_i)\\ln(1-p(x_i;\\theta))\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Since $p(x,\\theta)\\in[0,1]$, this is negative. If the two classes are separated then we can approach 0 with suitable choices of $\\theta$, but $\\theta$ will diverge to infinity.\n",
    "\n",
    "To be more precise, suppose that $y_i=1$ if $x_i<x_0$ and $y_i=0$ if $x_i>x_0$. Take $\\theta = (\\beta_0,\\beta)=(-rx_0, r)$ for $r>0$. Then $\\beta_0 + \\beta x>0$ if $x>x_0$ and $\\beta_0 + \\beta x>0$ if $x<x_0$. So as $r\\rightarrow \\infty$, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0 + \\beta x_i \\rightarrow\n",
    "        \\begin{cases}\n",
    "            -\\infty & \\text{if } y_i=1 \\\\\n",
    "            \\infty & \\text{if } y_i=0\n",
    "        \\end{cases}\n",
    "    \\quad\\Longrightarrow\\quad\n",
    "    p(x_i;\\theta) \\rightarrow\n",
    "        \\begin{cases}\n",
    "            0 & \\text{if } y_i=1 \\\\\n",
    "            1 & \\text{if } y_i=0\n",
    "        \\end{cases}.\n",
    "\\end{equation}\n",
    "\n",
    "and $l(\\beta)\\rightarrow 0$. Clearly this path isn't unique, for example we could take a different separating point $x_0$.\n",
    "\n",
    "**(a)** If $x\\in\\mathbb{R}^p$ for $p>1$ the situation is very similar. Suppose that the two classes are separated by a hyperlane $\\tilde{\\beta}_0+ \\tilde{\\beta}^Tx=0$ with $y_i=1$ if $\\tilde{\\beta}_0+ \\tilde{\\beta}^Tx<0$ and $y_i=0$ if $\\tilde{\\beta}_0+ \\tilde{\\beta}^Tx>0$. Then $\\theta = (r\\tilde{\\beta}_0, r\\tilde{\\beta})$ diverges to infinity as $r\\to\\infty$ but $l(\\beta)$ converges to its supremum zero$.\n",
    "\n",
    "**(b)** There are a few ways to conceive of generalising to $K>2$ classes, but the point is that the log-likehihood has supremum 0 which can't be attained with finite values of $\\theta$. For a simple formulation, suppose that for $k=1,\\ldots, K-1$, the $k$th class can be separated from *all other* classes by a hyperplane $\\tilde{\\beta}_{0k} + \\tilde{\\beta}_{k}^Tx=0$. More precisely, suppose that $g_i = k$ if $\\tilde{\\beta}_{0k} + \\tilde{\\beta}_{k}^Tx>0$ and $g_i\\neq k$ if $\\tilde{\\beta}_{0k} + \\tilde{\\beta}_{k}^Tx<0$. Let $\\theta$ have $k$th component $(r\\tilde{\\beta}_{0k},r\\tilde{\\beta}_k)$. Then the log-likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "    l(\\beta) = \\sum_{i=1}^N\\sum_{k=1}^K y_{ik} \\ln (p_k(x_i; \\theta))\n",
    "\\end{equation}\n",
    "\n",
    "converges to 0 as $r\\to\\infty$ but $\\theta$ diverges to infinity.\n",
    "\n",
    "Note that this means that logistic regression fails for arguably the *simplest* type of classification problem. This motivates separating hyperplanes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6\n",
    "\n",
    "Suppose we have $N$ points $x_i$ in $\\mathbb{R}^p$ in general position, with class labels $y_i\\in\\{-1,1\\}$. Prove that the perceptron learning algorithm converges to a separating hyperplane in a finite number of steps:\n",
    "\n",
    "**(a)** Denote a hyperplane by $f(x)=\\beta_1^T+\\beta_0=0$, or in more compact notation $β^Tx^∗ = 0$, where $x^∗ = (x,1)$ and $\\beta=(\\beta_1,\\beta_0)$. Let $z_i=x_i^* / \\lVert x_i^*\\rVert$. Show that separability implies the existence of a $\\beta_{\\text{sep}}$ such that $y_i\\beta_{\\text{sep}}^T z_i ≥1\\,\\, \\forall i$\n",
    "\n",
    "**(b)** Give a current $\\beta_{\\text{old}}$, the perceptron algorithm identifies a point $z_i$ that is misclassified, and produces the update $\\beta_{\\text{new}}\\leftarrow \\beta_{\\text{old}}+y_iz_i$. Show that $\\lVert \\beta_{\\text{new}} - \\beta_{\\text{sep}}\\rVert^2 \\leq \\lVert \\beta_{\\text{old}} - \\beta_{\\text{sep}}\\rVert^2 -1$, and hence the algorithm converges to a separating hyperplane in no more than $\\lVert \\beta_{\\text{start}} - \\beta_{\\text{sep}}\\rVert^2$ steps (Ripley, 1996)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** If the data are separable then there exists a $\\beta$ such that $\\beta^Tx_i>0$ if $y_i=1$ and $\\beta^Tx_i<0$ if $y_i=-1$. Now it is just a case of scaling $\\beta$.\n",
    "\n",
    "**(b)** Note that this is just stochastic gradient descent with a changing learning rate $\\rho$. We have\n",
    "\n",
    "\\begin{align}\n",
    "    \\lVert \\beta_{\\text{new}} - \\beta_{\\text{sep}}\\rVert ^ 2\n",
    "        & = \\lVert \\beta_{\\text{old}} + y_iz_i - \\beta_{\\text{sep}} \\rVert^2 \\\\\n",
    "        & = \\lVert \\beta_{\\text{old}} - \\beta_{\\text{sep}}\\rVert ^ 2 + 2\\langle y_iz_i, \\beta_{\\text{old}} - \\beta_{\\text{sep}}\\rangle + \\lVert y_iz_i \\rVert^2 \\\\\n",
    "        & = \\lVert \\beta_{\\text{old}} - \\beta_{\\text{sep}}\\rVert ^ 2 + 2\\langle y_iz_i, \\beta_{\\text{old}}\\rangle - 2\\langle y_iz_i, \\beta_{\\text{sep}}\\rangle + 1.\n",
    "\\end{align}\n",
    "\n",
    "By the definition of $\\beta_{\\text{sep}}$, $\\langle y_iz_i, \\beta_{\\text{sep}}\\rangle\\geq 1$ and since the point $z_i$ is misclassified $\\langle y_iz_i, \\beta_{\\text{old}}\\rangle <0$. This implies that \n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\beta_{\\text{new}} - \\beta_{\\text{sep}}\\rVert ^ 2 \\leq \\lVert \\beta_{\\text{old}} - \\beta_{\\text{sep}}\\rVert ^ 2 - 1\n",
    "\\end{equation}\n",
    "\n",
    "and so the algorithm will converge in no more than $\\lVert \\beta_{\\text{start}} - \\beta_{\\text{sep}}\\rVert ^ 2$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8\n",
    "\n",
    "Consider the multivariate Gaussian model $X | G = k \\sim N(\\mu_k,\\Sigma)$, with the additional restriction that $\\text{rank}\\{\\mu_k\\}_1^K = L < \\text{max}(K − 1,p)$. Derive the constrained MLEs for the $\\mu_k$ and $\\Sigma$. Show that the Bayes classification rule is equivalent to classifying in the reduced subspace computed by LDA (Hastie and Tibshirani, 1996b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
