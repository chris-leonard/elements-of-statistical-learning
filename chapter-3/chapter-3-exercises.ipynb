{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "Show that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Without loss of generality, assume that the smaller model has had the final feature $\\mathbf{x}_p$ removed. Let $\\hat{\\mathbf{y}}$ denote the least squares approximation for the larger model and $\\hat{\\mathbf{y}}^{\\prime}$ be that for the smaller. We need to show that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\text{RSS}_0 - \\text{RSS}_1}{\\text{RSS}_1 / (N-p-1)} = \\frac{\\hat{\\beta}_p^2}{\\hat{\\sigma}^2 v_p},\n",
    "\\end{equation}\n",
    "\n",
    "where $v_j$ is the $j$th diagonal element of $(\\mathbf{X}^T \\mathbf{X})^{-1}$. \n",
    "\n",
    "First note that by definition $\\hat{\\sigma}^2 = \\text{RSS}_0 / (N-p-1)$. Moreover, since $\\hat{\\mathbf{y}}$ is the projection of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$, their difference is orthogonal to any element of the columns space. In particular, it is orthogonal to $\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime}$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2 \n",
    "        + \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Rightarrow \\text{RSS}_0 - \\text{RSS}_1\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2 \n",
    "            - \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2\n",
    "         = \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "Now let $\\mathbf{z}_0,\\ldots ,\\mathbf{z}_p$ denote the orthogonal basis of the column space of $\\mathbf{X}$, obtained from $\\mathbf{x}_0,\\ldots,\\mathbf{x}_p$ using the Gram-Schmidt process (Algorithm 3.1). The least squares estimates  $\\hat{\\mathbf{y}}$ and $\\hat{\\mathbf{y}}^{\\prime}$ are the projections of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$ and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{span}(\\{\\, \\mathbf{x}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\} = \\text{span}(\\{\\, \\mathbf{z}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\}\n",
    "\\end{equation}\n",
    "\n",
    "respectively. Since the $\\mathbf{z}_j$ are orthogonal, this implies that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} = \\frac{\\langle \\mathbf{z}_p, \\mathbf{y}\\rangle}{\\langle \\mathbf{z}_p, \\mathbf{z}_p\\rangle}\\mathbf{z}_p = \\hat{\\beta}_p \\mathbf{z}_p.\n",
    "\\end{equation}\n",
    "\n",
    "Putting these elements together, it just remains to show that $v_p = \\lVert \\mathbf{z}_p\\rVert ^{-2}$. But, if $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ is the QR-decomposition of $\\mathbf{X}$ then $(\\mathbf{X}^T \\mathbf{X})^{-1} = \\mathbf{R}^{-1}(\\mathbf{R}^{-1})^T$. Since $\\mathbf{R}$ is upper-triangular, the $p$th diagonal element of $\\mathbf{R}^{-1}$ is $R_{pp}^{-1} = \\lVert \\mathbf{z}_p\\rVert ^{-1}$ and the claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "\n",
    "Given data on two variables $X$ and $Y$, consider fitting a cubic polynomial regression model $f(X) = \\sum_{j=0}^3\\beta_jX^j$. In addition to plotting the fitted curve, you would like a 95% confidence band about the curve. Consider the following two approaches:\n",
    "\n",
    "1. At each point $x_0$, form a 95% confidence interval for the linear function $a^T\\beta = \\sum_{j=0}^3 \\beta_jx_0^j$.\n",
    "2. Form a 95% confidence set for $\\beta$ as in (3.15), which in turn generates confidence intervals for $f(x_0)$.\n",
    "\n",
    "How do these approaches differ? Which band is likely to be wider? Conduct a small simulation experiment to compare the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### Construction of Confidence Intervals\n",
    "\n",
    "Let $p=3$ and $\\alpha=0.05$.\n",
    "\n",
    "**1.** We have $\\beta\\sim\\mathcal{N}(\\hat{\\beta}, \\sigma^2\\mathbf{I})$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    x_0^T(\\hat{\\beta}-\\beta)\\sim\\mathcal{N}(0,x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\sigma^2).\n",
    "\\end{equation}\n",
    "\n",
    "Let $v = x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\in\\mathbf{R}$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{x_0^T(\\hat{\\beta} - \\beta)}{\\hat{\\sigma}\\sqrt{v}}\\sim t_{N-p-1},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{\\sigma}$ is the unbiased estimate for $\\sigma$ on p.47. Therefore, a 100(1-\\alpha)% confidence interval for $f(x_0) = x_0^T\\beta$ has endpoints\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x_0) \\pm \\hat{\\sigma}\\sqrt{x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}x_0}~t_{N-p-1,\\alpha/2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{f}(x_0) = x_0^T\\hat{\\beta}$ and $t_{N-p-1,\\alpha/2}$ is the $\\alpha/2$th percentile of a $T$ distribution with $N-p-1$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** By the argument on p.49, an approximate 95% confidence set for $f(x_0)$ is the set of $x_0^T\\beta$ such that $\\beta$ lies in \n",
    "\n",
    "\\begin{equation}\n",
    "    C = \\big\\{\\, \\beta \\,\\big| \\, \\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 \\leq \\hat{\\sigma}^2\\chi^2 \\,\\big\\},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\chi^2 = \\chi_{p+1,\\alpha}^2$ is the $\\alpha$th percentile of a chi-squared distribution with $p+1$ degrees of freedom.\n",
    "\n",
    "First note that $C$ is an ellipsoid in $\\mathbb{R}^{p+1}$. This implies that the restriction of the linear function $x_0^T\\beta$ to $C$ achieves its maximum and minimum on the boundary $\\partial C$ of $C$ and takes every value in between. In particular, $\\{ x_0^T\\beta\\mid\\beta\\in C\\}$ is an interval and the endpoints of this interval are the maximum and minimum of $x_0^T\\beta$ subject to the constraint $\\beta\\in\\partial C$, or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 = \\hat{\\sigma}^2\\chi^2.\n",
    "\\end{equation}\n",
    "\n",
    "We solve this problem using Lagrange multipliers. Let\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\beta,\\lambda) = x_0^T\\beta - \\lambda\\left(\\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 - \\hat{\\sigma}^2\\chi^2\\right).\n",
    "\\end{equation}\n",
    "\n",
    "This has gradient $\\nabla \\mathcal{L} = (\\frac{\\partial \\mathcal{L}}{\\partial \\beta},\\frac{\\partial\\mathcal{L}}{\\partial \\lambda})$ with\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = x_0 - 2\\lambda\\mathbf{X}^T(\\mathbf{X}\\beta - \\mathbf{X}\\hat{\\beta}),\n",
    "        \\qquad \\frac{\\partial\\mathcal{L}}{\\partial \\lambda} = \\lVert \\mathbf{X}(\\beta - \\hat{\\beta})\\rVert^2 - \\hat{\\sigma}^2\\chi^2.\n",
    "\\end{equation}\n",
    "\n",
    "Any solution to our optimisation problem will have $\\nabla\\mathcal{L}=0$. Setting the first partial derivative equation to zero gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}^T\\mathbf{X}(\\beta-\\hat{\\beta}) = \\frac{1}{2\\lambda}x_0 \n",
    "        \\quad\\Rightarrow\\quad \\beta = \\hat{\\beta} + \\frac{1}{2\\lambda} \\mathbf{X}^T\\mathbf{X}x_0.\n",
    "\\end{equation}\n",
    "\n",
    "Setting $\\frac{\\partial\\mathcal{L}}{\\partial \\lambda}=0$ and substituting this in give\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Big\\lVert \\frac{1}{2\\lambda}\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\Big\\rVert^2 = \\hat{\\sigma}^2\\chi^2\n",
    "        \\quad\\Rightarrow\\quad \\frac{1}{2\\lambda} = \\pm \\frac{\\hat{\\sigma}\\chi}{\\lVert \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\rVert}.\n",
    "\\end{equation}\n",
    "\n",
    "So, since we know the optimisation problem has a maximuma and a minimum, they must occur at\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta = \\hat{\\beta} \\pm \\hat{\\sigma}\\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}x_0}{\\lVert \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\rVert}\\chi.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the confidence interval for $f(x_0)$ has endpoints\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x_0) \\pm \\hat{\\sigma}\\frac{x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}x_0}{\\lVert \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}x_0\\rVert}\\chi_{p+1,\\alpha}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "The first method is exact (given the assumptions), whereas the second uses the approximation of $t_{N-p-1}$ by the standard normal distribution $p+1$ times. Since the $T$ distribution is less peaked than the standard normal, $t_{k, \\alpha} > z_{\\alpha}$ for $\\alpha<0.5$ and thus we should expect the second method to *underestimate* the length of the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3\n",
    "\n",
    "Gauss–Markov theorem:\n",
    "\n",
    "**(a)** Prove the Gauss–Markov theorem: the least squares estimate of a parameter $a^T \\beta$ has variance no bigger than that of any other linear unbiased estimate of $a^T \\beta$ (Section 3.2.2).\n",
    "\n",
    "**(b)** The matrix inequality $\\mathbf{B} \\preccurlyeq \\mathbf{A}$ holds if $\\mathbf{A} − \\mathbf{B}$ is positive semidefinite. Show that if $\\hat{\\mathbf{V}}$ is the variance-covariance matrix of the least squares estimate of $\\beta$ and $\\tilde{\\mathbf{V}}$ is the variance-covariance matrix of any other linear unbiased estimate, then $\\hat{\\mathbf{V}} \\preccurlyeq \\tilde{\\mathbf{V}}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5\n",
    "\n",
    "Consider the ridge regression problem (3.41). Show that this problem is equivalent to the problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^c = \\underset{\\beta^c}{\\text{argmin}} \\Bigg\\{ \\sum_{i=1}^N \\big[ y_i - \\beta_0^c  - \\sum_{j=1}^p (x_{ij}-\\bar{x}_j)\\beta_j^c\\big]^2 + \\lambda \\sum_{j=1}^p (\\beta_j^c)^2\\Bigg\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Give the correspondence between $\\beta^c$ and the original $\\beta$ in (3.41). Characterize the solution to this modified criterion. Show that a similar result holds for the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Since\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0^c + \\sum_{j=1}^p(x_{ij} - \\bar{x}_j)\\beta_j^c \n",
    "        = \\left( \\beta_0^c - \\sum_{j=1}^p \\bar{x}_j\\beta_j^c\\right) + \\sum_{j=1}^p x_{ij}\\beta_j^c,\n",
    "\\end{equation}\n",
    "\n",
    "the two problems are equivalent with \n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0^c = \\beta_0 + \\sum_{j=1}^p \\bar{x}_j\\beta_j = \\frac{1}{N}\\sum_{i=1}^N \\hat{y}_i\n",
    "\\end{equation}\n",
    "\n",
    "and $\\beta_j^c = \\beta_j$ for $j\\neq 0$.\n",
    "\n",
    "At the minimum, the derivative with respect to $\\beta_0^c$ of the expression in braces is zero, so\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^N \\left[ y_i - \\beta_0^c  - \\sum_{j=1}^p (x_{ij}-\\bar{x}_j)\\beta_j^c \\right] = 0 \\quad\n",
    "        \\Rightarrow \\quad \\left( \\sum_{i=1}^N y_i \\right) - N\\beta_0^c - 0 = 0\n",
    "\\end{align}\n",
    "\n",
    "and thus $\\beta_0^c = \\bar{y}$.\n",
    "\n",
    "LASSO???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Show that the ridge regression estimate is the mean (and mode) of the posterior distribution, under a Gaussian prior $\\beta\\sim \\mathcal{N}(0, \\tau^2 \\mathbf{I})$, and Gaussian sampling model $y\\sim\\mathcal{N}(\\mathbf{X}\\beta, \\sigma^2\\mathbf{I})$. Find the relationship between the regularization parameter $\\lambda$ in the ridge formula, and the variances $\\tau^2$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The posterior distribution for $\\beta$ has density function\n",
    "\n",
    "\\begin{align}\n",
    "    f_{\\beta\\mid Y_1, \\ldots, Y_N}(\\beta)\n",
    "        & \\propto f_{Y_1, \\ldots, Y_N\\mid\\beta}(y_1,\\ldots, y_N)f_{\\beta}(\\beta) \\\\\n",
    "        & = \\left(\\prod_{i=1}^N f_{Y\\mid\\beta}(y_i)\\right) f_{\\beta}(\\beta) \\\\\n",
    "        & \\propto \\text{exp}\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - x_i^T\\beta)^2 - \\frac{1}{\\tau^2}\\lVert \\beta\\rVert^2\\right)\\\\\n",
    "        & = \\text{exp}\\left( -\\frac{1}{2\\sigma^2}\\left( \\lVert\\mathbf{y} - \\mathbf{X}\\beta\\rVert^2 + \\frac{\\sigma^2}{\\tau^2}\\lVert\\beta\\rVert^2\\right)\\right).\n",
    "\\end{align}\n",
    "\n",
    "So the mode of this distribution is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{\\beta}{\\text{argmax}} \\left(f_{\\beta\\mid Y_1, \\ldots, Y_N}(\\beta)\\right)\n",
    "        = \\underset{\\beta}{\\text{argmin}}\\left(\\lVert\\mathbf{y} - \\mathbf{X}\\beta\\rVert^2 + \\frac{\\sigma^2}{\\tau^2}\\lVert\\beta\\rVert^2\\right).\n",
    "\\end{equation}\n",
    "\n",
    "This is the ridge regression estimate with $\\lambda = \\sigma^2/\\tau^2$. Moreover, since the posterior is Gaussian (p.64) the mean equals the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
