{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1\n",
    "\n",
    "Show how to solve the generalized eigenvalue problem $\\text{max}\\, a^T \\mathbf{B}a$ subject to $a^T \\mathbf{W}a = 1$ by transforming to a standard eigenvalue problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Take the eigendecomposition $\\mathbf{W}=\\mathbf{U}\\mathbf{D}\\mathbf{U}^T$ where $\\mathbf{U}$ is a $p\\times p$ orthogonal matrix. Let $\\mathbf{M}^* = \\mathbf{M}\\mathbf{U}\\mathbf{D}^{-\\frac{1}{2}}$ and $a^*=\\mathbf{D}^{\\frac{1}{2}}\\mathbf{U}^Ta$. The covariance matrix of $\\mathbf{M}^*$ can be obtained from that of $\\mathbf{M}$ by conjugating:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{B}^* = \\mathbf{U}^T\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{B}\\mathbf{D}^{\\frac{1}{2}}\\mathbf{U},\n",
    "\\end{equation}\n",
    "\n",
    "so the constrained optimisation problem is equivalent to $\\text{max}\\, a^{*T}\\mathbf{B}^*a^*$ subject to $a^{*T}a=1$.\n",
    "\n",
    "Consider the eigendecomposition $\\mathbf{B}^*=\\mathbf{V}^*\\mathbf{D}_B\\mathbf{V}^{*T}$ there the diagonal entries of $\\mathbf{D}_B$ are in decreasing order (they are all positive). Then $a^{*T}\\mathbf{B}^*a^* = \\lVert\\mathbf{D}_B^{\\frac{1}{2}}\\mathbf{V}^{*T}a^*\\rVert^2$. Write $a^*$ as a linear combination of the eigenvectors: $a^* = \\sum_{i=1}^p \\lambda_iv_i^*$. This transforms the optimisation problem into the following: $\\text{max}\\,\\sum d_i\\lambda_i^2$ subejct to $\\sum \\lambda_i^2=1$. This is a standard Lagrange multiplier problem with solution $a^* = v_1^*$.\n",
    "\n",
    "Transforming back to the original coordinates gives\n",
    "\n",
    "\\begin{equation}\n",
    "    a = \\mathbf{U}\\mathbf{D}^{-\\frac{1}{2}}a^* = \\mathbf{U}\\mathbf{D}^{-\\frac{1}{2}} v_1^* = v_1\n",
    "\\end{equation}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2\n",
    "\n",
    "Suppose we have features $x \\in \\mathbb{R}^p$, a two-class response, with class sizes $N_1$, $N_2$, and the targets coded as $âˆ’N/N_1$, $N/N_2$.\n",
    "\n",
    "**(a)** Show that the LDA rule classifies to class 2 if\n",
    "\n",
    "\\begin{equation}\n",
    "    x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        > \\frac{1}{2}(\\hat{\\mu}_2+\\hat{\\mu}_1)^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)-\\log(N_2/N_1),\n",
    "\\end{equation}\n",
    "\n",
    "and class 1 otherwise.\n",
    "\n",
    "**(b)** Consider minimization of the least squares criterion\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{i=1}^N (y_i-\\beta_0-x_i^T\\beta)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Show that the solution $\\hat{\\beta}$ satisfies\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma}+N\\hat{\\Sigma}_B\\right]\\beta = N(\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "(after simplification), where $\\hat{\\Sigma}_B=\\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)(\\hat{\\mu}_2-\\hat{\\mu}_1)^T$.\n",
    "\n",
    "**(c)** Hence show that $\\hat{\\Sigma}_B\\beta$ is in the direction $(\\hat{\\mu}_2-\\hat{\\mu}_1)$ and thus\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar multiple.\n",
    "\n",
    "**(d)** Show that this result holds for any (distinct) coding of the two classes.\n",
    "\n",
    "**(e)** Find the solution $\\hat{\\beta}_0$ (up to the same scalar multiple as in (c)), and hence the predicted value $\\hat{f}(x) = \\hat{\\beta}_0 + x^T\\hat{\\beta}$. Consider the following rule: classify to class 2 if $\\hat{f}(x) > 0$ and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** The discriminant functions for LDA are\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_k(x) = x\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\ln(\\pi_k).\n",
    "\\end{equation}\n",
    "\n",
    "Estimating $\\mu_k$ by $\\hat{\\mu}_k$, $\\Sigma$ by $\\hat{\\Sigma}$, and $\\pi_k$ by $N_k/N$, we classify to class 2 if and only if\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{\\delta}_2(x) \n",
    "        & > \\hat{\\delta}_1(x) \\\\\n",
    "    \\Longleftrightarrow x\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 - \\frac{1}{2}\\hat{\\mu}_2^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 + \\ln(N_2/N) \n",
    "        & > x\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 - \\frac{1}{2}\\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 + \\ln(N_1/N) \\\\\n",
    "    \\Longleftrightarrow x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        & > \\frac{1}{2}\\left(\\hat{\\mu}_2\\hat{\\Sigma}^{-1}\\hat{\\mu}_2 - \\hat{\\mu}_1\\hat{\\Sigma}^{-1}\\hat{\\mu}_1\\right) + \\ln(N_1/N_2) \\\\\n",
    "    \\Longleftrightarrow x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1) \n",
    "        & > \\frac{1}{2}(\\hat{\\mu}_2+\\hat{\\mu}_1)^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)-\\log(N_2/N_1)    \n",
    "\\end{align}\n",
    "\n",
    "and to class 1 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Let $\\tilde{\\mathbf{X}}$ denote the input data matrix augmented with the intercept column. We know that $\\hat{\\beta}$ satisfies $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta} = \\tilde{\\mathbf{X}}^T\\mathbf{y}$. \n",
    "\n",
    "We wish to relate $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta}$ and $\\mathbf{X}^T\\mathbf{X}\\beta$. The $(j, k)$ entry of $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$ is $\\sum_i x_{ij}x_{ik}^T$. Taking $k=0$, the $j$th entry of the 0th column is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_i x_{ij}x_{i0}^T = \\sum_{g_i=1}x_{ij} + \\sum_{g_i=2}x_{ij} = N_1\\hat{\\mu}_{1j} + N_2\\hat{\\mu}_{2j}.\n",
    "\\end{equation}\n",
    "\n",
    "So $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$ is a $(p+1)\\times(p+1)$-matrix with $(0, 0)$ entry $N$, $(1,0),\\ldots (p, 0)$ entries taken up by $N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2$, $(0,1),\\ldots (0, p)$ entries taken up by $N_1\\hat{\\mu}^T_1 + N_2\\hat{\\mu}^T_2$, and bottom right $p\\times p$ submatrix equal to $\\mathbf{X}^T\\mathbf{X}$. This implies that $\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}\\hat{\\beta}$ is equal to the block matrix\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        N\\beta_0 + (N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta \\\\\n",
    "        \\beta_0(N_1\\hat{\\mu}_2 + N_2\\hat{\\mu}_2) + \\mathbf{X}^T\\mathbf{X}\\beta\n",
    "    \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where the top expression is a scaler and the bottom is a $p\\times 1$ vector.\n",
    "\n",
    "We have already seen that this is equal to $\\tilde{\\mathbf{X}}^T\\mathbf{y}$. The $j$th entry of $\\tilde{\\mathbf{X}}^T\\mathbf{y}$ is\n",
    "\n",
    "\\begin{equation}    \n",
    "    \\mathbf{x}_j^T\\mathbf{y} = \\frac{N}{N_2}\\left(\\sum_{g_i=2}x_{ij}\\right) - \\frac{N}{N_1}\\left(\\sum_{g_i=1}x_{ij}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "so $\\tilde{\\mathbf{X}}^T\\mathbf{y}$ is $N(\\hat{\\mu}_2 - \\hat{\\mu}_2)$ augmented with a zero in the 0th place. Equating the expressions in the 0th place gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta.\n",
    "\\end{equation}\n",
    "\n",
    "Equating the remaining terms and substituting this in gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[ \\mathbf{X}^T\\mathbf{X} - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)^T(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\right]\\beta = N(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "It remains to show that the expression in square brackets equals $(N-2)\\hat{\\Sigma} + N\\hat{\\Sigma}_B$.\n",
    "\n",
    "Indeed,\n",
    "\n",
    "\\begin{align}\n",
    "    (N-2)\\hat{\\Sigma}\n",
    "        & = \\sum_{k=1}^2\\sum_{g_i=k} (x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T \\\\\n",
    "        & = \\sum_{k=1}^2\\sum_{g_i=k} (x_ix_i^T-\\hat{\\mu}_kx_i^T -x_i\\hat{\\mu}_k^T + \\hat{\\mu}_k\\hat{\\mu}_k^T \\\\\n",
    "        & = \\sum_{k=1}^2 \\left[\\left(\\sum_{g_i=k} x_i x_i^T\\right)-N_k\\hat{\\mu}_k\\hat{\\mu}^T_k - N_k\\hat{\\mu}_k\\hat{\\mu}^T_k + N_k\\hat{\\mu}_k\\hat{\\mu}^T_k\\right]  \\\\\n",
    "        & = \\mathbf{X}^T\\mathbf{X} - \\sum_{k=1}^2 N_k\\hat{\\mu}_k\\hat{\\mu}^T_k.\n",
    "\\end{align}\n",
    "\n",
    "So,\n",
    "\n",
    "\\begin{align}\n",
    "    (N-2)\\hat{\\Sigma} + N\\hat{\\Sigma}_B\n",
    "    & = \\mathbf{X}^T\\mathbf{X}\n",
    "        - N_1\\hat{\\mu}_1\\hat{\\mu}_1^T \n",
    "        - N_2\\hat{\\mu}_2\\hat{\\mu}_2^T\n",
    "        +\\frac{N_1N_2}{N}(\\hat{\\mu}_2 - \\hat{\\mu}_1)(\\hat{\\mu}_2 - \\hat{\\mu}_1)^T \\\\\n",
    "    & = \\mathbf{X}^T\\mathbf{X}\n",
    "        + \\left(\\frac{N_1N_2}{N} - N_1\\right) \\hat{\\mu}_1\\hat{\\mu}_1^T \n",
    "        + \\left(\\frac{N_1N_2}{N} - N_2\\right) \\hat{\\mu}_2\\hat{\\mu}_2^T \n",
    "        - \\frac{N_1N_2}{N} (\\hat{\\mu}_1\\hat{\\mu}_2^T + \\hat{\\mu}_2\\hat{\\mu}_1^T) \\\\\n",
    "    & = \\mathbf{X}^T\\mathbf{X}\n",
    "        - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)^T,\n",
    "\\end{align}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** By definition, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}_B\\beta \n",
    "        = \\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta \n",
    "        = \\left[\\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta\\right] (\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}\\hat{\\beta} = \\frac{N}{N-2}\\left[ 1 - \\frac{N_1N_2}{N^2}(\\hat{\\mu}_2-\\hat{\\mu}_1)^T\\beta\\right] (\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "and thus $\\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Changing the coding makes little difference to our solution to part (b). Suppose that the targets are coded as $c_1$ and $c_2$. Then \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X}^T\\mathbf{y} = \n",
    "        \\begin{pmatrix}\n",
    "            c_1N_1 + c_2N_2 \\\\\n",
    "            c_1N_1\\hat{\\mu}_1 + c_2N_2\\hat{\\mu}_2\n",
    "        \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "Taking this through our solution we end up with\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma} + N \\hat{\\Sigma}_B\\right]\\beta + \\frac{c_1N_1+c_2N_2}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\n",
    "        = c_1N_1\\hat{\\mu}_1 + c_2N_2\\hat{\\mu}_2.\n",
    "\\end{equation}\n",
    "\n",
    "Rearranging this gives\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[(N-2)\\hat{\\Sigma} + N \\hat{\\Sigma}_B\\right]\\beta = (c_2-c_1)\\frac{N_1N_2}{N}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "This implies that $\\hat{\\beta} \\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1)$ as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** By part (b), $\\beta_0 = -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\beta$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0 \\propto -\\frac{1}{N}(N_1\\hat{\\mu}_1^T + N_2\\hat{\\mu}_2^T)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)\n",
    "\\end{equation}\n",
    "\n",
    "with the same constant of proportionality as $\\beta\\propto \\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1)$. Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{f}(x) \\propto \\left(x - \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\right)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "So we classify to class 2 iff\n",
    "\n",
    "\\begin{equation}\n",
    "    x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2 - \\hat{\\mu}_1) > \\frac{1}{N}(N_1\\hat{\\mu}_1 + N_2\\hat{\\mu}_2)\\hat{\\Sigma}^{-1}(\\hat{\\mu}_2-\\hat{\\mu}_1).\n",
    "\\end{equation}\n",
    "\n",
    "Since $N=N_1 + N_2$, this is the same as the LDA rule if $N_1=N_2$. If they are not equal, the rules will be different in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3\n",
    "\n",
    "Suppose we transform the original predictors $\\mathbf{X}$ to $\\hat{\\mathbf{Y}}$ via linear regression. In detail, let $\\hat{\\mathbf{Y}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}\\hat{\\mathbf{B}}$, where $\\mathbf{Y}$ is the indicator response matrix. Similarly for any input $x\\in\\mathbb{R}^p$, we get a transformed vector $\\hat{y} = \\hat{\\mathbf{B}}^T x\\in\\mathbb{R}^K$. Show that LDA using $\\hat{\\mathbf{Y}}$ is identical to LDA in the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Let $\\hat{\\pi}_k$, $\\hat{\\mu}_k$, and $\\hat{\\Sigma}$ denote the parameter estimates for the original predictors. Clearly $\\hat{\\pi}$ is unchanged under the transformation. The new estimates of class means are\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N_k}\\sum_{g_i = k}\\hat{y}_i = \\frac{1}{N_k}\\sum_{g_i = k}\\hat{\\mathbf{B}}^T x_i = \\hat{\\mathbf{B}}^T\\hat{\\mu}_k.\n",
    "\\end{equation}\n",
    "\n",
    "Since\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\Sigma}=\\frac{1}{N-K}\\left(\\mathbf{X}^T\\mathbf{X} - \\sum_{k=1}^K N_k\\hat{\\mu}_k\\hat{\\mu}_k^T\\right),\n",
    "\\end{equation}\n",
    "\n",
    "the transformed covariance estimate is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{N-K}\\left(\\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} - \\sum_{k=1}^K N_k \\hat{\\mathbf{B}}^T\\hat{\\mu}_k\\hat{\\mu}_k^T\\hat{\\mathbf{B}}\\right) = \\hat{\\mathbf{B}}^T\\hat{\\Sigma}\\hat{\\mathbf{B}}.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore the new linear discriminant functions are\n",
    "\n",
    "\\begin{align}\n",
    "    \\delta_k(\\hat{y}) \n",
    "        & = (x^T\\hat{\\mathbf{B}}) \\left(\\hat{\\mathbf{B}}^{-1} \\hat{\\Sigma}^{-1} (\\hat{\\mathbf{B}}^T)^{-1}\\right) (\\hat{\\mathbf{B}}^T\\hat{\\mu}_k) - \\frac{1}{2} (\\hat{\\mu}_k^T\\hat{\\mathbf{B}}) \\left(\\hat{\\mathbf{B}}^{-1} \\hat{\\Sigma}^{-1} (\\hat{\\mathbf{B}}^T)^{-1}\\right) (\\hat{\\mathbf{B}}^T\\hat{\\mu}_k) + \\ln(\\hat{\\pi}_k) \\\\\n",
    "        & = x^T\\hat{\\Sigma}\\hat{\\mu}_k - \\frac{1}{2} \\hat{\\mu}_k^T\\hat{\\Sigma}\\hat{\\mu}_k + \\ln(\\hat{\\pi}_k) \\\\\n",
    "        & = \\delta_k(x),\n",
    "\\end{align}\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4\n",
    "\n",
    "Consider the multilogit model with $K$ classes (4.17). Let $\\beta$ be the $(p + 1)(K âˆ’ 1)$-vector consisting of all the coefficients. Define a suitably enlarged version of the input vector $x$ to accommodate this vectorized coefficient matrix. Derive the Newton-Raphson algorithm for maximizing the multinomial log-likelihood, and describe how you would implement this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Given a length $p$ input vector $x$ (we assume the intercept 1 is already included), let $\\tilde{x}$ be the $(p+1)(K-1) \\times (K-1)$ block matrix with $K-1$ $x$'s along the diagonal and 0s elsewhere. Then $\\tilde{x}^T\\beta$ is the length $K-1$ vector with $k$th entry $\\beta_k^T x$ (again we include the intercept term in $\\beta_k$). From this the vector of probabilities $p_k(x; \\theta)$ can easily be calculated.\n",
    "\n",
    "We now derive the score equations and Newton-Raphson algorithm. Let $y_i = (y_{i0}, \\ldots, y_{iK})^T$ denote the $i$th row of the indicator response matrix $\\mathbf{Y}$. The multinomial log-likelihood is\n",
    "\n",
    "\\begin{align}\n",
    "    l(\\beta)\n",
    "        & = \\sum_{i=1}^N \\sum_{k=1}^{K} y_{ik} \\ln (p_k(x_i; \\theta)) \\\\\n",
    "        & = \\sum_{i=1}^N \\left[\\sum_{k=1}^{K-1}\\left[ y_{ik} \\left( \\beta_k^T x_i - \\ln(1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i))\\right)\\right] - (1-\\sum_{l=1}^{K-1} y_{il})\\ln(1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i))\\right] \\\\\n",
    "        & = \\sum_{i=1}^N \\left[\\sum_{k=1}^{K-1} \\left( y_{ik} \\beta_k^T x_i \\right) - \\ln(1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i))\\right].\n",
    "\\end{align}\n",
    "\n",
    "Differentiating this with respect to one of the $\\beta_k$ gives\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial l(\\beta)}{\\partial \\beta_k} \n",
    "        & = \\sum_{i=1}^N \\left[ y_{ik}x_i - \\frac{\\exp(\\beta_k^Tx)}{1 + \\sum_{l=1}^{K-1} \\exp(\\beta_l^T x_i)} x_i \\right] \\\\\n",
    "        & = \\sum_{i=1}^N (y_{ik} - p_k(x_i;\\theta))x_i\n",
    "\\end{align}\n",
    "\n",
    "and differentiating again yields\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k \\partial \\beta_l^T}\n",
    "        = \\begin{cases}\n",
    "                -\\sum_{i=1}^N p_k(x_i; \\theta) p_l(x_i; \\theta) x_i x_i^T & \\text{if } k\\neq l \\\\\n",
    "                -\\sum_{i=1}^N p_k(x_i; \\theta) (1 - p_k(x_i; \\theta)) x_i x_i^T & \\text{if } k=l\n",
    "          \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This gives us enough to implement Newton-Raphson: concatenate the $\\frac{\\partial l(\\beta)}{\\partial \\beta_k}$ and $\\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k \\partial \\beta_l^T}$ into a single vector and block matrix $\\frac{\\partial l(\\beta)}{\\partial \\beta}$ and $\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}$, respectively. Then set\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta^{\\text{new}} = \\beta^{\\text{old}} - \\left(\\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\\right)^{-1} \\frac{\\partial l(\\beta)}{\\partial \\beta}.\n",
    "\\end{equation}\n",
    "\n",
    "We will now describe how to use the enlarged vector $\\tilde{x}$ to put everything in terms of matrix products. The idea is to vectorise natural matrices like $\\mathbf{Y}$ in a similar fashion to $\\beta$.\n",
    "\n",
    "Let $\\tilde{\\mathbf{X}}$ denote the $(k-1)N\\times (p+1)(K-1)$-matrix obtained by stacking $\\tilde{x}_1^T, \\ldots, \\tilde{x}_N^T$ on top of each other, or equivalently obtained from $\\mathbf{X}$ by replacing each $x_i^T$ with its tilde counterpart. Let $\\tilde{\\mathbf{y}}$ be the length $(k-1)N$ vector obtained by stacking the observations $y_i$ on top of each other in order. Similarly, let $\\mathbf{p}_i$ denote the length $K-1$ vector with entries $p_1(x_i; \\theta), \\ldots p_{K-1}(x_i; \\theta)$ and let $\\tilde{\\mathbf{p}}$ be the length $(k-1)N$ vector obtained by stacking the $\\mathbf{p}_i$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^N \\tilde{x}_i(y_i - \\mathbf{p}_i) = \\tilde{\\mathbf{X}}^T(\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{p}})\n",
    "\\end{equation}\n",
    "\n",
    "For $i\\in\\{1, \\ldots, N\\}$, let $\\mathbf{W}_i$ denote the $(K-1)\\times(K-1)$ matrix with $(k, l)$ entry $p_k(x_i;\\theta)p_l(x_i;\\theta)$ if $k\\neq l$ and $p_k(x_i;\\theta)(1-p_k(x_i;\\theta)$ if $k=l$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\n",
    "         = \\sum_{i=1}^N \\tilde{x}_i \\mathbf{W}_i \\tilde{x}_i^T = \\tilde{\\mathbf{X}}^T\\mathbf{W}\\tilde{\\mathbf{X}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{W}$ is the $(K-1)N\\times (K-1)N$ block matrix with $\\mathbf{W}_1,\\ldots, \\mathbf{W}_N$ along the diagonal.\n",
    "\n",
    "With this in hand, the Newton-Raphson algorithm can be expressed as an iterated reweighted least squares procedure as in the $K=2$ case:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta^{\\text{new}} = (\\tilde{\\mathbf{X}}^T\\mathbf{W}\\tilde{\\mathbf{X}})^{-1}\\tilde{\\mathbf{X}}^T\\mathbf{W}\\tilde{\\mathbf{z}}, \\quad\\text{ where }\\quad \\tilde{\\mathbf{z}} = \\tilde{\\mathbf{X}}\\beta^{\\text{old}} + \\mathbf{W}^{-1}(\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{p}})\n",
    "\\end{equation}\n",
    "\n",
    "This can be implemented directly, updating $\\mathbf{W}$ and $\\tilde{\\mathbf{p}}$ at each step, or as the solution to the weighted least squares problem\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{\\beta}{\\text{min}} (\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{X}}\\beta)^T \\mathbf{W} (\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{X}}\\beta).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8\n",
    "\n",
    "Consider the multivariate Gaussian model $X | G = k \\sim N(\\mu_k,\\Sigma)$, with the additional restriction that $\\text{rank}\\{\\mu_k\\}_1^K = L < \\text{max}(K âˆ’ 1,p)$. Derive the constrained MLEs for the $\\mu_k$ and $\\Sigma$. Show that the Bayes classification rule is equivalent to classifying in the reduced subspace computed by LDA (Hastie and Tibshirani, 1996b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
