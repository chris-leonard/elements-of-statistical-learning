{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning - Chapter 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "Show that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Without loss of generality, assume that the smaller model has had the final feature $\\mathbf{x}_p$ removed. Let $\\hat{\\mathbf{y}}$ denote the least squares approximation for the larger model and $\\hat{\\mathbf{y}}^{\\prime}$ be that for the smaller. We need to show that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\text{RSS}_0 - \\text{RSS}_1}{\\text{RSS}_1 / (N-p-1)} = \\frac{\\hat{\\beta}_p^2}{\\hat{\\sigma}^2 v_p},\n",
    "\\end{equation}\n",
    "\n",
    "where $v_j$ is the $j$th diagonal element of $(\\mathbf{X}^T \\mathbf{X})^{-1}$. \n",
    "\n",
    "First note that by definition $\\hat{\\sigma}^2 = \\text{RSS}_0 / (N-p-1)$. Moreover, since $\\hat{\\mathbf{y}}$ is the projection of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$, their difference is orthogonal to any element of the columns space. In particular, it is orthogonal to $\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime}$, so\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2 \n",
    "        + \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Rightarrow \\text{RSS}_0 - \\text{RSS}_1\n",
    "         = \\lVert \\mathbf{y} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2 \n",
    "            - \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2\n",
    "         = \\lVert \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} \\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "Now let $\\mathbf{z}_0,\\ldots ,\\mathbf{z}_p$ denote the orthogonal basis of the column space of $\\mathbf{X}$, obtained from $\\mathbf{x}_0,\\ldots,\\mathbf{x}_p$ using the Gram-Schmidt process (Algorithm 3.1). The least squares estimates  $\\hat{\\mathbf{y}}$ and $\\hat{\\mathbf{y}}^{\\prime}$ are the projections of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$ and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{span}(\\{\\, \\mathbf{x}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\} = \\text{span}(\\{\\, \\mathbf{z}_j \\,\\mid\\, 0\\leq j\\leq p-1 \\,\\}\n",
    "\\end{equation}\n",
    "\n",
    "respectively. Since the $\\mathbf{z}_j$ are orthogonal, this implies that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}^{\\prime} = \\frac{\\langle \\mathbf{z}_p, \\mathbf{y}\\rangle}{\\langle \\mathbf{z}_p, \\mathbf{z}_p\\rangle}\\mathbf{z}_p = \\hat{\\beta}_p \\mathbf{z}_p.\n",
    "\\end{equation}\n",
    "\n",
    "Putting these elements together, it just remains to show that $v_p = \\lVert \\mathbf{z}_p\\rVert ^{-2}$. But, if $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ is the QR-decomposition of $\\mathbf{X}$ then $(\\mathbf{X}^T \\mathbf{X})^{-1} = \\mathbf{R}^{-1}(\\mathbf{R}^{-1})^T$. Since $\\mathbf{R}$ is upper-triangular, the $p$th diagonal element of $\\mathbf{R}^{-1}$ is $R_{pp}^{-1} = \\lVert \\mathbf{z}_p\\rVert ^{-1}$ and the claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "\n",
    "Given data on two variables $X$ and $Y$, consider fitting a cubic polynomial regression model $f(X) = \\sum_{j=0}^3\\beta_jX^j$. In addition to plotting the fitted curve, you would like a 95% confidence band about the curve. Consider the following two approaches:\n",
    "\n",
    "1. At each point $x_0$, form a 95% confidence interval for the linear function $a^T\\beta = \\sum_{j=0}^3 \\beta_jx_0^j$.\n",
    "2. Form a 95% confidence set for $\\beta$ as in (3.15), which in turn generates confidence intervals for $f(x_0)$.\n",
    "\n",
    "How do these approaches differ? Which band is likely to be wider? Conduct a small simulation experiment to compare the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5\n",
    "\n",
    "Consider the ridge regression problem (3.41). Show that this problem is equivalent to the problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}^c = \\underset{\\beta^c}{\\text{argmin}} \\Bigg\\{ \\sum_{i=1}^N \\big[ y_i - \\beta_0^c  - \\sum_{j=1}^p (x_{ij}-\\bar{x}_j)\\beta_j^c\\big]^2 + \\lambda \\sum_{j=1}^p (\\beta_j^c)^2\\Bigg\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Give the correspondence between $\\beta^c$ and the original $\\beta$ in (3.41). Characterize the solution to this modified criterion. Show that a similar result holds for the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Since\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0^c + \\sum_{j=1}^p(x_{ij} - \\bar{x}_j)\\beta_j^c \n",
    "        = \\left( \\beta_0^c - \\sum_{j=1}^p \\bar{x}_j\\beta_j^c\\right) + \\sum_{j=1}^p x_{ij}\\beta_j^c,\n",
    "\\end{equation}\n",
    "\n",
    "the two problems are equivalent with \n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_0^c = \\beta_0 + \\sum_{j=1}^p \\bar{x}_j\\beta_j = \\frac{1}{N}\\sum_{i=1}^N \\hat{y}_i\n",
    "\\end{equation}\n",
    "\n",
    "and $\\beta_j^c = \\beta_j$ for $j\\neq 0$.\n",
    "\n",
    "At the minimum, the derivative with respect to $\\beta_0^c$ of the expression in braces is zero, so\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^N \\left[ y_i - \\beta_0^c  - \\sum_{j=1}^p (x_{ij}-\\bar{x}_j)\\beta_j^c \\right] = 0 \\quad\n",
    "        \\Rightarrow \\quad \\left( \\sum_{i=1}^N y_i \\right) - N\\beta_0^c - 0 = 0\n",
    "\\end{align}\n",
    "\n",
    "and thus $\\beta_0^c = \\bar{y}$.\n",
    "\n",
    "LASSO???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Show that the ridge regression estimate is the mean (and mode) of the posterior distribution, under a Gaussian prior $\\beta\\sim \\mathcal{N}(0, \\tau^2 \\mathbf{I})$, and Gaussian sampling model $y\\sim\\mathcal{N}(\\mathbf{X}\\beta, \\sigma^2\\mathbf{I})$. Find the relationship between the regularization parameter $\\lambda$ in the ridge formula, and the variances $\\tau^2$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The posterior distribution for $\\beta$ has density function\n",
    "\n",
    "\\begin{align}\n",
    "    f_{\\beta\\mid Y_1, \\ldots, Y_N}(\\beta)\n",
    "        & \\propto f_{Y_1, \\ldots, Y_N\\mid\\beta}(y_1,\\ldots, y_N)f_{\\beta}(\\beta) \\\\\n",
    "        & = \\left(\\prod_{i=1}^N f_{Y\\mid\\beta}(y_i)\\right) f_{\\beta}(\\beta) \\\\\n",
    "        & \\propto \\text{exp}\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - x_i^T\\beta)^2 - \\frac{1}{\\tau^2}\\lVert \\beta\\rVert^2\\right)\\\\\n",
    "        & = \\text{exp}\\left( -\\frac{1}{2\\sigma^2}\\left( \\lVert\\mathbf{y} - \\mathbf{X}\\beta\\rVert^2 + \\frac{\\sigma^2}{\\tau^2}\\lVert\\beta\\rVert^2\\right)\\right).\n",
    "\\end{align}\n",
    "\n",
    "So the mode of this distribution is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{\\beta}{\\text{argmax}} \\left(f_{\\beta\\mid Y_1, \\ldots, Y_N}(\\beta)\\right)\n",
    "        = \\underset{\\beta}{\\text{argmin}}\\left(\\lVert\\mathbf{y} - \\mathbf{X}\\beta\\rVert^2 + \\frac{\\sigma^2}{\\tau^2}\\lVert\\beta\\rVert^2\\right).\n",
    "\\end{equation}\n",
    "\n",
    "This is the ridge regression estimate with $\\lambda = \\sigma^2/\\tau^2$. Moreover, since the posterior is Gaussian (p.64) the mean equals the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
