{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24ae93e-fd12-4cef-9877-777f8213a7c9",
   "metadata": {},
   "source": [
    "# Chapter 7 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548d429-0680-44be-91b6-773ed033883a",
   "metadata": {},
   "source": [
    "## 7.2 Bias, Variance and Model Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbe1db-4d56-4adc-87db-0ee702048785",
   "metadata": {},
   "source": [
    "For a given loss function $L(Y, \\hat{f}(X))$, where the prediction function $\\hat{f}$ has been estimated from training set $\\mathcal{T}$, the *Test error*, or *generalisation error* is the prediction error over an independent test sample:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Err}_{\\mathcal{T}} = \\text{E}_{X, Y}\\left[ L(Y, \\hat{f}(X)) \\mid \\mathcal{T} \\right].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc982192-8c40-49a7-821a-f25ce6f10df2",
   "metadata": {},
   "source": [
    "Taking the expectation over the training set $\\mathcal{T}$ gives the *expected test error* or *expected prediction error*:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Err} = \\text{E}_{\\mathcal{T}}\\left[\\text{Err}_{\\mathcal{T}} \\right].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e667f1-6eaa-47f4-935e-316448c7dc23",
   "metadata": {},
   "source": [
    "## 7.10 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcd548-248c-4fc9-89da-eb5f02be9f7f",
   "metadata": {},
   "source": [
    "$K$-fold cross-validation estimates $\\text{Err}$ (*not* $\\text{Err}_{\\mathcal{T}}$). \n",
    "Leave-one-out cross validation ($K=N$) provides an unbiased estimate but can have high variance.\n",
    "For $K=5, 10$, the estimator has lower variance but can be baised (because the training sets are smaller).\n",
    "The magnitude of this bias depends on where you are on the learning curve.\n",
    "\n",
    "*CORRECTION:* The claim that leave-one-out CV has higher variance is a common misconception. See this [Stack Exchange thread](https://stats.stackexchange.com/a/252031) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c09015-bcc0-404c-8d91-a841bbb1b60a",
   "metadata": {},
   "source": [
    "When hyper-parameter tuning, often a 'one-standard error' rule is used.\n",
    "This involves choosing the most parsimonious model whose error is no more than one standard error (*not* deviation) above the error of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1f4b7-5b15-4b25-a6aa-89aa879d31af",
   "metadata": {},
   "source": [
    "**Question:** How does the 'one-standard error' rule work when you have multiple hyper-parameters, so it isn't clear which model is most parsimonious?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5488ba-c787-4eb1-9db2-dafc7ec37c13",
   "metadata": {},
   "source": [
    "## 7.11 Bootstrap Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cde5d7-de6c-4d3a-870f-5c36909ec9fa",
   "metadata": {},
   "source": [
    "Given a training set $\\mathbb{Z}$ we take $B$ boostrap samples $\\mathbb{Z}^{*b}$ ($1\\leq b\\leq B$).\n",
    "If $S(\\mathbb{Z})$ is a quantity computed from $\\mathbb{Z}$, we can estimate any aspect of the distribution of $S(\\mathbb{Z})$ using the $S(\\mathbb{Z}^{*b})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af69c9-c827-4bb2-a8fb-b8464f5e8819",
   "metadata": {},
   "source": [
    "We can estimate $\\text{Err}$ for a model by taking the average error $\\hat{\\text{Err}}_{\\text{boot}}$ of models trained on bootstrap samples.\n",
    "This will tend to overestimate because on average $1-e^{-1} \\approx 0.632$ of the samples will belong to the training set.\n",
    "We can improve our estimate by taking the average error $\\hat{\\text{Err}}^{(1)}_{\\text{boot}}$ over samples not in the training set for each bootstrapped model.\n",
    "This then is biased to over-estimate $\\text{Err}$ because each training set is $\\approx 0.632$ the size of the full training set.\n",
    "The book gives an improved correction that that is a weighted average of $\\hat{\\text{Err}}^{(1)}_{\\text{boot}}$ with the training error.\n",
    "It depends on the *no-information error rate* - the error rate of our prediction rule if the inputs and outputs were independent.\n",
    "\n",
    "**Question:** The bootstrapped training sets will contain duplicates - what is the effect of these? Why not just remove these and use a random subset of proportion 0.632?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eae30-e73c-4918-8d45-3eee1efc3c0a",
   "metadata": {},
   "source": [
    "### 7.11.1 Example (Continued)\n",
    "\n",
    "Minimisation of cross-validation, bootstrap, or AIC over possible hyper-parameter values all yield models fairly close to the best available.\n",
    "In practice, AIC is often not available because estimating the effective number of parameters is difficult.\n",
    "\n",
    "For the purpose of model selection, it doesn't matter if our estimate of test error is biased as long as it doesn't affect the relative performance of different models.\n",
    "However, for the models tried in the book (linear and KNN) bootstrap and CV provide better estimates of test error.\n",
    "It states that for trees these under-estimate the true error by 10% because the search for best tree is strongly affected by the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ff5e8-461b-45e8-8fb1-626689c1c9a1",
   "metadata": {},
   "source": [
    "**Question:** What does the last sentence mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93506c-b628-4363-bb1d-e68d8c281604",
   "metadata": {},
   "source": [
    "## 7.12 Conditional or Expected Test Error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75ca69-dc37-44ac-9c0d-2f5fdd79d7e3",
   "metadata": {},
   "source": [
    "Both 10-fold CV and leave-one-out CV estimate $\\text{Err}$ rather than $\\text{Err}_{\\mathcal{T}}$ with 10-fold giving a better estimate.\n",
    "Similarly the bootstrap estimates $\\text{Err}$ rather than $\\text{Err}_{\\mathcal{T}}$.\n",
    "In general, estimating $\\text{Err}_{\\mathcal{T}}$ for a specific training set is a difficult problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4dc76-2ad0-44a4-926d-b964db05b00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
